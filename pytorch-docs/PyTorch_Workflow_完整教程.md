# PyTorch æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ - å®Œæ•´æ•™ç¨‹

> **æœ¬æ–‡æ¡£æ•´åˆäº†åŸå§‹æ•™ç¨‹ã€æœ€ä½³å®è·µå’Œ2024å¹´æœ€æ–°æŠ€æœ¯**

---

## ğŸ“š ç›®å½•

- [ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒå·¥ä½œæµç¨‹](#ç¬¬ä¸€éƒ¨åˆ†æ ¸å¿ƒå·¥ä½œæµç¨‹)
  - [1. æ•°æ®å‡†å¤‡ä¸åŠ è½½](#1-æ•°æ®å‡†å¤‡ä¸åŠ è½½)
  - [2. æ„å»ºæ¨¡å‹](#2-æ„å»ºæ¨¡å‹)
  - [3. è®­ç»ƒæ¨¡å‹](#3-è®­ç»ƒæ¨¡å‹)
  - [4. æ¨¡å‹è¯„ä¼°ä¸é¢„æµ‹](#4-æ¨¡å‹è¯„ä¼°ä¸é¢„æµ‹)
  - [5. ä¿å­˜ä¸åŠ è½½æ¨¡å‹](#5-ä¿å­˜ä¸åŠ è½½æ¨¡å‹)
  - [6. å®Œæ•´æµç¨‹æ•´åˆ](#6-å®Œæ•´æµç¨‹æ•´åˆ)

- [ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜çº§æŠ€æœ¯ä¸æœ€ä½³å®è·µ](#ç¬¬äºŒéƒ¨åˆ†é«˜çº§æŠ€æœ¯ä¸æœ€ä½³å®è·µ)
  - [7. è®­ç»ƒå¾ªç¯ä¼˜åŒ–æŠ€æœ¯](#7-è®­ç»ƒå¾ªç¯ä¼˜åŒ–æŠ€æœ¯)
  - [8. æ¨¡å‹è¯„ä¼°æŒ‡æ ‡](#8-æ¨¡å‹è¯„ä¼°æŒ‡æ ‡)
  - [9. è°ƒè¯•ä¸ç›‘æ§](#9-è°ƒè¯•ä¸ç›‘æ§)
  - [10. ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ](#10-ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ)

- [ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®æˆ˜é¡¹ç›®](#ç¬¬ä¸‰éƒ¨åˆ†å®æˆ˜é¡¹ç›®)
  - [11. å®Œæ•´é¡¹ç›®ç¤ºä¾‹](#11-å®Œæ•´é¡¹ç›®ç¤ºä¾‹)
  - [12. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#12-å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)

---

## æ–‡æ¡£ä¿¡æ¯

**æ¥æº:** [Learn PyTorch for Deep Learning - Chapter 01](https://www.learnpytorch.io/01_pytorch_workflow/)
**ä½œè€…:** Daniel Bourke (Zero to Mastery)
**GitHub:** [pytorch-deep-learning](https://github.com/mrdbourke/pytorch-deep-learning)
**æ–‡æ¡£ç‰ˆæœ¬:** v2.0 (å¢å¼ºç‰ˆ)
**æ›´æ–°æ—¥æœŸ:** 2025-11-16
**é€‚ç”¨ PyTorch ç‰ˆæœ¬:** 1.12+

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ•™ç¨‹å,ä½ å°†èƒ½å¤Ÿ:

âœ… ç†è§£å®Œæ•´çš„ PyTorch æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹
âœ… æ„å»ºã€è®­ç»ƒå’Œè¯„ä¼° PyTorch æ¨¡å‹
âœ… å®ç°é«˜æ•ˆçš„è®­ç»ƒå¾ªç¯
âœ… åº”ç”¨æœ€ä½³å®è·µä¼˜åŒ–æ¨¡å‹æ€§èƒ½
âœ… ä¿å­˜å’Œéƒ¨ç½²è®­ç»ƒå¥½çš„æ¨¡å‹
âœ… è°ƒè¯•å’Œç›‘æ§è®­ç»ƒè¿‡ç¨‹
âœ… å¤„ç†å®é™…é¡¹ç›®ä¸­çš„å¸¸è§é—®é¢˜

---

# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒå·¥ä½œæµç¨‹

## PyTorch å·¥ä½œæµç¨‹æ¦‚è§ˆ

æœºå™¨å­¦ä¹ çš„æœ¬è´¨:**ä»è¿‡å»çš„æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼,ç”¨è¿™äº›æ¨¡å¼é¢„æµ‹æœªæ¥**

![PyTorch Workflow](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png)

### å®Œæ•´å·¥ä½œæµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PyTorch æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. æ•°æ®å‡†å¤‡ (Data Preparation)
   â”œâ”€â”€ æ”¶é›†å’ŒåŠ è½½æ•°æ®
   â”œâ”€â”€ æ•°æ®æ¸…æ´—å’Œè½¬æ¢
   â”œâ”€â”€ åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
   â””â”€â”€ æ•°æ®å¯è§†åŒ–
          â†“
2. æ„å»ºæ¨¡å‹ (Build Model)
   â”œâ”€â”€ å®šä¹‰æ¨¡å‹æ¶æ„
   â”œâ”€â”€ åˆå§‹åŒ–å‚æ•°
   â””â”€â”€ è®¾ç½®å‰å‘ä¼ æ’­
          â†“
3. è®­ç»ƒæ¨¡å‹ (Train Model)
   â”œâ”€â”€ é€‰æ‹©æŸå¤±å‡½æ•°
   â”œâ”€â”€ é€‰æ‹©ä¼˜åŒ–å™¨
   â”œâ”€â”€ å®ç°è®­ç»ƒå¾ªç¯
   â””â”€â”€ å®ç°éªŒè¯å¾ªç¯
          â†“
4. è¯„ä¼°ä¸é¢„æµ‹ (Evaluate & Predict)
   â”œâ”€â”€ åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
   â”œâ”€â”€ è®¡ç®—è¯„ä¼°æŒ‡æ ‡
   â”œâ”€â”€ å¯è§†åŒ–ç»“æœ
   â””â”€â”€ è¿›è¡Œæ¨ç†é¢„æµ‹
          â†“
5. ä¿å­˜ä¸éƒ¨ç½² (Save & Deploy)
   â”œâ”€â”€ ä¿å­˜æ¨¡å‹å‚æ•°
   â”œâ”€â”€ åŠ è½½æ¨¡å‹
   â””â”€â”€ éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
```

---

## 1. æ•°æ®å‡†å¤‡ä¸åŠ è½½

### 1.1 æœºå™¨å­¦ä¹ çš„ä¸¤ä¸ªæ ¸å¿ƒä»»åŠ¡

**ä»»åŠ¡ä¸€:** å°†æ•°æ®è½¬æ¢ä¸ºæ•°å­—è¡¨ç¤º (æ•°å€¼åŒ–)
**ä»»åŠ¡äºŒ:** æ„å»ºæˆ–é€‰æ‹©æ¨¡å‹æ¥å­¦ä¹ è¿™äº›æ•°å­—è¡¨ç¤º

### 1.2 ç¯å¢ƒå‡†å¤‡

```python
# å¯¼å…¥å¿…è¦çš„åº“
import torch
from torch import nn  # nn = neural networks
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

# æ£€æŸ¥ PyTorch ç‰ˆæœ¬
print(f"PyTorch version: {torch.__version__}")

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
torch.manual_seed(42)
```

### 1.3 åˆ›å»ºæ•°æ®é›†

#### ç¤ºä¾‹: çº¿æ€§å›å½’æ•°æ®

```python
# å®šä¹‰çœŸå®çš„å‚æ•° (æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹å­¦ä¹ åˆ°è¿™äº›å€¼)
weight = 0.7
bias = 0.3

# åˆ›å»ºæ•°æ®: y = wx + b (çº¿æ€§å…³ç³»)
start = 0
end = 1
step = 0.02

# åˆ›å»ºç‰¹å¾ X
X = torch.arange(start, end, step).unsqueeze(dim=1)  # shape: [50, 1]

# åˆ›å»ºæ ‡ç­¾ y
y = weight * X + bias  # shape: [50, 1]

print(f"X shape: {X.shape}")
print(f"y shape: {y.shape}")
print(f"Number of samples: {len(X)}")
print(f"\nFirst 5 X values:\n{X[:5]}")
print(f"\nFirst 5 y values:\n{y[:5]}")
```

**è¾“å‡ºç¤ºä¾‹:**
```bash
X shape: torch.Size([50, 1])
y shape: torch.Size([50, 1])
Number of samples: 50

First 5 X values:
tensor([[0.0000],
        [0.0200],
        [0.0400],
        [0.0600],
        [0.0800]])

First 5 y values:
tensor([[0.3000],
        [0.3140],
        [0.3280],
        [0.3420],
        [0.3560]])
```

### 1.4 æ•°æ®é›†åˆ’åˆ†

#### è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†çš„ä½œç”¨

| æ•°æ®é›† | ç”¨é€” | å»ºè®®æ¯”ä¾‹ | ä½¿ç”¨é¢‘ç‡ |
|--------|------|---------|---------|
| **è®­ç»ƒé›† (Training)** | æ¨¡å‹ä»ä¸­å­¦ä¹ æ¨¡å¼ | 60-80% | å¿…é¡» |
| **éªŒè¯é›† (Validation)** | è°ƒæ•´è¶…å‚æ•°,é€‰æ‹©æœ€ä½³æ¨¡å‹ | 10-20% | æ¨è |
| **æµ‹è¯•é›† (Testing)** | æœ€ç»ˆè¯„ä¼°æ¨¡å‹æ€§èƒ½ | 10-20% | å¿…é¡» |

#### å®ç°æ•°æ®åˆ’åˆ†

```python
# 80% è®­ç»ƒ, 20% æµ‹è¯•
train_split = int(0.8 * len(X))

# åˆ’åˆ†æ•°æ®
X_train = X[:train_split]  # å‰ 80%
y_train = y[:train_split]

X_test = X[train_split:]   # å 20%
y_test = y[train_split:]

print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train)}")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(X_test)}")
```

#### æ›´å®Œæ•´çš„æ•°æ®åˆ’åˆ†<â€”â€”>ä¸‰åˆ†æ³• (æ¨èç”¨äºå¤§å‹é¡¹ç›®)

```python
def split_data(X, y, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """
    å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†

    å‚æ•°:
        X: ç‰¹å¾æ•°æ®
        y: æ ‡ç­¾æ•°æ®
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤ 0.7, å³ 70%)
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤ 0.15, å³ 15%)
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤ 0.15, å³ 15%)

    è¿”å›:
        X_train, y_train, X_val, y_val, X_test, y_test
    """
    # éªŒè¯ä¸‰ä¸ªæ¯”ä¾‹ä¹‹å’Œæ˜¯å¦ç­‰äº 1.0
    # ä½¿ç”¨ 1e-6 ä½œä¸ºå®¹å·®å€¼æ¥å¤„ç†æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
        "æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ç­‰äº 1"

    # è·å–æ•°æ®é›†çš„æ€»æ ·æœ¬æ•°
    n = len(X)
    
    # è®¡ç®—è®­ç»ƒé›†çš„ç»“æŸç´¢å¼• (ä¾‹å¦‚: 1000 * 0.7 = 700)
    train_end = int(n * train_ratio)
    
    # è®¡ç®—éªŒè¯é›†çš„ç»“æŸç´¢å¼• (ä¾‹å¦‚: 1000 * (0.7 + 0.15) = 850)
    val_end = int(n * (train_ratio + val_ratio))

    # åˆ’åˆ†è®­ç»ƒé›†: ä»å¼€å§‹åˆ° train_end
    X_train, y_train = X[:train_end], y[:train_end]
    
    # åˆ’åˆ†éªŒè¯é›†: ä» train_end åˆ° val_end
    X_val, y_val = X[train_end:val_end], y[train_end:val_end]
    
    # åˆ’åˆ†æµ‹è¯•é›†: ä» val_end åˆ°ç»“æŸ
    X_test, y_test = X[val_end:], y[val_end:]

    # è¿”å›åˆ’åˆ†åçš„å…­ä¸ªæ•°æ®é›†
    return X_train, y_train, X_val, y_val, X_test, y_test


# ä½¿ç”¨ç¤ºä¾‹: è°ƒç”¨å‡½æ•°è¿›è¡Œæ•°æ®åˆ’åˆ†
X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y)

# æ‰“å°å„æ•°æ®é›†çš„æ ·æœ¬æ•°é‡,éªŒè¯åˆ’åˆ†ç»“æœ
print(f"è®­ç»ƒ: {len(X_train)}, éªŒè¯: {len(X_val)}, æµ‹è¯•: {len(X_test)}")
```

### 1.5 æ•°æ®å¯è§†åŒ–ï¼ˆéå¸¸é‡è¦ï¼‰

> **æ•°æ®æ¢ç´¢è€…çš„åº§å³é“­:** "å¯è§†åŒ–,å¯è§†åŒ–,å¯è§†åŒ–!"

```python
# å¯¼å…¥ matplotlib å¹¶è®¾ç½®ä¸­æ–‡å­—ä½“ è§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜
import matplotlib.pyplot as plt

# è®¾ç½®é»˜è®¤å­—ä½“ä¸ºé»‘ä½“,ç”¨äºæ­£ç¡®æ˜¾ç¤ºä¸­æ–‡å­—ç¬¦
plt.rcParams['font.sans-serif'] = ['SimHei']  # é»‘ä½“

# è§£å†³åæ ‡è½´è´Ÿå· '-' æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
plt.rcParams['axes.unicode_minus'] = False    # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

def plot_predictions(train_data=X_train,
                     train_labels=y_train,
                     test_data=X_test,
                     test_labels=y_test,
                     predictions=None):
    """
    ç»˜åˆ¶è®­ç»ƒæ•°æ®ã€æµ‹è¯•æ•°æ®å’Œé¢„æµ‹ç»“æœ

    å‚æ•°:
        train_data: è®­ç»ƒç‰¹å¾
        train_labels: è®­ç»ƒæ ‡ç­¾
        test_data: æµ‹è¯•ç‰¹å¾
        test_labels: æµ‹è¯•æ ‡ç­¾
        predictions: æ¨¡å‹é¢„æµ‹ (å¯é€‰,é»˜è®¤ä¸º None)
    """
    # åˆ›å»ºå›¾å½¢,è®¾ç½®ç”»å¸ƒå¤§å°ä¸º 10x7 è‹±å¯¸
    plt.figure(figsize=(10, 7))

    # ç»˜åˆ¶è®­ç»ƒæ•°æ®æ•£ç‚¹å›¾
    # c="b": è“è‰², s=4: ç‚¹çš„å¤§å°, label: å›¾ä¾‹æ ‡ç­¾
    plt.scatter(train_data, train_labels, c="b", s=4, label="è®­ç»ƒæ•°æ®")

    # ç»˜åˆ¶æµ‹è¯•æ•°æ®æ•£ç‚¹å›¾ (ç»¿è‰²)
    plt.scatter(test_data, test_labels, c="g", s=4, label="æµ‹è¯•æ•°æ®")

    # å¦‚æœæä¾›äº†é¢„æµ‹ç»“æœ,åˆ™ç»˜åˆ¶é¢„æµ‹æ•£ç‚¹å›¾
    if predictions is not None:
        # ç»˜åˆ¶é¢„æµ‹ç»“æœ (çº¢è‰²),ç”¨äºå¯¹æ¯”çœŸå®æµ‹è¯•æ•°æ®
        plt.scatter(test_data, predictions, c="r", s=4, label="é¢„æµ‹")

    # æ·»åŠ å›¾ä¾‹,è®¾ç½®å­—ä½“å¤§å°ä¸º 14
    plt.legend(prop={"size": 14})
    
    # è®¾ç½® X è½´æ ‡ç­¾
    plt.xlabel("X")
    
    # è®¾ç½® Y è½´æ ‡ç­¾
    plt.ylabel("y")
    
    # è®¾ç½®å›¾è¡¨æ ‡é¢˜
    plt.title("æ•°æ®å’Œé¢„æµ‹å¯è§†åŒ–")
    
    # æ·»åŠ ç½‘æ ¼çº¿,alpha=0.3 è®¾ç½®é€æ˜åº¦ä½¿ç½‘æ ¼ä¸ä¼šè¿‡äºçªå‡º
    plt.grid(True, alpha=0.3)


# è°ƒç”¨å‡½æ•°å¯è§†åŒ–åŸå§‹æ•°æ® (ä¸åŒ…å«é¢„æµ‹ç»“æœ)
plot_predictions()

# æ˜¾ç¤ºå›¾å½¢
plt.show()
```

### 1.6 æ•°æ®åŠ è½½æœ€ä½³å®è·µ

#### ä½¿ç”¨ DataLoader å¤„ç†å¤§å‹æ•°æ®é›†

```python
# å¯¼å…¥ PyTorch æ•°æ®åŠ è½½å·¥å…·
from torch.utils.data import TensorDataset, DataLoader

# åˆ›å»º Dataset å¯¹è±¡,å°†ç‰¹å¾å’Œæ ‡ç­¾å°è£…åœ¨ä¸€èµ·
# TensorDataset ä¼šè‡ªåŠ¨å°† X å’Œ y é…å¯¹,æ–¹ä¾¿æ‰¹é‡åŠ è½½
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

# è®¾ç½®æ‰¹æ¬¡å¤§å° (æ¯æ¬¡è®­ç»ƒä½¿ç”¨çš„æ ·æœ¬æ•°é‡)
BATCH_SIZE = 8

# åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(
    train_dataset,              # è¦åŠ è½½çš„æ•°æ®é›†
    batch_size=BATCH_SIZE,      # æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡
    shuffle=True,               # æ¯ä¸ª epoch å¼€å§‹æ—¶æ‰“ä¹±æ•°æ®,é¿å…æ¨¡å‹è®°ä½æ•°æ®é¡ºåº
    num_workers=2,              # ä½¿ç”¨ 2 ä¸ªå­è¿›ç¨‹å¹¶è¡ŒåŠ è½½æ•°æ®,åŠ å¿«é€Ÿåº¦
    pin_memory=True             # å°†æ•°æ®å›ºå®šåœ¨å†…å­˜ä¸­,åŠ å¿« CPU åˆ° GPU çš„ä¼ è¾“é€Ÿåº¦
)

# åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False               # æµ‹è¯•æ—¶ä¸æ‰“ä¹±æ•°æ®,ä¿æŒè¯„ä¼°çš„ä¸€è‡´æ€§
)

# æŸ¥çœ‹ä¸€ä¸ª batch çš„æ•°æ®å½¢çŠ¶
# ä½¿ç”¨ break åªæŸ¥çœ‹ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
for batch_X, batch_y in train_loader:
    print(f"Batch X shape: {batch_X.shape}")  # è¾“å‡º: torch.Size([8, 1])
    print(f"Batch y shape: {batch_y.shape}")  # è¾“å‡º: torch.Size([8, 1])
    break  # åªæŸ¥çœ‹ç¬¬ä¸€ä¸ªæ‰¹æ¬¡åé€€å‡ºå¾ªç¯
```

---

## 2. æ„å»ºæ¨¡å‹

### 2.1 PyTorch æ¨¡å‹æ„å»ºæ ¸å¿ƒç»„ä»¶

| PyTorch æ¨¡å— | åŠŸèƒ½è¯´æ˜ |
|-------------|---------|
| `torch.nn` | åŒ…å«æ„å»ºç¥ç»ç½‘ç»œçš„æ‰€æœ‰æ¨¡å— |
| `torch.nn.Parameter` | å¯è®­ç»ƒçš„å‚æ•°,ä¼šè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ |
| `torch.nn.Module` | æ‰€æœ‰ç¥ç»ç½‘ç»œçš„åŸºç±»ï¼Œç¥ç»ç½‘ç»œçš„æ‰€æœ‰æ„å»ºå—éƒ½æ˜¯å­ç±»ã€‚ |
| `torch.optim` | åŒ…å«å„ç§ä¼˜åŒ–ç®—æ³• ï¼ˆè¿™äº›ç®—æ³•å‘Šè¯‰å­˜å‚¨åœ¨å¦‚ä½•æœ€å¥½åœ°æ”¹å˜ä»¥æ”¹å–„æ¢¯åº¦ä¸‹é™ï¼Œè¿›è€Œå‡å°‘æŸå¤±ï¼‰ä¸­å­˜å‚¨çš„æ¨¡å‹å‚æ•°ã€‚|
| `def forward()` | å®šä¹‰å‰å‘ä¼ æ’­çš„è®¡ç®—è¿‡ç¨‹ |

### 2.2 çº¿æ€§å›å½’æ¨¡å‹ (æ‰‹åŠ¨å‚æ•°ç‰ˆæœ¬)

```python
import torch
from torch import nn

class LinearRegressionModel(nn.Module):
    """
    ç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹: y = wx + b
    
    è¿™æ˜¯æ‰‹åŠ¨å®šä¹‰å‚æ•°çš„ç‰ˆæœ¬ï¼Œç”¨äºç†è§£ PyTorch æ¨¡å‹çš„åŸºæœ¬æ„å»ºæ–¹å¼
    ç»§æ‰¿è‡ª nn.Moduleï¼Œè¿™æ˜¯æ‰€æœ‰ PyTorch ç¥ç»ç½‘ç»œçš„åŸºç±»
    """
    def __init__(self):
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œè¿™æ˜¯å¿…é¡»çš„
        # å®ƒä¼šè®¾ç½®æ¨¡å‹çš„åŸºç¡€åŠŸèƒ½ï¼ˆå¦‚å‚æ•°è¿½è¸ªã€è®¾å¤‡ç®¡ç†ç­‰ï¼‰
        super().__init__()

        # åˆå§‹åŒ–æƒé‡å‚æ•° (æ–œç‡ w)
        # nn.Parameter: å°†å¼ é‡æ³¨å†Œä¸ºæ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°
        # torch.randn(1): ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­éšæœºåˆå§‹åŒ–ä¸€ä¸ªå€¼
        # requires_grad=True: å‘Šè¯‰ PyTorch åœ¨åå‘ä¼ æ’­æ—¶è®¡ç®—æ­¤å‚æ•°çš„æ¢¯åº¦
        self.weight = nn.Parameter(
            torch.randn(1, dtype=torch.float),  # å½¢çŠ¶ä¸º [1]ï¼Œå•ä¸ªæµ®ç‚¹æ•°
            requires_grad=True  # å¯ç”¨æ¢¯åº¦è®¡ç®—ï¼Œä½¿å…¶å¯ä»¥é€šè¿‡ä¼˜åŒ–å™¨æ›´æ–°
        )

        # åˆå§‹åŒ–åç½®å‚æ•° (æˆªè· b)
        # åŒæ ·ä½¿ç”¨ nn.Parameter åŒ…è£…ï¼Œä½¿å…¶æˆä¸ºå¯å­¦ä¹ å‚æ•°
        self.bias = nn.Parameter(
            torch.randn(1, dtype=torch.float),  # å½¢çŠ¶ä¸º [1]ï¼Œå•ä¸ªæµ®ç‚¹æ•°
            requires_grad=True  # å¯ç”¨æ¢¯åº¦è®¡ç®—
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­: å®šä¹‰æ¨¡å‹å¦‚ä½•ä»è¾“å…¥è®¡ç®—è¾“å‡º
        
        è¿™ä¸ªæ–¹æ³•åœ¨è°ƒç”¨ model(x) æ—¶ä¼šè¢«è‡ªåŠ¨æ‰§è¡Œ
        å®ç°çº¿æ€§æ–¹ç¨‹: y = weight * x + bias

        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾å¼ é‡ï¼Œå½¢çŠ¶å¯ä»¥æ˜¯ [batch_size, 1] æˆ– [n_samples]

        è¿”å›:
            é¢„æµ‹å€¼å¼ é‡ï¼Œä¸è¾“å…¥å½¢çŠ¶ç›¸åŒ
        """
        # æ‰§è¡Œçº¿æ€§å˜æ¢: y = wx + b
        # PyTorch ä¼šè‡ªåŠ¨è¿›è¡Œå¹¿æ’­ (broadcasting)
        return self.weight * x + self.bias

# ==================== ä½¿ç”¨æ¨¡å‹ ====================

# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´ï¼ˆå¯é‡å¤æ€§ï¼‰
torch.manual_seed(42)

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model_0 = LinearRegressionModel()

# æ‰“å°æ¨¡å‹ç»“æ„ï¼Œæ˜¾ç¤ºæ¨¡å‹çš„å±‚æ¬¡å’Œå‚æ•°
print(f"æ¨¡å‹ç»“æ„:\n{model_0}")

# æ‰“å°åˆå§‹å‚æ•°å€¼ï¼ˆéšæœºåˆå§‹åŒ–çš„ï¼‰
print(f"\nåˆå§‹å‚æ•°:")
print(f"  æƒé‡ (weight): {model_0.weight}")
print(f"  åç½® (bias): {model_0.bias}")

# è·å–æ‰€æœ‰å¯å­¦ä¹ å‚æ•°çš„åˆ—è¡¨
# è¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼ŒåŒ…å«æ‰€æœ‰ requires_grad=True çš„å‚æ•°
list(model_0.parameters())

# è·å–æ¨¡å‹çš„çŠ¶æ€å­—å…¸
# è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯å‚æ•°åï¼Œå€¼æ˜¯å‚æ•°å¼ é‡
# å¸¸ç”¨äºä¿å­˜å’ŒåŠ è½½æ¨¡å‹
model_0.state_dict()
```

### 2.3 çº¿æ€§å›å½’æ¨¡å‹ (ä½¿ç”¨ nn.Linear)

```python
class LinearRegressionModelV2(nn.Module):
    """
    ä½¿ç”¨ nn.Linear çš„çº¿æ€§å›å½’æ¨¡å‹ (æ¨èæ–¹å¼)
    
    nn.Linear æ˜¯ PyTorch å†…ç½®çš„çº¿æ€§å±‚ï¼Œä¼šè‡ªåŠ¨å¤„ç†æƒé‡å’Œåç½®çš„åˆå§‹åŒ–
    è¿™æ˜¯å®é™…å¼€å‘ä¸­çš„æ ‡å‡†åšæ³•ï¼Œæ¯”æ‰‹åŠ¨å®šä¹‰å‚æ•°æ›´ç®€æ´ä¸”æ›´ä¸å®¹æ˜“å‡ºé”™
    """
    def __init__(self):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__()

        # åˆ›å»ºçº¿æ€§å±‚
        # nn.Linear ä¼šè‡ªåŠ¨åˆ›å»ºå¹¶åˆå§‹åŒ– weight å’Œ bias å‚æ•°
        # å†…éƒ¨å®ç°: y = x @ weight.T + bias
        self.linear_layer = nn.Linear(
            in_features=1,   # è¾“å…¥ç‰¹å¾çš„ç»´åº¦ï¼ˆæ¯ä¸ªæ ·æœ¬æœ‰ 1 ä¸ªç‰¹å¾ï¼‰
            out_features=1   # è¾“å‡ºç‰¹å¾çš„ç»´åº¦ï¼ˆé¢„æµ‹ 1 ä¸ªå€¼ï¼‰
        )
        # æ³¨æ„: nn.Linear çš„ weight å½¢çŠ¶æ˜¯ [out_features, in_features]
        # è¿™é‡Œæ˜¯ [1, 1]ï¼Œbias å½¢çŠ¶æ˜¯ [out_features]ï¼Œè¿™é‡Œæ˜¯ [1]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­: å°†è¾“å…¥ä¼ é€’ç»™çº¿æ€§å±‚
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾å¼ é‡
            
        è¿”å›:
            çº¿æ€§å±‚çš„è¾“å‡ºï¼ˆé¢„æµ‹å€¼ï¼‰
        """
        # ç›´æ¥è°ƒç”¨çº¿æ€§å±‚è¿›è¡Œè®¡ç®—
        # ç­‰ä»·äº: weight * x + bias
        return self.linear_layer(x)

# ==================== ä½¿ç”¨æ¨¡å‹ ====================

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
torch.manual_seed(42)

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model_1 = LinearRegressionModelV2()

# æ‰“å°æ¨¡å‹ç»“æ„
# ä¼šæ˜¾ç¤º LinearRegressionModelV2 åŒ…å«ä¸€ä¸ª Linear å±‚
print(f"æ¨¡å‹ç»“æ„:\n{model_1}")

# æ‰“å°çº¿æ€§å±‚çš„å‚æ•°ä¿¡æ¯
print(f"\nLinear å±‚å‚æ•°:")
# named_parameters() è¿”å›å‚æ•°åç§°å’Œå‚æ•°å¼ é‡çš„è¿­ä»£å™¨
for name, param in model_1.named_parameters():
    print(f"  {name}: {param.shape}")
    # è¾“å‡ºç¤ºä¾‹:
    # linear_layer.weight: torch.Size([1, 1])
    # linear_layer.bias: torch.Size([1])
```

### 2.4 æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯

```python
def model_info(model):
    """
    æ‰“å°æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
    
    è¿™ä¸ªå·¥å…·å‡½æ•°ç”¨äºæ£€æŸ¥æ¨¡å‹çš„ç»“æ„ã€å‚æ•°å’ŒçŠ¶æ€
    åœ¨è°ƒè¯•å’Œç†è§£æ¨¡å‹æ—¶éå¸¸æœ‰ç”¨
    
    å‚æ•°:
        model: PyTorch æ¨¡å‹å®ä¾‹ (nn.Module)
    """
    print("=" * 70)
    print("æ¨¡å‹ä¿¡æ¯")
    print("=" * 70)

    # ==================== 1. æ¨¡å‹ç»“æ„ ====================
    # æ‰“å°æ¨¡å‹çš„å±‚æ¬¡ç»“æ„å’Œç»„æˆ
    # ä¼šæ˜¾ç¤ºæ¨¡å‹ç±»åå’Œæ‰€æœ‰å­æ¨¡å—
    print(f"\næ¨¡å‹ç»“æ„:\n{model}\n")

    # ==================== 2. å‚æ•°è¯¦æƒ… ====================
    print("å‚æ•°è¯¦æƒ…:")
    total_params = 0  # ç”¨äºç´¯è®¡æ€»å‚æ•°æ•°é‡
    
    # named_parameters() è¿”å› (å‚æ•°å, å‚æ•°å¼ é‡) çš„è¿­ä»£å™¨
    # åªåŒ…å« requires_grad=True çš„å‚æ•°
    for name, param in model.named_parameters():
        print(f"  {name}:")  # å‚æ•°åç§°ï¼Œå¦‚ 'linear_layer.weight'
        
        # param.shape: å‚æ•°å¼ é‡çš„å½¢çŠ¶ï¼Œå¦‚ torch.Size([1, 1])
        print(f"    å½¢çŠ¶: {param.shape}")
        
        # param.numel(): å‚æ•°ä¸­å…ƒç´ çš„æ€»æ•° (number of elements)
        # ä¾‹å¦‚ [3, 4] å½¢çŠ¶çš„å¼ é‡æœ‰ 12 ä¸ªå…ƒç´ 
        print(f"    æ•°é‡: {param.numel()}")
        
        # param.requires_grad: æ˜¯å¦éœ€è¦è®¡ç®—æ¢¯åº¦
        # True è¡¨ç¤ºè¿™ä¸ªå‚æ•°ä¼šåœ¨è®­ç»ƒä¸­è¢«æ›´æ–°
        print(f"    éœ€è¦æ¢¯åº¦: {param.requires_grad}")
        
        # param.data: å‚æ•°çš„å®é™…æ•°å€¼ï¼ˆä¸å¸¦æ¢¯åº¦ä¿¡æ¯ï¼‰
        # ç›´æ¥è®¿é—®å¼ é‡çš„å€¼ï¼Œä¸ä¼šæ„å»ºè®¡ç®—å›¾
        print(f"    å½“å‰å€¼: {param.data}\n")
        
        # ç´¯åŠ å‚æ•°æ•°é‡
        total_params += param.numel()

    # æ‰“å°æ€»å‚æ•°é‡
    # å¯¹äºå¤æ‚æ¨¡å‹ï¼Œè¿™ä¸ªæ•°å­—å¯èƒ½è¾¾åˆ°æ•°ç™¾ä¸‡ç”šè‡³æ•°åäº¿
    print(f"æ€»å‚æ•°é‡: {total_params}")

    # ==================== 3. çŠ¶æ€å­—å…¸ ====================
    # state_dict() è¿”å›æ¨¡å‹æ‰€æœ‰å‚æ•°çš„å­—å…¸
    # é”®æ˜¯å‚æ•°åï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œå€¼æ˜¯å‚æ•°å¼ é‡
    # è¿™æ˜¯ä¿å­˜å’ŒåŠ è½½æ¨¡å‹çš„æ ‡å‡†æ–¹å¼
    print(f"\nçŠ¶æ€å­—å…¸:\n{model.state_dict()}")
    # è¾“å‡ºç¤ºä¾‹: OrderedDict([('linear_layer.weight', tensor([[...]]), 
    #                         ('linear_layer.bias', tensor([...]))])

    print("=" * 70)

# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================
# è°ƒç”¨å‡½æ•°æŸ¥çœ‹ model_1 çš„è¯¦ç»†ä¿¡æ¯
# è¿™ä¼šæ˜¾ç¤º LinearRegressionModelV2 çš„æ‰€æœ‰å‚æ•°å’Œç»“æ„
model_info(model_1)
```

### 2.5 ä½¿ç”¨æœªè®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹

```python
# ==================== ä½¿ç”¨æœªè®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹ ====================
# ç›®çš„: æŸ¥çœ‹éšæœºåˆå§‹åŒ–çš„æ¨¡å‹é¢„æµ‹æ•ˆæœï¼ˆä½œä¸ºåŸºçº¿å¯¹æ¯”ï¼‰

# å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
# eval() ä¼šå…³é—­æŸäº›è®­ç»ƒæ—¶æ‰éœ€è¦çš„åŠŸèƒ½ï¼ˆå¦‚ Dropoutã€BatchNormï¼‰
# è™½ç„¶è¿™ä¸ªç®€å•æ¨¡å‹æ²¡æœ‰è¿™äº›å±‚ï¼Œä½†å…»æˆå¥½ä¹ æƒ¯å¾ˆé‡è¦
model_1.eval()

# ä½¿ç”¨æ¨ç†æ¨¡å¼è¿›è¡Œé¢„æµ‹
# torch.inference_mode() æ˜¯æ¨èçš„æ¨ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨
# ä½œç”¨:
#   1. ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æº
#   2. æ¯” torch.no_grad() æ›´å¿«ï¼Œå› ä¸ºå®ƒå®Œå…¨ç¦ç”¨äº†è‡ªåŠ¨æ±‚å¯¼å¼•æ“
#   3. é€‚ç”¨äºä¸éœ€è¦åå‘ä¼ æ’­çš„åœºæ™¯ï¼ˆå¦‚é¢„æµ‹ã€éªŒè¯ï¼‰
#   4. ä»¥åŠ å¿«å‰å‘ä¼ æ’­ ï¼ˆæ•°æ®é€šè¿‡ forward() æ–¹æ³•ï¼‰çš„é€Ÿåº¦
with torch.inference_mode():
    # å°†æµ‹è¯•æ•°æ®ä¼ å…¥æ¨¡å‹è¿›è¡Œé¢„æµ‹
    # model_1(X_test) ä¼šè‡ªåŠ¨è°ƒç”¨ forward() æ–¹æ³•
    y_preds = model_1(X_test)

# æ‰“å°å‰ 5 ä¸ªé¢„æµ‹å€¼
# ç”±äºæ¨¡å‹æœªè®­ç»ƒï¼Œå‚æ•°æ˜¯éšæœºçš„ï¼Œé¢„æµ‹ç»“æœä¼šå¾ˆå·®
print(f"é¢„æµ‹å€¼ (å‰5ä¸ª):\n{y_preds[:5]}")

# æ‰“å°å‰ 5 ä¸ªçœŸå®å€¼ï¼Œç”¨äºå¯¹æ¯”
print(f"\nçœŸå®å€¼ (å‰5ä¸ª):\n{y_test[:5]}")

# ==================== å¯è§†åŒ–é¢„æµ‹ç»“æœ ====================
# è°ƒç”¨ä¹‹å‰å®šä¹‰çš„ç»˜å›¾å‡½æ•°ï¼Œå¯è§†åŒ–é¢„æµ‹æ•ˆæœ
# æœªè®­ç»ƒçš„æ¨¡å‹é¢„æµ‹åº”è¯¥æ˜¯ä¸€æ¡éšæœºçš„ç›´çº¿ï¼Œä¸çœŸå®æ•°æ®ç›¸å·®å¾ˆè¿œ
plot_predictions(predictions=y_preds)
plt.title("æœªè®­ç»ƒæ¨¡å‹çš„é¢„æµ‹ (åº”è¯¥å¾ˆå·®)")
plt.show()

# æ³¨æ„: è¿™ä¸ªå¯è§†åŒ–å±•ç¤ºäº†è®­ç»ƒçš„å¿…è¦æ€§
# é€šè¿‡å¯¹æ¯”è®­ç»ƒå‰åçš„é¢„æµ‹ï¼Œå¯ä»¥ç›´è§‚çœ‹åˆ°æ¨¡å‹å­¦ä¹ çš„æ•ˆæœ
```

### 2.6 æ¨¡å‹è®¾å¤‡ç®¡ç†

```python
# ==================== è®¾å¤‡æ£€æµ‹ ====================
# æ£€æŸ¥æ˜¯å¦æœ‰ NVIDIA GPU å¯ç”¨
# torch.cuda.is_available() è¿”å› True è¡¨ç¤ºç³»ç»Ÿæœ‰å¯ç”¨çš„ CUDA GPU
# GPU å¯ä»¥å¤§å¹…åŠ é€Ÿæ·±åº¦å­¦ä¹ è®­ç»ƒï¼ˆé€šå¸¸å¿« 10-100 å€ï¼‰
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# å…¶ä»–è®¾å¤‡é€‰é¡¹:
# - "mps": Apple Silicon (M1/M2) çš„ Metal Performance Shaders
# - "cpu": CPUï¼ˆæ‰€æœ‰ç³»ç»Ÿéƒ½æ”¯æŒï¼Œä½†é€Ÿåº¦è¾ƒæ…¢ï¼‰

# ==================== å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡ ====================
# .to(device) æ–¹æ³•å°†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°å’Œç¼“å†²åŒºç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
# è¿™æ˜¯ä¸€ä¸ªå°±åœ°æ“ä½œï¼ˆin-placeï¼‰ï¼Œä½†é€šå¸¸ä¹ æƒ¯é‡æ–°èµ‹å€¼
# æ³¨æ„: æ¨¡å‹å’Œæ•°æ®å¿…é¡»åœ¨åŒä¸€è®¾å¤‡ä¸Šæ‰èƒ½è¿›è¡Œè®¡ç®—
model_1 = model_1.to(device)

# ==================== å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ ====================
# åŒæ ·éœ€è¦å°†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ç§»åŠ¨åˆ°ç›¸åŒè®¾å¤‡
# å¦‚æœæ¨¡å‹åœ¨ GPU ä¸Šï¼Œæ•°æ®ä¹Ÿå¿…é¡»åœ¨ GPU ä¸Š
# .to() ä¼šåˆ›å»ºæ•°æ®çš„å‰¯æœ¬å¹¶ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡

# ç§»åŠ¨è®­ç»ƒæ•°æ®
X_train = X_train.to(device)
y_train = y_train.to(device)

# ç§»åŠ¨æµ‹è¯•æ•°æ®
X_test = X_test.to(device)
y_test = y_test.to(device)

# ==================== éªŒè¯è®¾å¤‡ä½ç½® ====================
# æ£€æŸ¥æ¨¡å‹å‚æ•°æ‰€åœ¨çš„è®¾å¤‡
# next(model_1.parameters()) è·å–æ¨¡å‹çš„ç¬¬ä¸€ä¸ªå‚æ•°
# .device å±æ€§æ˜¾ç¤ºå¼ é‡æ‰€åœ¨çš„è®¾å¤‡
print(f"æ¨¡å‹è®¾å¤‡: {next(model_1.parameters()).device}")

# æ£€æŸ¥æ•°æ®æ‰€åœ¨çš„è®¾å¤‡
# ç¡®ä¿æ¨¡å‹å’Œæ•°æ®åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼Œå¦åˆ™ä¼šæŠ¥é”™
print(f"æ•°æ®è®¾å¤‡: {X_train.device}")

# å¸¸è§é”™è¯¯: RuntimeError: Expected all tensors to be on the same device
# è§£å†³æ–¹æ³•: ç¡®ä¿æ¨¡å‹å’Œæ‰€æœ‰è¾“å…¥æ•°æ®éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Š
```

---

## 3. è®­ç»ƒæ¨¡å‹

### 3.1 æŸå¤±å‡½æ•°

**æŸå¤±å‡½æ•° (Loss Function):** è¡¡é‡æ¨¡å‹é¢„æµ‹ä¸çœŸå®å€¼ä¹‹é—´çš„å·®è·

#### å¸¸ç”¨æŸå¤±å‡½æ•°

| æŸå¤±å‡½æ•° | PyTorch å®ç° | é€‚ç”¨åœºæ™¯ |
|---------|-------------|---------|
| **å‡æ–¹è¯¯å·® (MSE)** | `nn.MSELoss()` | å›å½’é—®é¢˜ (å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ) |
| **å¹³å‡ç»å¯¹è¯¯å·® (MAE)** | `nn.L1Loss()` | å›å½’é—®é¢˜ (å¯¹å¼‚å¸¸å€¼é²æ£’) |
| **äº¤å‰ç†µæŸå¤±** | `nn.CrossEntropyLoss()` | å¤šåˆ†ç±»é—®é¢˜ |
| **äºŒå…ƒäº¤å‰ç†µ** | `nn.BCELoss()` | äºŒåˆ†ç±»é—®é¢˜ |
| **äºŒå…ƒäº¤å‰ç†µ (å¸¦ logits)** | `nn.BCEWithLogitsLoss()` | äºŒåˆ†ç±» (æ›´ç¨³å®š) |

```python
# åˆ›å»ºæŸå¤±å‡½æ•°
loss_fn = nn.L1Loss()  # MAE for regression

# æ‰‹åŠ¨è®¡ç®—æŸå¤±ç¤ºä¾‹
with torch.inference_mode():
    y_pred = model_1(X_train)
    loss = loss_fn(y_pred, y_train)
    print(f"åˆå§‹æŸå¤±: {loss}")
```

### 3.2 ä¼˜åŒ–å™¨

**ä¼˜åŒ–å™¨ (Optimizer):** ä½¿ç”¨æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°

#### å¸¸ç”¨ä¼˜åŒ–å™¨

| ä¼˜åŒ–å™¨ | PyTorch å®ç° | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|-------|-------------|-----|---------|
| **SGD** | `torch.optim.SGD()` | æœ€åŸºç¡€,ç¨³å®š | ç®€å•ä»»åŠ¡ |
| **Adam** | `torch.optim.Adam()` | è‡ªé€‚åº”å­¦ä¹ ç‡,æœ€æµè¡Œ | å¤§å¤šæ•°ä»»åŠ¡ (2024æ¨è) |
| **AdamW** | `torch.optim.AdamW()` | Adam + æƒé‡è¡°å‡ | NLP, Transformers (2024æ¨è) |
| **RMSprop** | `torch.optim.RMSprop()` | é€‚åº”å­¦ä¹ ç‡ | RNN |
| **Adagrad** | `torch.optim.Adagrad()` | ç¨€ç–æ¢¯åº¦ | ç¨€ç–æ•°æ® |

```python
# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = torch.optim.SGD(
    params=model_1.parameters(),  # è¦ä¼˜åŒ–çš„å‚æ•°
    lr=0.01  # å­¦ä¹ ç‡ (learning rate) - é‡è¦çš„è¶…å‚æ•°
)

# æŸ¥çœ‹ä¼˜åŒ–å™¨ä¿¡æ¯
print(f"ä¼˜åŒ–å™¨: {optimizer}")
print(f"å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")
```

### 3.3 è®­ç»ƒå¾ªç¯ (æ ¸å¿ƒ)

#### è®­ç»ƒå¾ªç¯çš„æ ‡å‡†æ­¥éª¤

```
è®­ç»ƒé˜¶æ®µ:
1. å‰å‘ä¼ æ’­ (Forward pass) - è®¡ç®—é¢„æµ‹
2. è®¡ç®—æŸå¤± (Calculate loss) - è¡¡é‡é¢„æµ‹è´¨é‡
3. æ¸…é›¶æ¢¯åº¦ (Zero gradients) - æ¸…é™¤ä¸Šä¸€æ­¥çš„æ¢¯åº¦
4. åå‘ä¼ æ’­ (Backpropagation) - è®¡ç®—æ¢¯åº¦
5. æ›´æ–°å‚æ•° (Optimizer step) - ä½¿ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°

è¯„ä¼°é˜¶æ®µ:
1. å‰å‘ä¼ æ’­ (ä¸è®¡ç®—æ¢¯åº¦)
2. è®¡ç®—æŸå¤±
```

#### åŸºç¡€è®­ç»ƒå¾ªç¯å®ç°

```python
# è®­ç»ƒè®¾ç½®
epochs = 100

# ç”¨äºè®°å½•
epoch_count = []
train_loss_values = []
test_loss_values = []

print("å¼€å§‹è®­ç»ƒ...")
print(f"{'Epoch':<6} {'è®­ç»ƒæŸå¤±':<12} {'æµ‹è¯•æŸå¤±':<12}")
print("-" * 35)

for epoch in range(epochs):
    ### è®­ç»ƒæ¨¡å¼ ###
    model_1.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

    # 1. å‰å‘ä¼ æ’­
    y_pred = model_1(X_train)

    # 2. è®¡ç®—æŸå¤±
    loss = loss_fn(y_pred, y_train)

    # 3. æ¸…é›¶æ¢¯åº¦
    optimizer.zero_grad()

    # 4. åå‘ä¼ æ’­ (è®¡ç®—æ¢¯åº¦)
    loss.backward()

    # 5. æ›´æ–°å‚æ•°
    optimizer.step()

    ### è¯„ä¼°æ¨¡å¼ ###
    model_1.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    with torch.inference_mode():
        # 1. å‰å‘ä¼ æ’­
        test_pred = model_1(X_test)

        # 2. è®¡ç®—æµ‹è¯•æŸå¤±
        test_loss = loss_fn(test_pred, y_test)

    # è®°å½•æŸå¤±
    if epoch % 10 == 0:
        epoch_count.append(epoch)
        train_loss_values.append(loss.item())
        test_loss_values.append(test_loss.item())

        print(f"{epoch:<6} {loss.item():<12.4f} {test_loss.item():<12.4f}")

print("\nè®­ç»ƒå®Œæˆ!")
```

### 3.4 å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹

```python
def plot_loss_curves(epoch_count, train_loss, test_loss):
    """ç»˜åˆ¶è®­ç»ƒå’Œæµ‹è¯•æŸå¤±æ›²çº¿"""
    plt.figure(figsize=(10, 7))

    plt.plot(epoch_count, train_loss, label="è®­ç»ƒæŸå¤±", color="blue")
    plt.plot(epoch_count, test_loss, label="æµ‹è¯•æŸå¤±", color="orange")

    plt.title("è®­ç»ƒå’Œæµ‹è¯•æŸå¤±æ›²çº¿")
    plt.xlabel("Epoch")
    plt.ylabel("æŸå¤±")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

# ç»˜åˆ¶æŸå¤±æ›²çº¿
plot_loss_curves(epoch_count, train_loss_values, test_loss_values)
```

### 3.5 å®Œæ•´çš„è®­ç»ƒå‡½æ•° (ç”Ÿäº§çº§)

```python
def train_model(model,
                train_loader,
                test_loader,
                loss_fn,
                optimizer,
                epochs,
                device="cpu",
                print_every=10):
    """
    è®­ç»ƒ PyTorch æ¨¡å‹çš„å®Œæ•´å‡½æ•°

    å‚æ•°:
        model: PyTorch æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        loss_fn: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        epochs: è®­ç»ƒè½®æ•°
        device: è®­ç»ƒè®¾å¤‡ (cpu/cuda)
        print_every: æ‰“å°é¢‘ç‡

    è¿”å›:
        results: åŒ…å«æŸå¤±å†å²çš„å­—å…¸
    """
    # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
    model = model.to(device)

    # è®°å½•å†å²
    results = {
        "train_loss": [],
        "test_loss": [],
        "epoch": []
    }

    for epoch in range(epochs):
        ### è®­ç»ƒé˜¶æ®µ ###
        model.train()
        train_loss = 0

        for batch, (X, y) in enumerate(train_loader):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            X, y = X.to(device), y.to(device)

            # 1. å‰å‘ä¼ æ’­
            y_pred = model(X)

            # 2. è®¡ç®—æŸå¤±
            loss = loss_fn(y_pred, y)
            train_loss += loss.item()

            # 3. æ¸…é›¶æ¢¯åº¦
            optimizer.zero_grad()

            # 4. åå‘ä¼ æ’­
            loss.backward()

            # 5. æ›´æ–°å‚æ•°
            optimizer.step()

        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        train_loss /= len(train_loader)

        ### æµ‹è¯•é˜¶æ®µ ###
        model.eval()
        test_loss = 0

        with torch.inference_mode():
            for X, y in test_loader:
                X, y = X.to(device), y.to(device)

                # å‰å‘ä¼ æ’­
                test_pred = model(X)

                # è®¡ç®—æŸå¤±
                loss = loss_fn(test_pred, y)
                test_loss += loss.item()

        # è®¡ç®—å¹³å‡æµ‹è¯•æŸå¤±
        test_loss /= len(test_loader)

        # è®°å½•ç»“æœ
        if epoch % print_every == 0:
            results["train_loss"].append(train_loss)
            results["test_loss"].append(test_loss)
            results["epoch"].append(epoch)

            print(f"Epoch: {epoch} | "
                  f"Train loss: {train_loss:.5f} | "
                  f"Test loss: {test_loss:.5f}")

    return results
```

---

## 4. æ¨¡å‹è¯„ä¼°ä¸é¢„æµ‹

### 4.1 æ¨ç†æ¨¡å¼ (Inference Mode)

```python
# æ–¹æ³• 1: torch.inference_mode() (æ¨è,æ›´å¿«)
with torch.inference_mode():
    y_preds = model_1(X_test)

# æ–¹æ³• 2: torch.no_grad() (æ—§ç‰ˆ,ä»ç„¶æ”¯æŒ)
with torch.no_grad():
    y_preds = model_1(X_test)
```

**ä¸ºä»€ä¹ˆä½¿ç”¨æ¨ç†æ¨¡å¼?**
- âœ… å…³é—­æ¢¯åº¦è¿½è¸ª (èŠ‚çœå†…å­˜)
- âœ… åŠ å¿«å‰å‘ä¼ æ’­é€Ÿåº¦
- âœ… `torch.inference_mode()` æ¯” `torch.no_grad()` æ›´å¿«

### 4.2 è¯„ä¼°æ¨¡å‹æ€§èƒ½

```python
# è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
model_1.eval()

with torch.inference_mode():
    # è¿›è¡Œé¢„æµ‹
    y_preds = model_1(X_test)

    # è®¡ç®—æœ€ç»ˆæµ‹è¯•æŸå¤±
    final_loss = loss_fn(y_preds, y_test)
    print(f"æœ€ç»ˆæµ‹è¯•æŸå¤±: {final_loss:.4f}")

# æŸ¥çœ‹å­¦åˆ°çš„å‚æ•°
print(f"\nå­¦åˆ°çš„å‚æ•°:")
print(f"  æƒé‡: {model_1.state_dict()['linear_layer.weight'].item():.4f}")
print(f"  åç½®: {model_1.state_dict()['linear_layer.bias'].item():.4f}")

print(f"\nçœŸå®å‚æ•°:")
print(f"  æƒé‡: {weight}")
print(f"  åç½®: {bias}")

# å¯è§†åŒ–é¢„æµ‹ç»“æœ
plot_predictions(predictions=y_preds)
plt.title("è®­ç»ƒåçš„é¢„æµ‹ç»“æœ")
plt.show()
```

---

## 5. ä¿å­˜ä¸åŠ è½½æ¨¡å‹

### 5.1 PyTorch æ¨¡å‹ä¿å­˜æ–¹æ³•

PyTorch æä¾›ä¸‰ç§ä¸»è¦æ–¹æ³•:

1. **`torch.save()`** - ä¿å­˜å¯¹è±¡åˆ°ç£ç›˜
2. **`torch.load()`** - ä»ç£ç›˜åŠ è½½å¯¹è±¡
3. **`model.load_state_dict()`** - åŠ è½½æ¨¡å‹å‚æ•°å­—å…¸

### 5.2 ä¿å­˜å’ŒåŠ è½½ state_dict (æ¨è)

```python
from pathlib import Path

# 1. åˆ›å»ºæ¨¡å‹ç›®å½•
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)

# 2. å®šä¹‰ä¿å­˜è·¯å¾„
MODEL_NAME = "01_pytorch_workflow_model.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# 3. ä¿å­˜æ¨¡å‹ state_dict
print(f"ä¿å­˜æ¨¡å‹åˆ°: {MODEL_SAVE_PATH}")
torch.save(obj=model_1.state_dict(),  # åªä¿å­˜å‚æ•°
           f=MODEL_SAVE_PATH)

# 4. åŠ è½½æ¨¡å‹
# é¦–å…ˆåˆ›å»ºæ¨¡å‹å®ä¾‹
loaded_model = LinearRegressionModelV2()

# åŠ è½½ä¿å­˜çš„å‚æ•°
loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

# 5. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
loaded_model.eval()

# 6. éªŒè¯åŠ è½½çš„æ¨¡å‹
with torch.inference_mode():
    loaded_preds = loaded_model(X_test)

# æ£€æŸ¥é¢„æµ‹æ˜¯å¦ä¸€è‡´
print(f"åŸå§‹æ¨¡å‹é¢„æµ‹: {y_preds[:3]}")
print(f"åŠ è½½æ¨¡å‹é¢„æµ‹: {loaded_preds[:3]}")
print(f"é¢„æµ‹æ˜¯å¦ç›¸åŒ: {torch.allclose(y_preds, loaded_preds)}")
```

### 5.3 ä¿å­˜å®Œæ•´æ¨¡å‹ (ä¸æ¨èä½†æœ‰æ—¶æœ‰ç”¨)

```python
# ä¿å­˜å®Œæ•´æ¨¡å‹
FULL_MODEL_PATH = MODEL_PATH / "full_model.pth"
torch.save(model_1, FULL_MODEL_PATH)

# åŠ è½½å®Œæ•´æ¨¡å‹
loaded_full_model = torch.load(FULL_MODEL_PATH)
loaded_full_model.eval()
```

### 5.4 ä¿å­˜å’ŒåŠ è½½æ£€æŸ¥ç‚¹ (Checkpoint)

```python
def save_checkpoint(model, optimizer, epoch, loss, filepath):
    """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }
    torch.save(checkpoint, filepath)
    print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ° {filepath}")

def load_checkpoint(filepath, model, optimizer):
    """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
    checkpoint = torch.load(filepath)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']

    return model, optimizer, epoch, loss

# ä½¿ç”¨ç¤ºä¾‹
# ä¿å­˜
save_checkpoint(model_1, optimizer, epoch=100, loss=final_loss,
                filepath=MODEL_PATH / "checkpoint.pth")

# åŠ è½½
model_1, optimizer, start_epoch, loss = load_checkpoint(
    MODEL_PATH / "checkpoint.pth", model_1, optimizer
)
```

### 5.5 æœ€ä½³å®è·µæ€»ç»“

| åœºæ™¯ | æ¨èæ–¹æ³• | åŸå›  |
|-----|---------|-----|
| **ç”Ÿäº§éƒ¨ç½²** | `state_dict()` | æ›´çµæ´»,å¯ç§»æ¤æ€§å¥½ |
| **ä¸­æ–­è®­ç»ƒ** | Checkpoint | å¯æ¢å¤è®­ç»ƒçŠ¶æ€ |
| **å¿«é€ŸåŸå‹** | å®Œæ•´æ¨¡å‹ | ç®€å•å¿«é€Ÿ |
| **è·¨ç‰ˆæœ¬** | `state_dict()` | é¿å… PyTorch ç‰ˆæœ¬é—®é¢˜ |

---

## 6. å®Œæ•´æµç¨‹æ•´åˆ

### 6.1 ç«¯åˆ°ç«¯ç¤ºä¾‹ä»£ç 

```python
"""
å®Œæ•´çš„ PyTorch çº¿æ€§å›å½’å·¥ä½œæµç¨‹
"""
import torch
from torch import nn
import matplotlib.pyplot as plt
from pathlib import Path

# ============ 1. å‡†å¤‡æ•°æ® ============
print("1. å‡†å¤‡æ•°æ®...")
weight = 0.7
bias = 0.3

X = torch.arange(0, 1, 0.02).unsqueeze(dim=1)
y = weight * X + bias

train_split = int(0.8 * len(X))
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

print(f"   è®­ç»ƒæ ·æœ¬: {len(X_train)}, æµ‹è¯•æ ·æœ¬: {len(X_test)}")

# ============ 2. æ„å»ºæ¨¡å‹ ============
print("\n2. æ„å»ºæ¨¡å‹...")

class LinearRegressionModelV2(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_layer = nn.Linear(in_features=1, out_features=1)

    def forward(self, x):
        return self.linear_layer(x)

torch.manual_seed(42)
model = LinearRegressionModelV2()
print(f"   æ¨¡å‹: {model}")

# ============ 3. è®¾ç½®æŸå¤±å’Œä¼˜åŒ–å™¨ ============
print("\n3. è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨...")
loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
print(f"   æŸå¤±å‡½æ•°: {loss_fn}")
print(f"   ä¼˜åŒ–å™¨: {optimizer.__class__.__name__}")

# ============ 4. è®­ç»ƒæ¨¡å‹ ============
print("\n4. å¼€å§‹è®­ç»ƒ...")
epochs = 200

for epoch in range(epochs):
    # è®­ç»ƒæ¨¡å¼
    model.train()
    y_pred = model(X_train)
    loss = loss_fn(y_pred, y_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # è¯„ä¼°æ¨¡å¼
    model.eval()
    with torch.inference_mode():
        test_pred = model(X_test)
        test_loss = loss_fn(test_pred, y_test)

    if epoch % 50 == 0:
        print(f"   Epoch {epoch}: Train Loss = {loss:.4f}, Test Loss = {test_loss:.4f}")

# ============ 5. è¯„ä¼°æ¨¡å‹ ============
print("\n5. è¯„ä¼°æ¨¡å‹...")
model.eval()
with torch.inference_mode():
    y_preds = model(X_test)
    final_loss = loss_fn(y_preds, y_test)

print(f"   æœ€ç»ˆæµ‹è¯•æŸå¤±: {final_loss:.4f}")
print(f"   å­¦åˆ°çš„æƒé‡: {model.state_dict()['linear_layer.weight'].item():.4f}")
print(f"   å­¦åˆ°çš„åç½®: {model.state_dict()['linear_layer.bias'].item():.4f}")

# ============ 6. ä¿å­˜æ¨¡å‹ ============
print("\n6. ä¿å­˜æ¨¡å‹...")
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(exist_ok=True)
SAVE_PATH = MODEL_PATH / "linear_model.pth"
torch.save(model.state_dict(), SAVE_PATH)
print(f"   æ¨¡å‹å·²ä¿å­˜åˆ°: {SAVE_PATH}")

print("\nâœ… å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæ¯•!")
```

---

# ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜çº§æŠ€æœ¯ä¸æœ€ä½³å®è·µ

## 7. è®­ç»ƒå¾ªç¯ä¼˜åŒ–æŠ€æœ¯

### 7.1 è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)

**ä¼˜åŠ¿:**
- âœ… è®­ç»ƒé€Ÿåº¦æå‡ 1.5-3å€
- âœ… GPU å†…å­˜å‡åŠ
- âœ… ä¿æŒç›¸åŒçš„å‡†ç¡®ç‡

```python
from torch.cuda.amp import autocast, GradScaler

# åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨
scaler = GradScaler()

for epoch in range(epochs):
    model.train()

    for batch, (X, y) in enumerate(train_loader):
        X, y = X.to(device), y.to(device)

        # ä½¿ç”¨æ··åˆç²¾åº¦
        with autocast():
            y_pred = model(X)
            loss = loss_fn(y_pred, y)

        optimizer.zero_grad()

        # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
        scaler.scale(loss).backward()

        # æ›´æ–°å‚æ•°
        scaler.step(optimizer)
        scaler.update()
```

### 7.2 æ¢¯åº¦ç´¯ç§¯ (Gradient Accumulation)

**ç”¨é€”:** æ¨¡æ‹Ÿæ›´å¤§çš„ batch size (å½“ GPU å†…å­˜ä¸è¶³æ—¶)

```python
# æ¨¡æ‹Ÿ batch size = 32 (å®é™… batch size = 8)
ACCUMULATION_STEPS = 4

optimizer.zero_grad()

for i, (X, y) in enumerate(train_loader):
    X, y = X.to(device), y.to(device)

    # å‰å‘ä¼ æ’­
    y_pred = model(X)
    loss = loss_fn(y_pred, y)

    # å½’ä¸€åŒ–æŸå¤±
    loss = loss / ACCUMULATION_STEPS

    # åå‘ä¼ æ’­
    loss.backward()

    # æ¯ ACCUMULATION_STEPS æ›´æ–°ä¸€æ¬¡å‚æ•°
    if (i + 1) % ACCUMULATION_STEPS == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 7.3 å­¦ä¹ ç‡è°ƒåº¦å™¨ (Learning Rate Scheduler)

```python
from torch.optim.lr_scheduler import (
    StepLR,
    ReduceLROnPlateau,
    CosineAnnealingLR
)

# æ–¹æ³• 1: StepLR - æ¯ N ä¸ª epoch é™ä½å­¦ä¹ ç‡
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

# æ–¹æ³• 2: ReduceLROnPlateau - å½“æŒ‡æ ‡åœæ­¢æ”¹å–„æ—¶é™ä½å­¦ä¹ ç‡
scheduler = ReduceLROnPlateau(optimizer, mode='min',
                             factor=0.1, patience=10)

# æ–¹æ³• 3: CosineAnnealingLR - ä½™å¼¦é€€ç«
scheduler = CosineAnnealingLR(optimizer, T_max=epochs)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for epoch in range(epochs):
    # ... è®­ç»ƒä»£ç  ...

    # StepLR / CosineAnnealingLR
    scheduler.step()

    # ReduceLROnPlateau (éœ€è¦ç›‘æ§æŒ‡æ ‡)
    # scheduler.step(val_loss)

    # æ‰“å°å½“å‰å­¦ä¹ ç‡
    current_lr = optimizer.param_groups[0]['lr']
    print(f"Epoch {epoch}, LR: {current_lr}")
```

### 7.4 æ¢¯åº¦è£å‰ª (Gradient Clipping)

**ç”¨é€”:** é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

```python
import torch.nn.utils as nn_utils

MAX_GRAD_NORM = 1.0

for epoch in range(epochs):
    for X, y in train_loader:
        optimizer.zero_grad()

        y_pred = model(X)
        loss = loss_fn(y_pred, y)
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        nn_utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)

        optimizer.step()
```

### 7.5 æ—©åœ (Early Stopping)

```python
class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=7, min_delta=0):
        """
        å‚æ•°:
            patience: å®¹å¿å¤šå°‘ä¸ª epoch æ²¡æœ‰æ”¹å–„
            min_delta: æœ€å°æ”¹å–„é‡
        """
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            print(f"EarlyStopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# ä½¿ç”¨ç¤ºä¾‹
early_stopping = EarlyStopping(patience=10, min_delta=0.001)

for epoch in range(epochs):
    # ... è®­ç»ƒå’ŒéªŒè¯ ...

    early_stopping(val_loss)

    if early_stopping.early_stop:
        print("Early stopping triggered!")
        break
```

---

## 8. æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

### 8.1 å›å½’é—®é¢˜æŒ‡æ ‡

```python
def regression_metrics(y_true, y_pred):
    """è®¡ç®—å›å½’é—®é¢˜çš„å¸¸ç”¨æŒ‡æ ‡"""
    # è½¬æ¢ä¸º numpy æ•°ç»„
    y_true = y_true.cpu().numpy()
    y_pred = y_pred.cpu().numpy()

    # MAE (Mean Absolute Error)
    mae = np.mean(np.abs(y_true - y_pred))

    # MSE (Mean Squared Error)
    mse = np.mean((y_true - y_pred) ** 2)

    # RMSE (Root Mean Squared Error)
    rmse = np.sqrt(mse)

    # RÂ² Score
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    r2 = 1 - (ss_res / ss_tot)

    return {
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'RÂ²': r2
    }

# ä½¿ç”¨ç¤ºä¾‹
model.eval()
with torch.inference_mode():
    y_pred = model(X_test)

metrics = regression_metrics(y_test, y_pred)
print("æ¨¡å‹è¯„ä¼°æŒ‡æ ‡:")
for name, value in metrics.items():
    print(f"  {name}: {value:.4f}")
```

### 8.2 ä½¿ç”¨ torchmetrics åº“

```python
# å®‰è£…: pip install torchmetrics

from torchmetrics import MeanAbsoluteError, MeanSquaredError, R2Score

# åˆ›å»ºæŒ‡æ ‡
mae_metric = MeanAbsoluteError()
mse_metric = MeanSquaredError()
r2_metric = R2Score()

# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ›´æ–°
for X, y in test_loader:
    with torch.inference_mode():
        y_pred = model(X)

    # æ›´æ–°æŒ‡æ ‡
    mae_metric.update(y_pred, y)
    mse_metric.update(y_pred, y)
    r2_metric.update(y_pred, y)

# è®¡ç®—æœ€ç»ˆå€¼
print(f"MAE: {mae_metric.compute():.4f}")
print(f"MSE: {mse_metric.compute():.4f}")
print(f"RÂ²: {r2_metric.compute():.4f}")
```

---

## 9. è°ƒè¯•ä¸ç›‘æ§

### 9.1 PyTorch Profiler

```python
from torch.profiler import profile, ProfilerActivity

with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
            record_shapes=True) as prof:
    for _ in range(10):
        y_pred = model(X_train)
        loss = loss_fn(y_pred, y_train)
        loss.backward()

# æ‰“å°ç»Ÿè®¡ä¿¡æ¯
print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))
```

### 9.2 ä½¿ç”¨ TensorBoard

```python
from torch.utils.tensorboard import SummaryWriter

# åˆ›å»º writer
writer = SummaryWriter('runs/linear_regression')

for epoch in range(epochs):
    # ... è®­ç»ƒ ...

    # è®°å½•æŸå¤±
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/test', test_loss, epoch)

    # è®°å½•å­¦ä¹ ç‡
    writer.add_scalar('Learning Rate',
                     optimizer.param_groups[0]['lr'], epoch)

    # è®°å½•å‚æ•°ç›´æ–¹å›¾
    for name, param in model.named_parameters():
        writer.add_histogram(name, param, epoch)

writer.close()

# åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir=runs
```

### 9.3 ä½¿ç”¨ Weights & Biases (æ¨è)

```python
# å®‰è£…: pip install wandb

import wandb

# åˆå§‹åŒ–
wandb.init(project="pytorch-linear-regression",
          config={
              "learning_rate": 0.01,
              "epochs": 100,
              "batch_size": 8
          })

# è®­ç»ƒå¾ªç¯
for epoch in range(epochs):
    # ... è®­ç»ƒ ...

    # è®°å½•æŒ‡æ ‡
    wandb.log({
        "epoch": epoch,
        "train_loss": train_loss,
        "test_loss": test_loss,
        "learning_rate": optimizer.param_groups[0]['lr']
    })

# ä¿å­˜æ¨¡å‹åˆ° W&B
wandb.save("model.pth")
```

---

## 10. ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ

### 10.1 æ¨¡å‹å¯¼å‡ºä¸º ONNX

```python
# å¯¼å‡ºæ¨¡å‹ä¸º ONNX æ ¼å¼ (è·¨å¹³å°æ¨ç†)
dummy_input = torch.randn(1, 1)

torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    export_params=True,
    opset_version=11,
    input_names=['input'],
    output_names=['output']
)

print("æ¨¡å‹å·²å¯¼å‡ºä¸º ONNX æ ¼å¼")
```

### 10.2 æ¨¡å‹é‡åŒ– (åŠ é€Ÿæ¨ç†)

```python
# åŠ¨æ€é‡åŒ–
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear},  # è¦é‡åŒ–çš„å±‚ç±»å‹
    dtype=torch.qint8
)

# æµ‹è¯•é‡åŒ–æ¨¡å‹
with torch.inference_mode():
    quantized_pred = quantized_model(X_test)

print(f"åŸå§‹æ¨¡å‹å¤§å°: {os.path.getsize('model.pth') / 1024:.2f} KB")
torch.save(quantized_model.state_dict(), 'quantized_model.pth')
print(f"é‡åŒ–æ¨¡å‹å¤§å°: {os.path.getsize('quantized_model.pth') / 1024:.2f} KB")
```

### 10.3 æ¨¡å‹æœåŠ¡åŒ– (ä½¿ç”¨ FastAPI)

```python
# api.py
from fastapi import FastAPI
from pydantic import BaseModel
import torch

app = FastAPI()

# åŠ è½½æ¨¡å‹
model = LinearRegressionModelV2()
model.load_state_dict(torch.load('model.pth'))
model.eval()

class PredictionRequest(BaseModel):
    value: float

class PredictionResponse(BaseModel):
    prediction: float

@app.post("/predict", response_model=PredictionResponse)
def predict(request: PredictionRequest):
    with torch.inference_mode():
        X = torch.tensor([[request.value]])
        pred = model(X)

    return PredictionResponse(prediction=pred.item())

# è¿è¡Œ: uvicorn api:app --reload
```

---

# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®æˆ˜é¡¹ç›®

## 11. å®Œæ•´é¡¹ç›®ç¤ºä¾‹

### 11.1 é¡¹ç›®ç»“æ„

```
pytorch_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ checkpoints/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data.py        # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ model.py       # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ train.py       # è®­ç»ƒé€»è¾‘
â”‚   â”œâ”€â”€ evaluate.py    # è¯„ä¼°é€»è¾‘
â”‚   â””â”€â”€ utils.py       # å·¥å…·å‡½æ•°
â”œâ”€â”€ config.yaml        # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt   # ä¾èµ–
â””â”€â”€ main.py           # ä¸»å…¥å£
```

### 11.2 é…ç½®æ–‡ä»¶ (config.yaml)

```yaml
# æ•°æ®é…ç½®
data:
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  batch_size: 32

# æ¨¡å‹é…ç½®
model:
  input_features: 1
  output_features: 1

# è®­ç»ƒé…ç½®
training:
  epochs: 100
  learning_rate: 0.01
  optimizer: "Adam"
  loss_function: "MSE"

# è®¾å¤‡é…ç½®
device: "cuda"  # or "cpu"

# ä¿å­˜é…ç½®
save:
  model_dir: "models"
  checkpoint_dir: "models/checkpoints"
```

### 11.3 å®Œæ•´ä»£ç å®ç°

#### src/data.py

```python
"""æ•°æ®å¤„ç†æ¨¡å—"""
import torch
from torch.utils.data import TensorDataset, DataLoader, random_split

def create_linear_data(weight=0.7, bias=0.3, num_samples=1000):
    """åˆ›å»ºçº¿æ€§æ•°æ®"""
    X = torch.linspace(0, 1, num_samples).unsqueeze(1)
    y = weight * X + bias
    # æ·»åŠ ä¸€äº›å™ªå£°
    y = y + torch.randn_like(y) * 0.02
    return X, y

def prepare_dataloaders(X, y, train_ratio=0.7, val_ratio=0.15,
                       batch_size=32, num_workers=2):
    """å‡†å¤‡æ•°æ®åŠ è½½å™¨"""
    # åˆ›å»º dataset
    dataset = TensorDataset(X, y)

    # è®¡ç®—åˆ†å‰²å¤§å°
    n = len(dataset)
    train_size = int(n * train_ratio)
    val_size = int(n * val_ratio)
    test_size = n - train_size - val_size

    # åˆ†å‰²æ•°æ®é›†
    train_ds, val_ds, test_ds = random_split(
        dataset,
        [train_size, val_size, test_size]
    )

    # åˆ›å»º DataLoader
    train_loader = DataLoader(train_ds, batch_size=batch_size,
                            shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_ds, batch_size=batch_size,
                          shuffle=False, num_workers=num_workers)
    test_loader = DataLoader(test_ds, batch_size=batch_size,
                           shuffle=False, num_workers=num_workers)

    return train_loader, val_loader, test_loader
```

#### src/model.py

```python
"""æ¨¡å‹å®šä¹‰æ¨¡å—"""
import torch
from torch import nn

class LinearRegressionModel(nn.Module):
    """çº¿æ€§å›å½’æ¨¡å‹"""
    def __init__(self, input_features=1, output_features=1):
        super().__init__()
        self.linear = nn.Linear(input_features, output_features)

    def forward(self, x):
        return self.linear(x)

def create_model(config, device="cpu"):
    """æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹"""
    model = LinearRegressionModel(
        input_features=config['model']['input_features'],
        output_features=config['model']['output_features']
    )
    return model.to(device)
```

#### src/train.py

```python
"""è®­ç»ƒæ¨¡å—"""
import torch
from tqdm.auto import tqdm

def train_epoch(model, dataloader, loss_fn, optimizer, device):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0

    for X, y in dataloader:
        X, y = X.to(device), y.to(device)

        # å‰å‘ä¼ æ’­
        y_pred = model(X)
        loss = loss_fn(y_pred, y)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)

def validate(model, dataloader, loss_fn, device):
    """éªŒè¯å‡½æ•°"""
    model.eval()
    total_loss = 0

    with torch.inference_mode():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            y_pred = model(X)
            loss = loss_fn(y_pred, y)
            total_loss += loss.item()

    return total_loss / len(dataloader)

def train(model, train_loader, val_loader, config, device):
    """å®Œæ•´è®­ç»ƒæµç¨‹"""
    # è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    loss_fn = nn.MSELoss()
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=config['training']['learning_rate']
    )

    # è®­ç»ƒå¾ªç¯
    epochs = config['training']['epochs']
    history = {'train_loss': [], 'val_loss': []}

    for epoch in tqdm(range(epochs), desc="Training"):
        train_loss = train_epoch(model, train_loader, loss_fn,
                                optimizer, device)
        val_loss = validate(model, val_loader, loss_fn, device)

        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}, "
                  f"Val Loss = {val_loss:.4f}")

    return history
```

#### src/evaluate.py

```python
"""è¯„ä¼°æ¨¡å—"""
import torch
import numpy as np

def evaluate_model(model, dataloader, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()

    all_preds = []
    all_targets = []

    with torch.inference_mode():
        for X, y in dataloader:
            X = X.to(device)
            y_pred = model(X)

            all_preds.append(y_pred.cpu())
            all_targets.append(y)

    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    predictions = torch.cat(all_preds)
    targets = torch.cat(all_targets)

    # è®¡ç®—æŒ‡æ ‡
    mae = torch.mean(torch.abs(predictions - targets))
    mse = torch.mean((predictions - targets) ** 2)
    rmse = torch.sqrt(mse)

    return {
        'MAE': mae.item(),
        'MSE': mse.item(),
        'RMSE': rmse.item()
    }
```

#### main.py

```python
"""ä¸»ç¨‹åºå…¥å£"""
import torch
import yaml
from pathlib import Path

from src.data import create_linear_data, prepare_dataloaders
from src.model import create_model
from src.train import train
from src.evaluate import evaluate_model

def main():
    # åŠ è½½é…ç½®
    with open('config.yaml', 'r') as f:
        config = yaml.safe_load(f)

    # è®¾ç½®è®¾å¤‡
    device = config['device'] if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    # åˆ›å»ºæ•°æ®
    print("Creating data...")
    X, y = create_linear_data(num_samples=1000)

    # å‡†å¤‡æ•°æ®åŠ è½½å™¨
    print("Preparing dataloaders...")
    train_loader, val_loader, test_loader = prepare_dataloaders(
        X, y,
        train_ratio=config['data']['train_ratio'],
        val_ratio=config['data']['val_ratio'],
        batch_size=config['data']['batch_size']
    )

    # åˆ›å»ºæ¨¡å‹
    print("Creating model...")
    model = create_model(config, device)

    # è®­ç»ƒæ¨¡å‹
    print("Training model...")
    history = train(model, train_loader, val_loader, config, device)

    # è¯„ä¼°æ¨¡å‹
    print("\nEvaluating model...")
    metrics = evaluate_model(model, test_loader, device)
    print("Test Metrics:")
    for name, value in metrics.items():
        print(f"  {name}: {value:.4f}")

    # ä¿å­˜æ¨¡å‹
    model_dir = Path(config['save']['model_dir'])
    model_dir.mkdir(exist_ok=True)

    model_path = model_dir / 'final_model.pth'
    torch.save(model.state_dict(), model_path)
    print(f"\nModel saved to {model_path}")

if __name__ == "__main__":
    main()
```

---

## 12. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 12.1 è®­ç»ƒé—®é¢˜

#### Q1: æŸå¤±ä¸ä¸‹é™

**å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆ:**

```python
# 1. å­¦ä¹ ç‡å¤ªå°
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # å°è¯•å¢å¤§

# 2. å­¦ä¹ ç‡å¤ªå¤§
optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)  # å°è¯•å‡å°

# 3. æ£€æŸ¥æ¢¯åº¦æ˜¯å¦ä¸º None
for name, param in model.named_parameters():
    if param.grad is None:
        print(f"{name} has no gradient!")

# 4. æ£€æŸ¥æ˜¯å¦å¿˜è®°è°ƒç”¨ optimizer.zero_grad()
optimizer.zero_grad()  # å¿…é¡»åœ¨ loss.backward() ä¹‹å‰

# 5. ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

#### Q2: æ¨¡å‹è¿‡æ‹Ÿåˆ

```python
# 1. æ·»åŠ  Dropout
class ModelWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(10, 50)
        self.dropout = nn.Dropout(0.5)
        self.linear2 = nn.Linear(50, 1)

    def forward(self, x):
        x = torch.relu(self.linear1(x))
        x = self.dropout(x)
        return self.linear2(x)

# 2. ä½¿ç”¨æƒé‡è¡°å‡ (L2 æ­£åˆ™åŒ–)
optimizer = torch.optim.Adam(model.parameters(),
                            lr=0.001,
                            weight_decay=1e-4)

# 3. æ—©åœ
# (è§å‰é¢çš„æ—©åœå®ç°)

# 4. æ•°æ®å¢å¼º (å¯¹äºå›¾åƒ/æ–‡æœ¬)
# 5. ä½¿ç”¨æ›´å¤šè®­ç»ƒæ•°æ®
```

#### Q3: æ¨¡å‹æ¬ æ‹Ÿåˆ

```python
# 1. å¢åŠ æ¨¡å‹å¤æ‚åº¦
class BiggerModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(1, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.layers(x)

# 2. è®­ç»ƒæ›´å¤š epochs
epochs = 500  # å¢åŠ 

# 3. é™ä½æ­£åˆ™åŒ–å¼ºåº¦
optimizer = torch.optim.Adam(model.parameters(),
                            lr=0.001,
                            weight_decay=1e-5)  # å‡å°

# 4. å°è¯•æ›´å¤æ‚çš„ç‰¹å¾å·¥ç¨‹
```

### 12.2 æ€§èƒ½é—®é¢˜

#### Q4: è®­ç»ƒé€Ÿåº¦æ…¢

```python
# 1. ä½¿ç”¨ GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# 2. å¢åŠ  batch size
train_loader = DataLoader(dataset, batch_size=64)  # å¢å¤§

# 3. ä½¿ç”¨ DataLoader çš„ num_workers
train_loader = DataLoader(dataset, num_workers=4, pin_memory=True)

# 4. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    output = model(input)
    loss = loss_fn(output, target)

# 5. ä½¿ç”¨ torch.compile (PyTorch 2.0+)
model = torch.compile(model)
```

#### Q5: å†…å­˜ä¸è¶³

```python
# 1. å‡å° batch size
train_loader = DataLoader(dataset, batch_size=16)  # å‡å°

# 2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
# (è§å‰é¢çš„æ¢¯åº¦ç´¯ç§¯å®ç°)

# 3. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)
from torch.utils.checkpoint import checkpoint

def forward(self, x):
    x = checkpoint(self.layer1, x)
    x = checkpoint(self.layer2, x)
    return x

# 4. æ¸…ç†ç¼“å­˜
torch.cuda.empty_cache()

# 5. ä½¿ç”¨ FP16
model = model.half()
```

### 12.3 æ•°æ®é—®é¢˜

#### Q6: æ•°æ®ä¸å¹³è¡¡

```python
# 1. ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°
weights = torch.tensor([1.0, 10.0])  # ç±»åˆ«æƒé‡
loss_fn = nn.CrossEntropyLoss(weight=weights)

# 2. è¿‡é‡‡æ ·å°‘æ•°ç±»
from torch.utils.data import WeightedRandomSampler

# è®¡ç®—æ ·æœ¬æƒé‡
samples_weight = [...]  # æ ¹æ®ç±»åˆ«åˆ†é…æƒé‡
sampler = WeightedRandomSampler(samples_weight, len(samples_weight))
train_loader = DataLoader(dataset, sampler=sampler)

# 3. æ¬ é‡‡æ ·å¤šæ•°ç±»
```

#### Q7: æ•°æ®æ ‡å‡†åŒ–

```python
# æ ‡å‡†åŒ–æ•°æ® (å‡å€¼=0, æ ‡å‡†å·®=1)
mean = X_train.mean()
std = X_train.std()

X_train_normalized = (X_train - mean) / std
X_test_normalized = (X_test - mean) / std

# æ³¨æ„: ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡æ¥æ ‡å‡†åŒ–æµ‹è¯•é›†
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨

### ä¼˜åŒ–æŠ€æœ¯æ•ˆæœå¯¹æ¯”

| æŠ€æœ¯ | è®­ç»ƒé€Ÿåº¦æå‡ | å†…å­˜èŠ‚çœ | ç²¾åº¦å½±å“ | å®ç°éš¾åº¦ |
|-----|------------|---------|---------|---------|
| **AMP** | 1.5-3x | 50% | æ—  | ä½ |
| **æ¢¯åº¦ç´¯ç§¯** | -10% | 75% | æ—  | ä½ |
| **DataLoader workers** | 1.5-2x | æ—  | æ—  | ä½ |
| **Gradient Checkpointing** | -20% | 80% | æ—  | ä¸­ |
| **æ¨¡å‹é‡åŒ–** | 2-4x | 75% | å¾®å° | ä¸­ |
| **ONNXå¯¼å‡º** | 1.2-2x | æ—  | æ—  | ä½ |

---

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬æ•™ç¨‹å,ä½ åº”è¯¥èƒ½å¤Ÿ:

- [ ] ç†è§£ PyTorch çš„å®Œæ•´å·¥ä½œæµç¨‹
- [ ] åˆ›å»ºå’Œåˆ’åˆ†æ•°æ®é›†
- [ ] æ„å»ºè‡ªå®šä¹‰ PyTorch æ¨¡å‹
- [ ] å®ç°è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯
- [ ] ä½¿ç”¨å„ç§æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
- [ ] ä¿å­˜å’ŒåŠ è½½æ¨¡å‹
- [ ] åº”ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- [ ] å®ç°å­¦ä¹ ç‡è°ƒåº¦
- [ ] ä½¿ç”¨ TensorBoard/W&B ç›‘æ§è®­ç»ƒ
- [ ] å¯¼å‡ºæ¨¡å‹ä¸º ONNX
- [ ] å¤„ç†å¸¸è§çš„è®­ç»ƒé—®é¢˜
- [ ] ä¼˜åŒ–è®­ç»ƒæ€§èƒ½
- [ ] æ„å»ºå®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®

---

## ğŸ“š æ¨èèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [PyTorch å®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/)
- [PyTorch æ•™ç¨‹](https://pytorch.org/tutorials/)
- [PyTorch ç¤ºä¾‹](https://github.com/pytorch/examples)

### å­¦ä¹ èµ„æº
- [PyTorch è®ºå›](https://discuss.pytorch.org/)
- [PyTorch Lightning](https://www.pytorchlightning.ai/)
- [Hugging Face Transformers](https://huggingface.co/transformers/)

### ä¹¦ç±æ¨è
- "Deep Learning with PyTorch" (å®˜æ–¹ä¹¦ç±)
- "Programming PyTorch for Deep Learning"

### åœ¨çº¿è¯¾ç¨‹
- Fast.ai - Practical Deep Learning
- Coursera - Deep Learning Specialization
- Zero to Mastery - PyTorch for Deep Learning

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬æ•™ç¨‹å,å»ºè®®å­¦ä¹ :

1. **02 - Neural Network Classification** - ç¥ç»ç½‘ç»œåˆ†ç±»
2. **03 - Computer Vision** - è®¡ç®—æœºè§†è§‰
3. **04 - Custom Datasets** - è‡ªå®šä¹‰æ•°æ®é›†
4. **05 - Going Modular** - æ¨¡å—åŒ–å¼€å‘
5. **06 - Transfer Learning** - è¿ç§»å­¦ä¹ 
6. **07 - Experiment Tracking** - å®éªŒè¿½è¸ª
7. **08 - Model Deployment** - æ¨¡å‹éƒ¨ç½²

---

## âœï¸ ç»ƒä¹ é¢˜

### åˆçº§ç»ƒä¹ 

1. **åˆ›å»ºè‡ªå·±çš„æ•°æ®é›†**
   - åˆ›å»ºä¸€ä¸ªäºŒæ¬¡å‡½æ•°æ•°æ®é›†: y = axÂ² + bx + c
   - æ„å»ºæ¨¡å‹æ¥å­¦ä¹ å‚æ•° a, b, c

2. **å®éªŒä¸åŒçš„è¶…å‚æ•°**
   - å°è¯•å­¦ä¹ ç‡: [0.001, 0.01, 0.1]
   - å°è¯•ä¼˜åŒ–å™¨: SGD, Adam, AdamW
   - ç»˜åˆ¶æŸå¤±æ›²çº¿å¹¶æ¯”è¾ƒ

3. **å®ç°éªŒè¯é›†**
   - å°†æ•°æ®åˆ’åˆ†ä¸º è®­ç»ƒ/éªŒè¯/æµ‹è¯• (70/15/15)
   - åœ¨éªŒè¯é›†ä¸Šé€‰æ‹©æœ€ä½³æ¨¡å‹

### ä¸­çº§ç»ƒä¹ 

4. **å®ç°å­¦ä¹ ç‡è°ƒåº¦å™¨**
   - ä½¿ç”¨ ReduceLROnPlateau
   - ç»˜åˆ¶å­¦ä¹ ç‡å˜åŒ–æ›²çº¿

5. **æ·»åŠ æ­£åˆ™åŒ–**
   - å®ç° L1/L2 æ­£åˆ™åŒ–
   - æ¯”è¾ƒæœ‰æ— æ­£åˆ™åŒ–çš„æ•ˆæœ

6. **å®ç°æ—©åœ**
   - ç›‘æ§éªŒè¯æŸå¤±
   - åœ¨æ€§èƒ½ä¸å†æå‡æ—¶åœæ­¢è®­ç»ƒ

### é«˜çº§ç»ƒä¹ 

7. **å®Œæ•´é¡¹ç›®**
   - æ„å»ºä¸€ä¸ªæˆ¿ä»·é¢„æµ‹æ¨¡å‹
   - ä½¿ç”¨çœŸå®æ•°æ®é›† (å¦‚ Boston Housing)
   - å®ç°å®Œæ•´çš„è®­ç»ƒã€è¯„ä¼°ã€ä¿å­˜æµç¨‹
   - éƒ¨ç½²ä¸º API æœåŠ¡

8. **æ€§èƒ½ä¼˜åŒ–**
   - å®ç°æ··åˆç²¾åº¦è®­ç»ƒ
   - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
   - å¯¼å‡ºä¸º ONNX å¹¶æ¯”è¾ƒæ€§èƒ½

9. **å®éªŒè¿½è¸ª**
   - ä½¿ç”¨ W&B æˆ– TensorBoard
   - è®°å½•æ‰€æœ‰è¶…å‚æ•°å’ŒæŒ‡æ ‡
   - è¿›è¡Œè¶…å‚æ•°æœç´¢

---

## ğŸ“ æ€»ç»“

æœ¬æ•™ç¨‹æ¶µç›–äº†:

âœ… **åŸºç¡€å·¥ä½œæµç¨‹** - ä»æ•°æ®åˆ°éƒ¨ç½²çš„å®Œæ•´æµç¨‹
âœ… **æ ¸å¿ƒæ¦‚å¿µ** - æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨ã€è®­ç»ƒå¾ªç¯
âœ… **æœ€ä½³å®è·µ** - æ•°æ®åˆ’åˆ†ã€æ¨¡å‹ä¿å­˜ã€æ¨ç†æ¨¡å¼
âœ… **é«˜çº§æŠ€æœ¯** - AMPã€æ¢¯åº¦ç´¯ç§¯ã€å­¦ä¹ ç‡è°ƒåº¦
âœ… **æ€§èƒ½ä¼˜åŒ–** - æå‡è®­ç»ƒé€Ÿåº¦å’Œå‡å°‘å†…å­˜ä½¿ç”¨
âœ… **ç”Ÿäº§éƒ¨ç½²** - ONNX å¯¼å‡ºã€é‡åŒ–ã€API æœåŠ¡
âœ… **å®æˆ˜é¡¹ç›®** - å®Œæ•´çš„é¡¹ç›®ç»“æ„å’Œä»£ç 
âœ… **é—®é¢˜è§£å†³** - å¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³

**è®°ä½:** æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªè¿­ä»£çš„è¿‡ç¨‹ã€‚ä¸æ–­å®éªŒã€å¯è§†åŒ–ã€è°ƒè¯•,ç›´åˆ°è·å¾—æ»¡æ„çš„ç»“æœ!

---

**æ–‡æ¡£ç‰ˆæœ¬:** v2.0 (å®Œæ•´å¢å¼ºç‰ˆ)
**åˆ›å»ºæ—¥æœŸ:** 2025-11-16
**ä½œè€…:** æ•´åˆè‡ª Learn PyTorch + 2024 æœ€ä½³å®è·µ
**è®¸å¯:** MIT License

**å¿«é€Ÿå¼€å§‹ä¸‹ä¸€ç« :** [PyTorch Neural Network Classification â†’](https://www.learnpytorch.io/02_pytorch_classification/)
