#!/usr/bin/env python3
"""
GLM-4.6 + LangChain SequentialChain ç¤ºä¾‹ (ç°ä»£è¯­æ³•)
æ¼”ç¤ºä½¿ç”¨ç°ä»£ Runnable è¯­æ³•è¿›è¡Œé“¾å¼è°ƒç”¨ï¼Œå·²å¼ƒç”¨ LLMChain
ä½¿ç”¨ prompt | model | output_parser çš„ç®¡é“è¯­æ³•
"""

import os
import dotenv
from typing import Dict, Any
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import SequentialChain
from langchain_core.runnables import RunnablePassthrough, RunnableParallel

# åŠ è½½ç¯å¢ƒå˜é‡ - ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½.envæ–‡ä»¶
dotenv.load_dotenv(dotenv_path="../.env")

# æ£€æŸ¥APIå¯†é’¥
api_key = os.getenv("ZHIPUAI_API_KEY")
if not api_key or api_key == "your-zhipu-api-key-here":
    print("âŒ é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ZHIPUAI_API_KEY")
    exit(1)

# åˆå§‹åŒ–GLMæ¨¡å‹
def get_glm_model(temperature: float = 0.7):
    """è·å–GLMæ¨¡å‹å®ä¾‹"""
    return ChatZhipuAI(
        model="glm-4.6",
        temperature=temperature,
        api_key=api_key
    )

def simple_sequential_chain_example():
    """SimpleSequentialChain ç®€å•ç¤ºä¾‹ - ä½¿ç”¨ç°ä»£è¯­æ³•"""
    print("=" * 60)
    print("ğŸ”— SimpleSequentialChain ç®€å•ç¤ºä¾‹ (ç°ä»£è¯­æ³•)")
    print("=" * 60)

    model = get_glm_model()

    # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆæ•…äº‹ä¸»é¢˜
    story_prompt = PromptTemplate.from_template(
        "è¯·ä¸ºä¸€ä¸ªå„¿ç«¥æ•…äº‹ç”Ÿæˆä¸€ä¸ªæœ‰è¶£çš„ä¸»é¢˜ï¼Œä¸»é¢˜å…³äº{topic}ã€‚"
    ) | model | StrOutputParser()

    # ç¬¬äºŒæ­¥ï¼šåŸºäºä¸»é¢˜å†™æ•…äº‹
    write_prompt = PromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹ä¸»é¢˜ï¼š{story_theme}\nè¯·å†™ä¸€ä¸ªç®€çŸ­çš„å„¿ç«¥æ•…äº‹ï¼Œé€‚åˆ5-8å²çš„å­©å­ã€‚"
    ) | model | StrOutputParser()

    # ç¬¬ä¸‰æ­¥ï¼šä¸ºæ•…äº‹æ·»åŠ å¯“æ„
    moral_prompt = PromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹æ•…äº‹ï¼š{story}\nè¯·ä¸ºè¿™ä¸ªæ•…äº‹å†™ä¸€ä¸ªç®€å•çš„å¯“æ„æ€»ç»“ã€‚"
    ) | model | StrOutputParser()

    # ç”±äºSimpleSequentialChainä»éœ€è¦ä¼ ç»Ÿé“¾ï¼Œæˆ‘ä»¬ä½¿ç”¨RunnablePassthroughæ¥å®ç°ç±»ä¼¼åŠŸèƒ½
    # æˆ–è€…ç›´æ¥ä½¿ç”¨ç®¡é“è¯­æ³•
    print("ğŸš€ ä½¿ç”¨ç°ä»£ç®¡é“è¯­æ³•:")

    # å®Œæ•´çš„ç®¡é“
    full_chain = (
        {
            "story_theme": story_prompt,
            "topic": RunnablePassthrough()
        }
        | RunnablePassthrough.assign(
            story=lambda x: write_prompt.invoke({"story_theme": x["story_theme"]})
        )
        | RunnablePassthrough.assign(
            moral=lambda x: moral_prompt.invoke({"story": x["story"]})
        )
    )

    # è¿è¡Œé“¾
    print("ğŸš€ å¼€å§‹è¿è¡Œç°ä»£è¯­æ³•çš„é“¾...")
    result = full_chain.invoke("å‹è°Š")

    print(f"\nğŸ‰ æœ€ç»ˆç»“æœ:")
    print(f"ğŸ“– æ•…äº‹ä¸»é¢˜: {result['story_theme']}")
    print(f"ğŸ“ æ•…äº‹å†…å®¹: {result['story'][:200]}...")
    print(f"ğŸ’¡ æ•…äº‹å¯“æ„: {result['moral']}")

def sequential_chain_example():
    """SequentialChain å¤æ‚ç¤ºä¾‹ - ä½¿ç”¨ç°ä»£è¯­æ³•"""
    print("\n" + "=" * 60)
    print("ğŸ”— SequentialChain å¤æ‚ç¤ºä¾‹ (ç°ä»£è¯­æ³•)")
    print("=" * 60)

    model = get_glm_model()

    # æ­¥éª¤1ï¼šç”Ÿæˆå¤§çº²
    outline_chain = (
        PromptTemplate.from_template(
            "ä¸º{audience}å†™ä¸€ç¯‡å…³äº{topic}çš„æ–‡ç« å¤§çº²ã€‚æ–‡ç« é£æ ¼ï¼š{style}ã€‚"
        ) | model | StrOutputParser()
    )

    # æ­¥éª¤2ï¼šæ ¹æ®å¤§çº²ç”Ÿæˆå†…å®¹
    content_chain = (
        PromptTemplate.from_template(
            "åŸºäºä»¥ä¸‹å¤§çº²ï¼š{outline}\nè¯·ä¸º{audience}å†™ä¸€ç¯‡å…³äº{topic}çš„è¯¦ç»†æ–‡ç« ã€‚æ–‡ç« é£æ ¼ï¼š{style}ï¼Œå­—æ•°çº¦{word_count}å­—ã€‚"
        ) | model | StrOutputParser()
    )

    # æ­¥éª¤3ï¼šç”Ÿæˆæ ‡é¢˜
    title_chain = (
        PromptTemplate.from_template(
            "åŸºäºä»¥ä¸‹æ–‡ç« å†…å®¹ï¼š{content}\nè¯·ç”Ÿæˆä¸€ä¸ªå¸å¼•äººçš„æ ‡é¢˜å’Œç®€çŸ­æ‘˜è¦ã€‚"
        ) | model | StrOutputParser()
    )

    # æ­¥éª¤4ï¼šç”Ÿæˆå…³é”®è¯
    keywords_chain = (
        PromptTemplate.from_template(
            "åŸºäºä»¥ä¸‹å†…å®¹ï¼š{content}\nè¯·æå–5ä¸ªç›¸å…³çš„å…³é”®è¯ã€‚"
        ) | model | StrOutputParser()
    )

    # ä½¿ç”¨ç°ä»£è¯­æ³•çš„å¹¶è¡Œå’Œé¡ºåºæ‰§è¡Œ
    overall_chain = (
        {
            "outline": lambda x: outline_chain.invoke({
                "audience": x["audience"],
                "topic": x["topic"],
                "style": x["style"]
            })
        }
        | RunnablePassthrough.assign(
            content=lambda x: content_chain.invoke({
                "outline": x["outline"],
                "audience": x["audience"],
                "topic": x["topic"],
                "style": x["style"],
                "word_count": x["word_count"]
            })
        )
        | RunnablePassthrough.assign(
            title_and_summary=lambda x: title_chain.invoke({"content": x["content"]})
        )
        | RunnablePassthrough.assign(
            keywords=lambda x: keywords_chain.invoke({"content": x["content"]})
        )
    )

    # è¿è¡Œé“¾
    print("ğŸš€ å¼€å§‹è¿è¡Œç°ä»£è¯­æ³•çš„é“¾...")
    result = overall_chain.invoke({
        "topic": "äººå·¥æ™ºèƒ½çš„æœªæ¥",
        "audience": "ç§‘æŠ€çˆ±å¥½è€…",
        "style": "ä¸“ä¸šä½†ä¸å¤±é€šä¿—",
        "word_count": "800"
    })

    print(f"\nğŸ‰ æœ€ç»ˆç»“æœ:")
    print(f"ğŸ“‹ å¤§çº²: {result['outline'][:200]}...")
    print(f"ğŸ“ å†…å®¹: {result['content'][:300]}...")
    print(f"ğŸ·ï¸ æ ‡é¢˜å’Œæ‘˜è¦: {result['title_and_summary']}")
    print(f"ğŸ”‘ å…³é”®è¯: {result['keywords']}")

def practical_content_creation_chain():
    """å®ç”¨å†…å®¹åˆ›å»ºé“¾ - åšå®¢æ–‡ç« ç”Ÿæˆå™¨ (ç°ä»£è¯­æ³•)"""
    print("\n" + "=" * 60)
    print("ğŸ“ å®ç”¨å†…å®¹åˆ›å»ºé“¾ - åšå®¢æ–‡ç« ç”Ÿæˆå™¨ (ç°ä»£è¯­æ³•)")
    print("=" * 60)

    model = get_glm_model()

    # æ­¥éª¤1ï¼šå¸‚åœºç ”ç©¶
    research_chain = (
        PromptTemplate.from_template(
            "é’ˆå¯¹ä¸»é¢˜'{topic}'ï¼Œè¿›è¡Œç®€å•çš„å¸‚åœºåˆ†æã€‚åŒ…æ‹¬ï¼š1)ç›®æ ‡å—ä¼— 2)ç«äº‰æƒ…å†µ 3)å†…å®¹æœºä¼š"
        ) | model | StrOutputParser()
    )

    # æ­¥éª¤2ï¼šå†…å®¹è§„åˆ’
    planning_chain = (
        PromptTemplate.from_template(
            "åŸºäºç ”ç©¶ï¼š{research}\nä¸º'{topic}'åˆ¶å®šå†…å®¹è®¡åˆ’ï¼š1)æ ¸å¿ƒè§‚ç‚¹ 2)æ–‡ç« ç»“æ„ 3)å…³é”®ä¿¡æ¯ç‚¹"
        ) | model | StrOutputParser()
    )

    # æ­¥éª¤3ï¼šè‰ç¨¿æ’°å†™
    draft_chain = (
        PromptTemplate.from_template(
            "åŸºäºè®¡åˆ’ï¼š{plan}\næ’°å†™å…³äº'{topic}'çš„åšå®¢è‰ç¨¿ï¼Œè¦æ±‚ï¼š{requirements}"
        ) | model | StrOutputParser()
    )

    # æ­¥éª¤4ï¼šå†…å®¹ä¼˜åŒ–
    optimize_chain = (
        PromptTemplate.from_template(
            "ä¼˜åŒ–ä»¥ä¸‹è‰ç¨¿ï¼š{draft}\næ”¹è¿›è¦æ±‚ï¼š1)å¢å¼ºå¯è¯»æ€§ 2)æ·»åŠ CTA 3)SEOä¼˜åŒ–å»ºè®®"
        ) | model | StrOutputParser()
    )

    # æ­¥éª¤5ï¼šç¤¾äº¤åª’ä½“æ¨å¹¿
    social_chain = (
        PromptTemplate.from_template(
            "åŸºäºä¼˜åŒ–å†…å®¹ï¼š{optimized_content}\nåˆ›ä½œ3æ¡ç¤¾äº¤åª’ä½“æ¨å¹¿æ–‡æ¡ˆï¼ˆTwitterã€LinkedInã€Facebookï¼‰"
        ) | model | StrOutputParser()
    )

    # ä½¿ç”¨ç°ä»£è¯­æ³•åˆ›å»ºå®Œæ•´çš„å†…å®¹åˆ›å»ºé“¾
    content_creation_chain = (
        {
            "research": lambda x: research_chain.invoke({"topic": x["topic"]})
        }
        | RunnablePassthrough.assign(
            plan=lambda x: planning_chain.invoke({
                "research": x["research"],
                "topic": x["topic"]
            })
        )
        | RunnablePassthrough.assign(
            draft=lambda x: draft_chain.invoke({
                "plan": x["plan"],
                "topic": x["topic"],
                "requirements": x["requirements"]
            })
        )
        | RunnablePassthrough.assign(
            optimized_content=lambda x: optimize_chain.invoke({"draft": x["draft"]})
        )
        | RunnablePassthrough.assign(
            social_posts=lambda x: social_chain.invoke({"optimized_content": x["optimized_content"]})
        )
    )

    # è¿è¡Œå†…å®¹åˆ›å»ºé“¾
    print("ğŸš€ å¼€å§‹è¿è¡Œç°ä»£è¯­æ³•çš„å†…å®¹åˆ›å»ºé“¾...")
    result = content_creation_chain.invoke({
        "topic": "è¿œç¨‹å·¥ä½œçš„æ•ˆç‡æå‡æŠ€å·§",
        "requirements": "å®ç”¨æ€§å¼ºï¼ŒåŒ…å«å…·ä½“æ¡ˆä¾‹ï¼Œé€‚åˆèŒåœºäººå£«é˜…è¯»ï¼Œé•¿åº¦1000å­—å·¦å³"
    })

    print(f"\nğŸ‰ å†…å®¹åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ” å¸‚åœºç ”ç©¶: {result['research'][:200]}...")
    print(f"ğŸ“‹ å†…å®¹è®¡åˆ’: {result['plan'][:200]}...")
    print(f"ğŸ“ ä¼˜åŒ–åå†…å®¹: {result['optimized_content'][:300]}...")
    print(f"ğŸ“± ç¤¾äº¤åª’ä½“æ–‡æ¡ˆ: {result['social_posts']}")

def translation_chain_example():
    """ç¿»è¯‘é“¾ç¤ºä¾‹ - ç°ä»£è¯­æ³•çš„å®é™…åº”ç”¨"""
    print("\n" + "=" * 60)
    print("ğŸŒ ç¿»è¯‘é“¾ç¤ºä¾‹ (ç°ä»£è¯­æ³•)")
    print("=" * 60)

    model = get_glm_model()

    # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹è¯­è¨€
    detect_chain = (
        PromptTemplate.from_template(
            "æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬çš„è¯­è¨€ï¼š{text}\nåªå›ç­”è¯­è¨€åç§°ï¼Œå¦‚ï¼šä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ç­‰ã€‚"
        ) | model | StrOutputParser()
    )

    # ç¬¬äºŒæ­¥ï¼šç¿»è¯‘æˆè‹±æ–‡
    translate_chain = (
        PromptTemplate.from_template(
            "å°†ä»¥ä¸‹{detected_language}æ–‡æœ¬ç¿»è¯‘æˆè‡ªç„¶æµç•…çš„è‹±æ–‡ï¼š{original_text}\nä¿æŒåŸæ–‡çš„æ„æ€å’Œè¯­æ°”ã€‚"
        ) | model | StrOutputParser()
    )

    # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç¿»è¯‘æ‘˜è¦
    summary_chain = (
        PromptTemplate.from_template(
            "åŸºäºåŸæ–‡ï¼š{original_text}\nå’Œè¯‘æ–‡ï¼š{translated_text}\nè¯·åˆ†æç¿»è¯‘è´¨é‡ï¼Œå¹¶ç»™å‡ºæ”¹è¿›å»ºè®®ã€‚"
        ) | model | StrOutputParser()
    )

    # ä½¿ç”¨ç°ä»£è¯­æ³•åˆ›å»ºç¿»è¯‘é“¾
    translation_chain = (
        {
            "original_text": RunnablePassthrough(),
            "detected_language": lambda x: detect_chain.invoke({"text": x})
        }
        | RunnablePassthrough.assign(
            translated_text=lambda x: translate_chain.invoke({
                "detected_language": x["detected_language"],
                "original_text": x["original_text"]
            })
        )
        | RunnablePassthrough.assign(
            translation_summary=lambda x: summary_chain.invoke({
                "original_text": x["original_text"],
                "translated_text": x["translated_text"]
            })
        )
    )

    # æµ‹è¯•ç¿»è¯‘
    test_text = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ï¼Œä»æ™ºèƒ½æ‰‹æœºåˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ŒAIæŠ€æœ¯æ— å¤„ä¸åœ¨ã€‚"

    print("ğŸš€ å¼€å§‹è¿è¡Œç°ä»£è¯­æ³•çš„ç¿»è¯‘é“¾...")
    result = translation_chain.invoke(test_text)

    print(f"\nğŸ‰ ç¿»è¯‘åˆ†æç»“æœ:")
    print(f"ğŸŒ æ£€æµ‹åˆ°çš„è¯­è¨€: {result['detected_language']}")
    print(f"ğŸ“ åŸæ–‡: {result['original_text']}")
    print(f"ğŸ”„ è¯‘æ–‡: {result['translated_text'][:200]}...")
    print(f"ğŸ“Š ç¿»è¯‘åˆ†æ: {result['translation_summary']}")

def chain_comparison():
    """é“¾ç±»å‹æ¯”è¾ƒå’Œæœ€ä½³å®è·µ - ç°ä»£è¯­æ³• vs ä¼ ç»Ÿè¯­æ³•"""
    print("\n" + "=" * 60)
    print("âš–ï¸ é“¾ç±»å‹æ¯”è¾ƒå’Œæœ€ä½³å®è·µ (ç°ä»£è¯­æ³• vs ä¼ ç»Ÿè¯­æ³•)")
    print("=" * 60)

    print("""
ğŸ“Š ä¼ ç»Ÿ LLMChain vs ç°ä»£ Runnable è¯­æ³•:

ğŸ”— ä¼ ç»Ÿ LLMChain è¯­æ³• (å·²å¼ƒç”¨):
âŒ ç¼ºç‚¹:
   - LLMChain åœ¨ LangChain 0.1.17 ä¸­å·²å¼ƒç”¨
   - å°†åœ¨ 1.0 ç‰ˆæœ¬ä¸­ç§»é™¤
   - é…ç½®å¤æ‚ï¼Œéœ€è¦æ˜ç¡®æŒ‡å®šè¾“å‡ºé”®
   - ä¸å¤Ÿçµæ´»

âœ… ç°ä»£ Runnable è¯­æ³• (æ¨è):
âœ… ä¼˜ç‚¹:
   - ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ | è¿›è¡Œé“¾å¼ç»„åˆ
   - æ›´ç®€æ´ç›´è§‚çš„è¯­æ³•
   - ä½¿ç”¨ RunnablePassthrough å’Œ RunnableParallel æä¾›çµæ´»æ€§
   - æ›´å¥½çš„ç±»å‹æ”¯æŒå’Œé”™è¯¯å¤„ç†
   - ç¬¦åˆå‡½æ•°å¼ç¼–ç¨‹ç†å¿µ

ğŸ¯ è¿ç§»å»ºè®®:
1. æ›¿æ¢ LLMChain â†’ prompt | model | output_parser
2. ä½¿ç”¨ RunnablePassthrough æ›¿ä»£å¤æ‚çš„æ‰‹åŠ¨æ•°æ®ä¼ é€’
3. ä½¿ç”¨ RunnableParallel è¿›è¡Œå¹¶è¡Œå¤„ç†
4. é‡‡ç”¨ç°ä»£çš„ç®¡é“è¯­æ³•æé«˜ä»£ç å¯è¯»æ€§

ğŸ”§ ä»£ç ç¤ºä¾‹å¯¹æ¯”:

ä¼ ç»Ÿè¯­æ³•:
   chain = LLMChain(llm=model, prompt=prompt, output_key="result")

ç°ä»£è¯­æ³•:
   chain = prompt | model | StrOutputParser()
    """)

def error_handling_example():
    """é”™è¯¯å¤„ç†ç¤ºä¾‹ - ç°ä»£è¯­æ³•"""
    print("\n" + "=" * 60)
    print("âš ï¸ é”™è¯¯å¤„ç†ç¤ºä¾‹ (ç°ä»£è¯­æ³•)")
    print("=" * 60)

    model = get_glm_model()

    try:
        # åˆ›å»ºä¸€ä¸ªå¯èƒ½å¤±è´¥çš„é“¾ - ä½¿ç”¨ç°ä»£è¯­æ³•
        risky_chain = (
            PromptTemplate.from_template(
                "è¯·{action}å…³äº{topic}çš„å†…å®¹ã€‚"
            ) | model | StrOutputParser()
        )

        print("ğŸš€ è¿è¡Œå¯èƒ½å¤±è´¥çš„ä»»åŠ¡...")
        result = risky_chain.invoke({
            "action": "ç”Ÿæˆ",
            "topic": "ä¸€ä¸ªä¸å­˜åœ¨çš„æ¦‚å¿µ"
        })

        print(f"âœ… ä»»åŠ¡æˆåŠŸå®Œæˆ: {result[:100]}...")

    except Exception as e:
        print(f"âŒ é“¾æ‰§è¡Œå‡ºé”™: {e}")
        print("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥è¾“å…¥å‚æ•°å’Œæç¤ºè¯æ¨¡æ¿")

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ GLM-4.6 + LangChain SequentialChain è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)

    try:
        # è¿è¡Œå„ç§ç¤ºä¾‹
        simple_sequential_chain_example()
        sequential_chain_example()
        practical_content_creation_chain()
        translation_chain_example()
        chain_comparison()
        error_handling_example()

        print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ“š æ›´å¤šä¿¡æ¯è¯·å‚è€ƒï¼š")
        print("- LangChainå®˜æ–¹æ–‡æ¡£: https://python.langchain.com/")
        print("- é“¾å¼è°ƒç”¨æŒ‡å—: https://python.langchain.com/docs/modules/chains/")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{e}")

if __name__ == "__main__":
    main()