#!/usr/bin/env python3
"""
GLM-4.6 + LangChain SequentialChain å’Œ SimpleSequentialChain ç¤ºä¾‹
æ¼”ç¤ºé“¾å¼è°ƒç”¨çš„ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µ
"""

import os
import dotenv
from typing import Dict, Any
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain

# åŠ è½½ç¯å¢ƒå˜é‡ - ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½.envæ–‡ä»¶
dotenv.load_dotenv(dotenv_path="../.env")

# æ£€æŸ¥APIå¯†é’¥
api_key = os.getenv("ZHIPUAI_API_KEY")
if not api_key or api_key == "your-zhipu-api-key-here":
    print("âŒ é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ZHIPUAI_API_KEY")
    exit(1)

# åˆå§‹åŒ–GLMæ¨¡å‹
def get_glm_model(temperature: float = 0.7):
    """è·å–GLMæ¨¡å‹å®ä¾‹"""
    return ChatZhipuAI(
        model="glm-4.6",
        temperature=temperature,
        api_key=api_key
    )

def simple_sequential_chain_example():
    """SimpleSequentialChain ç®€å•ç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ”— SimpleSequentialChain ç®€å•ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆæ•…äº‹ä¸»é¢˜
    story_prompt = PromptTemplate.from_template(
        "è¯·ä¸ºä¸€ä¸ªå„¿ç«¥æ•…äº‹ç”Ÿæˆä¸€ä¸ªæœ‰è¶£çš„ä¸»é¢˜ï¼Œä¸»é¢˜å…³äº{topic}ã€‚"
    )
    story_chain = LLMChain(llm=model, prompt=story_prompt)

    # ç¬¬äºŒæ­¥ï¼šåŸºäºä¸»é¢˜å†™æ•…äº‹
    write_prompt = PromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹ä¸»é¢˜ï¼š{story_theme}\nè¯·å†™ä¸€ä¸ªç®€çŸ­çš„å„¿ç«¥æ•…äº‹ï¼Œé€‚åˆ5-8å²çš„å­©å­ã€‚"
    )
    write_chain = LLMChain(llm=model, prompt=write_prompt)

    # ç¬¬ä¸‰æ­¥ï¼šä¸ºæ•…äº‹æ·»åŠ å¯“æ„
    moral_prompt = PromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹æ•…äº‹ï¼š{story}\nè¯·ä¸ºè¿™ä¸ªæ•…äº‹å†™ä¸€ä¸ªç®€å•çš„å¯“æ„æ€»ç»“ã€‚"
    )
    moral_chain = LLMChain(llm=model, prompt=moral_prompt)

    # åˆ›å»ºSimpleSequentialChain
    overall_chain = SimpleSequentialChain(
        chains=[story_chain, write_chain, moral_chain],
        verbose=True
    )

    # è¿è¡Œé“¾
    print("ğŸš€ å¼€å§‹è¿è¡ŒSimpleSequentialChain...")
    result = overall_chain.run("å‹è°Š")

    print(f"\nğŸ‰ æœ€ç»ˆç»“æœ:")
    print(f"{result}")

def sequential_chain_example():
    """SequentialChain å¤æ‚ç¤ºä¾‹ - å¤šè¾“å…¥å¤šè¾“å‡º"""
    print("\n" + "=" * 60)
    print("ğŸ”— SequentialChain å¤æ‚ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # æ­¥éª¤1ï¼šç”Ÿæˆå¤§çº²
    outline_prompt = PromptTemplate.from_template(
        "ä¸º{audience}å†™ä¸€ç¯‡å…³äº{topic}çš„æ–‡ç« å¤§çº²ã€‚æ–‡ç« é£æ ¼ï¼š{style}ã€‚"
    )
    outline_chain = LLMChain(
        llm=model,
        prompt=outline_prompt,
        output_key="outline"
    )

    # æ­¥éª¤2ï¼šæ ¹æ®å¤§çº²ç”Ÿæˆå†…å®¹
    content_prompt = PromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹å¤§çº²ï¼š{outline}\nè¯·ä¸º{audience}å†™ä¸€ç¯‡å…³äº{topic}çš„è¯¦ç»†æ–‡ç« ã€‚æ–‡ç« é£æ ¼ï¼š{style}ï¼Œå­—æ•°çº¦{word_count}å­—ã€‚"
    )
    content_chain = LLMChain(
        llm=model,
        prompt=content_prompt,
        output_key="content"
    )

    # æ­¥éª¤3ï¼šç”Ÿæˆæ ‡é¢˜
    title_prompt = PromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹æ–‡ç« å†…å®¹ï¼š{content}\nè¯·ç”Ÿæˆä¸€ä¸ªå¸å¼•äººçš„æ ‡é¢˜å’Œç®€çŸ­æ‘˜è¦ã€‚"
    )
    title_chain = LLMChain(
        llm=model,
        prompt=title_prompt,
        output_key="title_and_summary"
    )

    # æ­¥éª¤4ï¼šç”Ÿæˆå…³é”®è¯
    keywords_prompt = PromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹å†…å®¹ï¼š{content}\nè¯·æå–5ä¸ªç›¸å…³çš„å…³é”®è¯ã€‚"
    )
    keywords_chain = LLMChain(
        llm=model,
        prompt=keywords_prompt,
        output_key="keywords"
    )

    # åˆ›å»ºSequentialChain
    overall_chain = SequentialChain(
        chains=[outline_chain, content_chain, title_chain, keywords_chain],
        input_variables=["topic", "audience", "style", "word_count"],
        output_variables=["outline", "content", "title_and_summary", "keywords"],
        verbose=True
    )

    # è¿è¡Œé“¾
    print("ğŸš€ å¼€å§‹è¿è¡ŒSequentialChain...")
    result = overall_chain({
        "topic": "äººå·¥æ™ºèƒ½çš„æœªæ¥",
        "audience": "ç§‘æŠ€çˆ±å¥½è€…",
        "style": "ä¸“ä¸šä½†ä¸å¤±é€šä¿—",
        "word_count": "800"
    })

    print(f"\nğŸ‰ æœ€ç»ˆç»“æœ:")
    print(f"ğŸ“‹ å¤§çº²: {result['outline'][:200]}...")
    print(f"ğŸ“ å†…å®¹: {result['content'][:300]}...")
    print(f"ğŸ·ï¸ æ ‡é¢˜å’Œæ‘˜è¦: {result['title_and_summary']}")
    print(f"ğŸ”‘ å…³é”®è¯: {result['keywords']}")

def practical_content_creation_chain():
    """å®ç”¨å†…å®¹åˆ›å»ºé“¾ - åšå®¢æ–‡ç« ç”Ÿæˆå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ“ å®ç”¨å†…å®¹åˆ›å»ºé“¾ - åšå®¢æ–‡ç« ç”Ÿæˆå™¨")
    print("=" * 60)

    model = get_glm_model()

    # æ­¥éª¤1ï¼šå¸‚åœºç ”ç©¶
    research_prompt = PromptTemplate.from_template(
        "é’ˆå¯¹ä¸»é¢˜'{topic}'ï¼Œè¿›è¡Œç®€å•çš„å¸‚åœºåˆ†æã€‚åŒ…æ‹¬ï¼š1)ç›®æ ‡å—ä¼— 2)ç«äº‰æƒ…å†µ 3)å†…å®¹æœºä¼š"
    )
    research_chain = LLMChain(
        llm=model,
        prompt=research_prompt,
        output_key="research"
    )

    # æ­¥éª¤2ï¼šå†…å®¹è§„åˆ’
    planning_prompt = PromptTemplate.from_template(
        "åŸºäºç ”ç©¶ï¼š{research}\nä¸º'{topic}'åˆ¶å®šå†…å®¹è®¡åˆ’ï¼š1)æ ¸å¿ƒè§‚ç‚¹ 2)æ–‡ç« ç»“æ„ 3)å…³é”®ä¿¡æ¯ç‚¹"
    )
    planning_chain = LLMChain(
        llm=model,
        prompt=planning_prompt,
        output_key="plan"
    )

    # æ­¥éª¤3ï¼šè‰ç¨¿æ’°å†™
    draft_prompt = PromptTemplate.from_template(
        "åŸºäºè®¡åˆ’ï¼š{plan}\næ’°å†™å…³äº'{topic}'çš„åšå®¢è‰ç¨¿ï¼Œè¦æ±‚ï¼š{requirements}"
    )
    draft_chain = LLMChain(
        llm=model,
        prompt=draft_prompt,
        output_key="draft"
    )

    # æ­¥éª¤4ï¼šå†…å®¹ä¼˜åŒ–
    optimize_prompt = PromptTemplate.from_template(
        "ä¼˜åŒ–ä»¥ä¸‹è‰ç¨¿ï¼š{draft}\næ”¹è¿›è¦æ±‚ï¼š1)å¢å¼ºå¯è¯»æ€§ 2)æ·»åŠ CTA 3)SEOä¼˜åŒ–å»ºè®®"
    )
    optimize_chain = LLMChain(
        llm=model,
        prompt=optimize_prompt,
        output_key="optimized_content"
    )

    # æ­¥éª¤5ï¼šç¤¾äº¤åª’ä½“æ¨å¹¿
    social_prompt = PromptTemplate.from_template(
        "åŸºäºä¼˜åŒ–å†…å®¹ï¼š{optimized_content}\nåˆ›ä½œ3æ¡ç¤¾äº¤åª’ä½“æ¨å¹¿æ–‡æ¡ˆï¼ˆTwitterã€LinkedInã€Facebookï¼‰"
    )
    social_chain = LLMChain(
        llm=model,
        prompt=social_prompt,
        output_key="social_posts"
    )

    # åˆ›å»ºå®Œæ•´çš„å†…å®¹åˆ›å»ºé“¾
    content_creation_chain = SequentialChain(
        chains=[research_chain, planning_chain, draft_chain, optimize_chain, social_chain],
        input_variables=["topic", "requirements"],
        output_variables=["research", "plan", "draft", "optimized_content", "social_posts"],
        verbose=True
    )

    # è¿è¡Œå†…å®¹åˆ›å»ºé“¾
    print("ğŸš€ å¼€å§‹è¿è¡Œå†…å®¹åˆ›å»ºé“¾...")
    result = content_creation_chain({
        "topic": "è¿œç¨‹å·¥ä½œçš„æ•ˆç‡æå‡æŠ€å·§",
        "requirements": "å®ç”¨æ€§å¼ºï¼ŒåŒ…å«å…·ä½“æ¡ˆä¾‹ï¼Œé€‚åˆèŒåœºäººå£«é˜…è¯»ï¼Œé•¿åº¦1000å­—å·¦å³"
    })

    print(f"\nğŸ‰ å†…å®¹åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ” å¸‚åœºç ”ç©¶: {result['research'][:200]}...")
    print(f"ğŸ“‹ å†…å®¹è®¡åˆ’: {result['plan'][:200]}...")
    print(f"ğŸ“ ä¼˜åŒ–åå†…å®¹: {result['optimized_content'][:300]}...")
    print(f"ğŸ“± ç¤¾äº¤åª’ä½“æ–‡æ¡ˆ: {result['social_posts']}")

def translation_chain_example():
    """ç¿»è¯‘é“¾ç¤ºä¾‹ - SimpleSequentialChainçš„å®é™…åº”ç”¨"""
    print("\n" + "=" * 60)
    print("ğŸŒ ç¿»è¯‘é“¾ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹è¯­è¨€
    detect_prompt = PromptTemplate.from_template(
        "æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬çš„è¯­è¨€ï¼š{text}\nåªå›ç­”è¯­è¨€åç§°ï¼Œå¦‚ï¼šä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ç­‰ã€‚"
    )
    detect_chain = LLMChain(llm=model, prompt=detect_prompt)

    # ç¬¬äºŒæ­¥ï¼šç¿»è¯‘æˆè‹±æ–‡
    translate_prompt = PromptTemplate.from_template(
        "å°†ä»¥ä¸‹{detected_language}æ–‡æœ¬ç¿»è¯‘æˆè‡ªç„¶æµç•…çš„è‹±æ–‡ï¼š{original_text}\nä¿æŒåŸæ–‡çš„æ„æ€å’Œè¯­æ°”ã€‚"
    )
    translate_chain = LLMChain(llm=model, prompt=translate_prompt)

    # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆç¿»è¯‘æ‘˜è¦
    summary_prompt = PromptTemplate.from_template(
        "åŸºäºåŸæ–‡ï¼š{original_text}\nå’Œè¯‘æ–‡ï¼š{translated_text}\nè¯·åˆ†æç¿»è¯‘è´¨é‡ï¼Œå¹¶ç»™å‡ºæ”¹è¿›å»ºè®®ã€‚"
    )
    summary_chain = LLMChain(llm=model, prompt=summary_prompt)

    # åˆ›å»ºç¿»è¯‘é“¾
    translation_chain = SimpleSequentialChain(
        chains=[detect_chain, translate_chain, summary_chain],
        verbose=True
    )

    # æµ‹è¯•ç¿»è¯‘
    test_text = "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ï¼Œä»æ™ºèƒ½æ‰‹æœºåˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ŒAIæŠ€æœ¯æ— å¤„ä¸åœ¨ã€‚"

    print("ğŸš€ å¼€å§‹è¿è¡Œç¿»è¯‘é“¾...")
    result = translation_chain.run(test_text)

    print(f"\nğŸ‰ ç¿»è¯‘åˆ†æç»“æœ:")
    print(f"{result}")

def chain_comparison():
    """é“¾ç±»å‹æ¯”è¾ƒå’Œæœ€ä½³å®è·µ"""
    print("\n" + "=" * 60)
    print("âš–ï¸ é“¾ç±»å‹æ¯”è¾ƒå’Œæœ€ä½³å®è·µ")
    print("=" * 60)

    print("""
ğŸ“Š SimpleSequentialChain vs SequentialChain:

ğŸ”— SimpleSequentialChain:
âœ… ä¼˜ç‚¹:
   - ç®€å•æ˜“ç”¨ï¼Œé€‚åˆçº¿æ€§ä»»åŠ¡
   - è‡ªåŠ¨ä¼ é€’å‰ä¸€ä¸ªé“¾çš„è¾“å‡º
   - é…ç½®ç®€å•

âŒ ç¼ºç‚¹:
   - åªèƒ½å¤„ç†å•ä¸€è¾“å…¥è¾“å‡º
   - æ— æ³•è®¿é—®ä¸­é—´ç»“æœ
   - ä¸å¤Ÿçµæ´»

ğŸ”— SequentialChain:
âœ… ä¼˜ç‚¹:
   - æ”¯æŒå¤šè¾“å…¥å¤šè¾“å‡º
   - å¯ä»¥è®¿é—®æ‰€æœ‰ä¸­é—´ç»“æœ
   - æ›´åŠ çµæ´»å’Œå¼ºå¤§
   - æ”¯æŒå¤æ‚çš„ä¾èµ–å…³ç³»

âŒ ç¼ºç‚¹:
   - é…ç½®ç›¸å¯¹å¤æ‚
   - éœ€è¦æ˜ç¡®æŒ‡å®šè¾“å…¥è¾“å‡ºå˜é‡

ğŸ¯ ä½¿ç”¨å»ºè®®:
1. ç®€å•çº¿æ€§ä»»åŠ¡ â†’ SimpleSequentialChain
2. å¤æ‚å¤šæ­¥éª¤ä»»åŠ¡ â†’ SequentialChain
3. éœ€è¦è®¿é—®ä¸­é—´ç»“æœ â†’ SequentialChain
4. å¿«é€ŸåŸå‹å¼€å‘ â†’ SimpleSequentialChain
    """)

def error_handling_example():
    """é”™è¯¯å¤„ç†ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("âš ï¸ é”™è¯¯å¤„ç†ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    try:
        # åˆ›å»ºä¸€ä¸ªå¯èƒ½å¤±è´¥çš„é“¾
        risky_prompt = PromptTemplate.from_template(
            "è¯·{action}å…³äº{topic}çš„å†…å®¹ã€‚"
        )

        risky_chain = LLMChain(llm=model, prompt=risky_prompt)

        # ä½¿ç”¨SimpleSequentialChainåŒ…è£…
        safe_chain = SimpleSequentialChain(
            chains=[risky_chain],
            verbose=True
        )

        print("ğŸš€ è¿è¡Œå¯èƒ½å¤±è´¥çš„ä»»åŠ¡...")
        result = safe_chain.run({
            "action": "ç”Ÿæˆ",
            "topic": "ä¸€ä¸ªä¸å­˜åœ¨çš„æ¦‚å¿µ"
        })

        print(f"âœ… ä»»åŠ¡æˆåŠŸå®Œæˆ: {result[:100]}...")

    except Exception as e:
        print(f"âŒ é“¾æ‰§è¡Œå‡ºé”™: {e}")
        print("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥è¾“å…¥å‚æ•°å’Œæç¤ºè¯æ¨¡æ¿")

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ GLM-4.6 + LangChain SequentialChain è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)

    try:
        # è¿è¡Œå„ç§ç¤ºä¾‹
        simple_sequential_chain_example()
        sequential_chain_example()
        practical_content_creation_chain()
        translation_chain_example()
        chain_comparison()
        error_handling_example()

        print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ“š æ›´å¤šä¿¡æ¯è¯·å‚è€ƒï¼š")
        print("- LangChainå®˜æ–¹æ–‡æ¡£: https://python.langchain.com/")
        print("- é“¾å¼è°ƒç”¨æŒ‡å—: https://python.langchain.com/docs/modules/chains/")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{e}")

if __name__ == "__main__":
    main()