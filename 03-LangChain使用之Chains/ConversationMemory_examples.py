#!/usr/bin/env python3
"""
GLM-4.6 + LangChain v1.0 å¯¹è¯è®°å¿†ç¤ºä¾‹
æ¼”ç¤ºä½¿ç”¨LangChain v1.0æ–°APIè¿›è¡Œå¯¹è¯è®°å¿†ç®¡ç†
ä½¿ç”¨Runnableå’Œmessagesæ‰‹åŠ¨ç®¡ç†å¯¹è¯å†å²
"""

import os
import dotenv
from typing import List, Dict, Any
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# åŠ è½½ç¯å¢ƒå˜é‡ - ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½.envæ–‡ä»¶
dotenv.load_dotenv(dotenv_path="../.env")

# æ£€æŸ¥APIå¯†é’¥
api_key = os.getenv("ZHIPUAI_API_KEY")
if not api_key or api_key == "your-zhipu-api-key-here":
    print("âŒ é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ZHIPUAI_API_KEY")
    exit(1)

# åˆå§‹åŒ–GLMæ¨¡å‹
def get_glm_model(temperature: float = 0.7):
    """è·å–GLMæ¨¡å‹å®ä¾‹"""
    return ChatZhipuAI(
        model="glm-4.6",
        temperature=temperature,
        api_key=api_key
    )


# ========== è®°å¿†ç®¡ç†å·¥å…·ç±» ==========
class ConversationBufferWindowMemory:
    """çª—å£è®°å¿† - ä¿æŒå›ºå®šè½®æ•°çš„å¯¹è¯å†å²"""

    def __init__(self, k: int = 3):
        self.k = k  # ä¿ç•™çš„å¯¹è¯è½®æ•°
        self.messages: List[Dict] = []  # å­˜å‚¨å¯¹è¯æ¶ˆæ¯

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°è®°å¿†"""
        self.messages.append({"role": role, "content": content})

        # å¦‚æœè¶…è¿‡kè½®ï¼Œåˆ é™¤æœ€æ—©çš„æ¶ˆæ¯ï¼ˆä¿ç•™systemæ¶ˆæ¯ï¼‰
        if len(self.messages) > self.k * 2:  # *2å› ä¸ºæ¯æ¬¡å¯¹è¯åŒ…å«userå’Œassistant
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªésystemæ¶ˆæ¯å¹¶åˆ é™¤
            non_system_count = sum(1 for msg in self.messages if msg["role"] != "system")
            if non_system_count > self.k * 2:
                # åˆ é™¤ç¬¬äºŒä¸ªæ¶ˆæ¯å¼€å§‹ï¼ˆç´¢å¼•1ï¼Œä¿ç•™systemï¼‰
                self.messages.pop(1)

    def get_formatted_messages(self, system_prompt: str = "") -> List[Dict]:
        """è·å–æ ¼å¼åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨"""
        result = []

        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system_prompt:
            result.append({"role": "system", "content": system_prompt})
        elif self.messages and self.messages[0]["role"] != "system":
            # å¦‚æœæ²¡æœ‰systemæ¶ˆæ¯ä½†æœ‰å…¶ä»–æ¶ˆæ¯ï¼Œæ·»åŠ é»˜è®¤systemæ¶ˆæ¯
            result.append({"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"})

        # æ·»åŠ å¯¹è¯å†å²ï¼ˆé™åˆ¶åœ¨kè½®ï¼‰
        recent_messages = self.messages[-self.k * 2:] if len(self.messages) > self.k * 2 else self.messages
        result.extend(recent_messages)

        return result

    def clear(self):
        """æ¸…ç©ºè®°å¿†"""
        self.messages = []


class ConversationTokenBufferMemory:
    """Tokené™åˆ¶è®°å¿† - åŸºäºtokenæ•°é‡é™åˆ¶å¯¹è¯å†å²"""

    def __init__(self, max_token_limit: int = 200):
        self.max_token_limit = max_token_limit
        self.messages: List[Dict] = []

    def _estimate_tokens(self, text: str) -> int:
        """ç²—ç•¥ä¼°ç®—tokenæ•°é‡ï¼ˆä¸­æ–‡çº¦4å­—ç¬¦=1tokenï¼‰"""
        return len(text) // 4

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯å¹¶æ£€æŸ¥tokené™åˆ¶"""
        self.messages.append({"role": role, "content": content})

        # æ£€æŸ¥æ€»tokenæ•°é‡
        total_tokens = sum(self._estimate_tokens(msg["content"]) for msg in self.messages)

        # å¦‚æœè¶…å‡ºé™åˆ¶ï¼Œåˆ é™¤æœ€æ—©çš„æ¶ˆæ¯
        while total_tokens > self.max_token_limit and len(self.messages) > 1:
            # ä¿ç•™systemæ¶ˆæ¯
            if self.messages[0]["role"] == "system":
                self.messages.pop(1)
            else:
                self.messages.pop(0)

            total_tokens = sum(self._estimate_tokens(msg["content"]) for msg in self.messages)

    def get_formatted_messages(self) -> List[Dict]:
        """è·å–æ ¼å¼åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨"""
        return self.messages.copy()

    def clear(self):
        """æ¸…ç©ºè®°å¿†"""
        self.messages = []


class ConversationSummaryMemory:
    """æ‘˜è¦è®°å¿† - è‡ªåŠ¨æ€»ç»“å¯¹è¯å†å²"""

    def __init__(self, llm):
        self.llm = llm
        self.summary = ""  # å­˜å‚¨å¯¹è¯æ‘˜è¦
        self.recent_messages: List[Dict] = []  # å­˜å‚¨æœ€è¿‘å‡ è½®å¯¹è¯

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯å¹¶æ›´æ–°æ‘˜è¦"""
        self.recent_messages.append({"role": role, "content": content})

        # åªä¿ç•™æœ€è¿‘2è½®å¯¹è¯ï¼ˆèŠ‚çœtokenï¼‰
        if len(self.recent_messages) > 4:  # 2è½®å¯¹è¯ = 4æ¡æ¶ˆæ¯
            self.recent_messages.pop(0)

        # æ›´æ–°æ‘˜è¦
        self._update_summary()

    def _update_summary(self):
        """ä½¿ç”¨LLMç”Ÿæˆå¯¹è¯æ‘˜è¦"""
        if len(self.recent_messages) < 4:  # è‡³å°‘2è½®å¯¹è¯æ‰ç”Ÿæˆæ‘˜è¦
            return

        summary_prompt = f"""
è¯·å°†ä»¥ä¸‹å¯¹è¯æ€»ç»“æˆç®€çŸ­çš„ä¸­æ–‡æ‘˜è¦ï¼ˆä¸è¶…è¿‡100å­—ï¼‰ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼š

{chr(10).join([f"{'ç”¨æˆ·' if msg['role']=='user' else 'åŠ©æ‰‹'}: {msg['content']}" for msg in self.recent_messages])}

æ‘˜è¦ï¼š
"""
        try:
            response = self.llm.invoke([HumanMessage(content=summary_prompt)])
            self.summary = response.content
        except Exception:
            pass  # æ‘˜è¦ç”Ÿæˆå¤±è´¥åˆ™è·³è¿‡

    def get_formatted_messages(self) -> List[Dict]:
        """è·å–æ ¼å¼åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨"""
        messages = []

        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if self.summary:
            messages.append({
                "role": "system",
                "content": f"å¯¹è¯æ‘˜è¦ï¼š{self.summary}"
            })
        else:
            messages.append({
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚"
            })

        messages.extend(self.recent_messages)
        return messages

    def clear(self):
        """æ¸…ç©ºè®°å¿†"""
        self.summary = ""
        self.recent_messages = []


# ========== ç¤ºä¾‹å‡½æ•° ==========

def conversation_buffer_window_memory_example():
    """çª—å£è®°å¿†ç¤ºä¾‹ - ä¿æŒå›ºå®šå¯¹è¯è½®æ•°"""
    print("=" * 60)
    print("ğŸªŸ ConversationBufferWindowMemory ç¤ºä¾‹ (v1.0)")
    print("=" * 60)

    model = get_glm_model()

    # åˆ›å»ºçª—å£è®°å¿† - åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯
    memory = ConversationBufferWindowMemory(k=3)

    system_prompt = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿè®°ä½æœ€è¿‘çš„å¯¹è¯å†…å®¹ã€‚"

    # æ¨¡æ‹Ÿå¯¹è¯
    conversations = [
        "ä½ å¥½ï¼Œæˆ‘å«å°æ˜",
        "æˆ‘å–œæ¬¢ç¼–ç¨‹å’Œäººå·¥æ™ºèƒ½",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "ä½ èƒ½æ¨èä¸€æœ¬Pythonä¹¦ç±å—ï¼Ÿ",
        "æˆ‘åˆšæ‰è¯´æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ",  # æµ‹è¯•è®°å¿†
        "æˆ‘æåˆ°æˆ‘å–œæ¬¢ä»€ä¹ˆï¼Ÿ"       # æµ‹è¯•è®°å¿†
    ]

    print("ğŸš€ å¼€å§‹å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(conversations, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ· [{i}]: {user_input}")

        # è·å–æ ¼å¼åŒ–æ¶ˆæ¯
        messages = memory.get_formatted_messages(system_prompt)
        messages.append({"role": "user", "content": user_input})

        # è°ƒç”¨æ¨¡å‹
        response = model.invoke(messages)
        print(f"ğŸ¤– AI: {response.content}")

        # æ›´æ–°è®°å¿†
        memory.add_message("user", user_input)
        memory.add_message("assistant", response.content)

        # æ˜¾ç¤ºå½“å‰è®°å¿†çŠ¶æ€
        print(f"ğŸ“ è®°å¿†ä¸­æ¶ˆæ¯æ•°: {len(memory.messages)}")


def conversation_token_buffer_memory_example():
    """Tokené™åˆ¶è®°å¿†ç¤ºä¾‹ - åŸºäºtokenæ•°é‡é™åˆ¶"""
    print("\n" + "=" * 60)
    print("ğŸª™ ConversationTokenBufferMemory ç¤ºä¾‹ (v1.0)")
    print("=" * 60)

    model = get_glm_model()

    # åˆ›å»ºtokené™åˆ¶è®°å¿† - æœ€å¤š200ä¸ªtoken
    memory = ConversationTokenBufferMemory(max_token_limit=200)

    # æ¨¡æ‹Ÿè¾ƒé•¿çš„å¯¹è¯
    conversations = [
        "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ä¸»è¦çš„åˆ†ç±»ï¼Ÿè¯·é€ä¸€ä»‹ç»ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯ã€‚",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
        "è¯·æ¨èä¸€äº›å­¦ä¹ æœºå™¨å­¦ä¹ çš„å…¥é—¨èµ„æºï¼ŒåŒ…æ‹¬ä¹¦ç±ã€åœ¨çº¿è¯¾ç¨‹å’Œå®è·µé¡¹ç›®ã€‚",
        "æˆ‘æƒ³çŸ¥é“æˆ‘çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"  # æµ‹è¯•è®°å¿†
    ]

    print("ğŸš€ å¼€å§‹å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(conversations, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ· [{i}]: {user_input[:30]}...")

        # è·å–æ ¼å¼åŒ–æ¶ˆæ¯
        messages = memory.get_formatted_messages()
        messages.append({"role": "user", "content": user_input})

        # è°ƒç”¨æ¨¡å‹
        response = model.invoke(messages)
        print(f"ğŸ¤– AI: {response.content[:50]}...")

        # æ›´æ–°è®°å¿†ï¼ˆä¼šè‡ªåŠ¨å¤„ç†tokené™åˆ¶ï¼‰
        memory.add_message("user", user_input)
        memory.add_message("assistant", response.content)

        # ä¼°ç®—tokenæ•°é‡
        total_chars = sum(len(msg["content"]) for msg in memory.messages)
        print(f"ğŸ“Š è®°å¿†ä¸­å­—ç¬¦æ•°: {total_chars}, ä¼°ç®—tokenæ•°: {total_chars // 4}")


def conversation_summary_memory_example():
    """æ‘˜è¦è®°å¿†ç¤ºä¾‹ - å¯¹è¯æ‘˜è¦è®°å¿†"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ ConversationSummaryMemory ç¤ºä¾‹ (v1.0)")
    print("=" * 60)

    model = get_glm_model()

    # åˆ›å»ºæ‘˜è¦è®°å¿†
    memory = ConversationSummaryMemory(llm=model)

    # æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
    conversations = [
        "æˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹",
        "æˆ‘æ²¡æœ‰ä»»ä½•ç¼–ç¨‹ç»éªŒï¼Œåº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ",
        "è¯·æ¨èä¸€äº›å…¥é—¨ä¹¦ç±å’Œåœ¨çº¿èµ„æº",
        "æˆ‘åº”è¯¥å…ˆå­¦Python 2è¿˜æ˜¯Python 3ï¼Ÿ",
        "æ ¹æ®ä¹‹å‰çš„å¯¹è¯ï¼Œæˆ‘åº”è¯¥æ€ä¹ˆå¼€å§‹ï¼Ÿ"
    ]

    print("ğŸš€ å¼€å§‹å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(conversations, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ· [{i}]: {user_input}")

        # è·å–æ ¼å¼åŒ–æ¶ˆæ¯ï¼ˆåŒ…å«æ‘˜è¦ï¼‰
        messages = memory.get_formatted_messages()
        messages.append({"role": "user", "content": user_input})

        # è°ƒç”¨æ¨¡å‹
        response = model.invoke(messages)
        print(f"ğŸ¤– AI: {response.content}")

        # æ›´æ–°è®°å¿†å’Œæ‘˜è¦
        memory.add_message("user", user_input)
        memory.add_message("assistant", response.content)

        # æ˜¾ç¤ºæ‘˜è¦
        if memory.summary:
            print(f"ğŸ“ å½“å‰æ‘˜è¦: {memory.summary}")


def conversation_summary_buffer_memory_example():
    """æ‘˜è¦ç¼“å†²è®°å¿†ç¤ºä¾‹ - ç»„åˆæ‘˜è¦å’Œç¼“å†²è®°å¿†"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ConversationSummaryBufferMemory ç¤ºä¾‹ (v1.0)")
    print("=" * 60)

    model = get_glm_model()

    # åˆ›å»ºç»„åˆè®°å¿†
    summary_memory = ConversationSummaryMemory(llm=model)
    buffer_memory = ConversationBufferWindowMemory(k=2)

    # æ¨¡æ‹Ÿé•¿æ—¶é—´å¯¹è¯
    conversations = [
        "æˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
        "1950å¹´ä»£æœ‰å“ªäº›é‡è¦äº‹ä»¶ï¼Ÿ",
        "å›¾çµæµ‹è¯•æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ä¸ºä»€ä¹ˆ1956å¹´è¢«ç§°ä¸ºAIçš„è¯ç”Ÿå¹´ï¼Ÿ",
        "ä¸“å®¶ç³»ç»Ÿåœ¨1980å¹´ä»£æœ‰ä»€ä¹ˆçªç ´ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ é©å‘½æ˜¯ä»ä»€ä¹ˆæ—¶å€™å¼€å§‹çš„ï¼Ÿ",
        "æ ¹æ®æˆ‘ä»¬è®¨è®ºçš„å†å²ï¼ŒAIçš„æœªæ¥è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æˆ‘ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯å…³äºä»€ä¹ˆçš„ï¼Ÿ"  # æµ‹è¯•è®°å¿†
    ]

    print("ğŸš€ å¼€å§‹é•¿æ—¶é—´å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(conversations, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ· [{i}]: {user_input[:30]}...")

        # è·å–æ‘˜è¦ä½œä¸ºç³»ç»Ÿæç¤º
        messages = summary_memory.get_formatted_messages()
        # æ·»åŠ æœ€è¿‘çš„ç¼“å†²æ¶ˆæ¯
        recent_messages = buffer_memory.get_formatted_messages()
        # åˆå¹¶æ¶ˆæ¯ï¼ˆé¿å…é‡å¤systemæ¶ˆæ¯ï¼‰
        if recent_messages and recent_messages[0]["role"] != "system":
            messages.extend(recent_messages)
        elif recent_messages:
            messages.extend(recent_messages[1:])  # è·³è¿‡é‡å¤çš„systemæ¶ˆæ¯

        messages.append({"role": "user", "content": user_input})

        # è°ƒç”¨æ¨¡å‹
        response = model.invoke(messages)
        print(f"ğŸ¤– AI: {response.content[:50]}...")

        # æ›´æ–°ä¸¤ç§è®°å¿†
        summary_memory.add_message("user", user_input)
        summary_memory.add_message("assistant", response.content)
        buffer_memory.add_message("user", user_input)
        buffer_memory.add_message("assistant", response.content)

        # æ˜¾ç¤ºæ‘˜è¦
        if summary_memory.summary:
            print(f"ğŸ“ æ‘˜è¦: {summary_memory.summary[:50]}...")


# ========== ä¸»å‡½æ•° ==========

if __name__ == "__main__":
    print("""
    ğŸ‰ LangChain v1.0 å¯¹è¯è®°å¿†ç¤ºä¾‹
    ============

    æ–°ç‰¹æ€§ï¼š
    1. ä½¿ç”¨Runnableå’Œmessages API
    2. æ‰‹åŠ¨ç®¡ç†å¯¹è¯å†å²
    3. æ”¯æŒçª—å£è®°å¿†ã€Tokené™åˆ¶è®°å¿†å’Œæ‘˜è¦è®°å¿†

    """)
    print()

    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    conversation_buffer_window_memory_example()
    conversation_token_buffer_memory_example()
    conversation_summary_memory_example()
    conversation_summary_buffer_memory_example()

    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("=" * 60)
