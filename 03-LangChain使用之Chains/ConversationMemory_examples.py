#!/usr/bin/env python3
"""
GLM-4.6 + LangChain ConversationMemory ç¤ºä¾‹ (ç°ä»£è¯­æ³•)
æ¼”ç¤ºå„ç§å¯¹è¯è®°å¿†ç±»å‹çš„ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µ
åŒ…æ‹¬ ConversationBufferWindowMemoryã€ConversationTokenBufferMemoryã€
ConversationSummaryMemoryã€ConversationSummaryBufferMemory
"""

import os
import dotenv
from typing import Dict, Any
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain.memory import (
    ConversationBufferWindowMemory,
    ConversationTokenBufferMemory,
    ConversationSummaryMemory,
    ConversationSummaryBufferMemory
)
from langchain.chains import LLMChain

# åŠ è½½ç¯å¢ƒå˜é‡ - ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½.envæ–‡ä»¶
dotenv.load_dotenv(dotenv_path="../.env")

# æ£€æŸ¥APIå¯†é’¥
api_key = os.getenv("ZHIPUAI_API_KEY")
if not api_key or api_key == "your-zhipu-api-key-here":
    print("âŒ é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ZHIPUAI_API_KEY")
    exit(1)

# åˆå§‹åŒ–GLMæ¨¡å‹
def get_glm_model(temperature: float = 0.7):
    """è·å–GLMæ¨¡å‹å®ä¾‹"""
    return ChatZhipuAI(
        model="glm-4.6",
        temperature=temperature,
        api_key=api_key
    )

def conversation_buffer_window_memory_example():
    """ConversationBufferWindowMemory ç¤ºä¾‹ - ä¿æŒå›ºå®šå¯¹è¯è½®æ•°"""
    print("=" * 60)
    print("ğŸªŸ ConversationBufferWindowMemory ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # åˆ›å»ºçª—å£è®°å¿† - åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯
    memory = ConversationBufferWindowMemory(
        k=3,  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
        return_messages=True,
        memory_key="chat_history"
    )

    # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿè®°ä½æœ€è¿‘çš„å¯¹è¯å†…å®¹ã€‚"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

    # ç”±äºmemoryéœ€è¦ä¼ ç»Ÿé“¾ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°
    def run_with_memory(user_input: str) -> str:
        # æ ¼å¼åŒ–æ¶ˆæ¯
        messages = prompt.format_messages(
            input=user_input,
            chat_history=memory.chat_memory.messages
        )

        # è°ƒç”¨æ¨¡å‹
        response = model.invoke(messages)

        # ä¿å­˜å¯¹è¯åˆ°è®°å¿†
        memory.chat_memory.add_user_message(user_input)
        memory.chat_memory.add_ai_message(response.content)

        return response.content

    # æ¨¡æ‹Ÿå¯¹è¯
    conversations = [
        "ä½ å¥½ï¼Œæˆ‘å«å°æ˜",
        "æˆ‘å–œæ¬¢ç¼–ç¨‹å’Œäººå·¥æ™ºèƒ½",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "ä½ èƒ½æ¨èä¸€æœ¬Pythonä¹¦ç±å—ï¼Ÿ",
        "æˆ‘åˆšæ‰è¯´æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ",  # æµ‹è¯•è®°å¿†
        "æˆ‘æåˆ°æˆ‘å–œæ¬¢ä»€ä¹ˆï¼Ÿ"       # æµ‹è¯•è®°å¿†
    ]

    print("ğŸš€ å¼€å§‹å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(conversations, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ· [{i}]: {user_input}")

        response = run_with_memory(user_input)
        print(f"ğŸ¤– AI: {response}")

        # æ˜¾ç¤ºå½“å‰è®°å¿†ä¸­çš„æ¶ˆæ¯æ•°é‡
        print(f"ğŸ“ è®°å¿†ä¸­æ¶ˆæ¯æ•°: {len(memory.chat_memory.messages)}")

    print(f"\nğŸ¯ è®°å¿†ä¸­çš„æœ€å{memory.k}è½®å¯¹è¯:")
    for msg in memory.chat_memory.messages:
        msg_type = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AI"
        print(f"  {msg_type}: {msg.content[:50]}...")

def conversation_token_buffer_memory_example():
    """ConversationTokenBufferMemory ç¤ºä¾‹ - åŸºäºtokenæ•°é‡é™åˆ¶"""
    print("\n" + "=" * 60)
    print("ğŸª™ ConversationTokenBufferMemory ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # åˆ›å»ºtokené™åˆ¶è®°å¿† - æœ€å¤š200ä¸ªtoken
    memory = ConversationTokenBufferMemory(
        llm=model,
        max_token_limit=200,  # æœ€å¤š200ä¸ªtoken
        return_messages=True,
        memory_key="chat_history"
    )

    # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯ä¸“å®¶åŠ©æ‰‹ï¼Œå¯¹è¯è®°å¿†åŸºäºtokenæ•°é‡é™åˆ¶ã€‚"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

    def run_with_memory(user_input: str) -> str:
        messages = prompt.format_messages(
            input=user_input,
            chat_history=memory.chat_memory.messages
        )

        response = model.invoke(messages)

        memory.chat_memory.add_user_message(user_input)
        memory.chat_memory.add_ai_message(response.content)

        return response.content

    # æ¨¡æ‹Ÿè¾ƒé•¿çš„å¯¹è¯
    conversations = [
        "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ä¸»è¦çš„åˆ†ç±»ï¼Ÿè¯·é€ä¸€ä»‹ç»ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯ã€‚",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
        "è¯·æ¨èä¸€äº›å­¦ä¹ æœºå™¨å­¦ä¹ çš„å…¥é—¨èµ„æºï¼ŒåŒ…æ‹¬ä¹¦ç±ã€åœ¨çº¿è¯¾ç¨‹å’Œå®è·µé¡¹ç›®ã€‚",
        "æˆ‘æƒ³çŸ¥é“æˆ‘çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"  # æµ‹è¯•è®°å¿†
    ]

    print("ğŸš€ å¼€å§‹å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(conversations, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ· [{i}]: {user_input}")

        response = run_with_memory(user_input)
        print(f"ğŸ¤– AI: {response[:100]}...")

        # ä¼°ç®—tokenæ•°é‡
        total_chars = sum(len(msg.content) for msg in memory.chat_memory.messages)
        estimated_tokens = total_chars // 4  # ç²—ç•¥ä¼°ç®—
        print(f"ğŸ“Š è®°å¿†ä¸­å­—ç¬¦æ•°: {total_chars}, ä¼°ç®—tokenæ•°: {estimated_tokens}")

def conversation_summary_memory_example():
    """ConversationSummaryMemory ç¤ºä¾‹ - å¯¹è¯æ‘˜è¦è®°å¿†"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ ConversationSummaryMemory ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # åˆ›å»ºæ‘˜è¦è®°å¿†
    memory = ConversationSummaryMemory(
        llm=model,
        return_messages=True,
        memory_key="chat_history"
    )

    # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦ä¹ åŠ©æ‰‹ï¼Œèƒ½å¤Ÿè®°ä½å¹¶æ€»ç»“å¯¹è¯å†å²ã€‚"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

    def run_with_memory(user_input: str) -> str:
        # æ·»åŠ å½“å‰æ¶ˆæ¯åˆ°è®°å¿†
        memory.save_context(
            {"input": user_input},
            {"output": "æˆ‘ç†è§£äº†æ‚¨çš„é—®é¢˜ï¼Œæ­£åœ¨ä¸ºæ‚¨è§£ç­”..."}
        )

        messages = prompt.format_messages(
            input=user_input,
            chat_history=[SystemMessage(content=memory.buffer)]
        )

        response = model.invoke(messages)

        # æ›´æ–°è¾“å‡ºåˆ°è®°å¿†
        memory.chat_memory.add_ai_message(response.content)

        return response.content

    # æ¨¡æ‹Ÿå­¦ä¹ ç›¸å…³çš„å¯¹è¯
    conversations = [
        "æˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹ï¼Œåº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ",
        "Pythonæœ‰å“ªäº›ä¸»è¦çš„åº”ç”¨é¢†åŸŸï¼Ÿ",
        "å­¦ä¹ Pythonéœ€è¦ä»€ä¹ˆåŸºç¡€çŸ¥è¯†ï¼Ÿ",
        "ä½ èƒ½æ¨èä¸€äº›Pythonå­¦ä¹ èµ„æºå—ï¼Ÿ",
        "æˆ‘åº”è¯¥å¦‚ä½•åˆ¶å®šå­¦ä¹ è®¡åˆ’ï¼Ÿ",
        "è¯·æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬çš„å¯¹è¯å†…å®¹"  # æµ‹è¯•æ‘˜è¦
    ]

    print("ğŸš€ å¼€å§‹å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(conversations, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ· [{i}]: {user_input}")

        response = run_with_memory(user_input)
        print(f"ğŸ¤– AI: {response[:100]}...")

        print(f"ğŸ“ å½“å‰æ‘˜è¦é•¿åº¦: {len(memory.buffer)} å­—ç¬¦")

    print(f"\nğŸ¯ å®Œæ•´å¯¹è¯æ‘˜è¦:")
    print(f"{memory.buffer}")

def conversation_summary_buffer_memory_example():
    """ConversationSummaryBufferMemory ç¤ºä¾‹ - æ··åˆæ‘˜è¦å’Œç¼“å†²è®°å¿†"""
    print("\n" + "=" * 60)
    print("ğŸ”„ ConversationSummaryBufferMemory ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # åˆ›å»ºæ··åˆè®°å¿† - æ‘˜è¦ + æœ€è¿‘2æ¡æ¶ˆæ¯
    memory = ConversationSummaryBufferMemory(
        llm=model,
        max_token_limit=300,  # æœ€å¤§tokené™åˆ¶
        return_messages=True,
        memory_key="chat_history"
    )

    # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªé¡¹ç›®ç®¡ç†åŠ©æ‰‹ï¼Œä½¿ç”¨æ··åˆè®°å¿†ç­–ç•¥ç®¡ç†å¯¹è¯å†å²ã€‚"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

    def run_with_memory(user_input: str) -> str:
        messages = prompt.format_messages(
            input=user_input,
            chat_history=memory.buffer + memory.chat_memory.messages
        )

        response = model.invoke(messages)

        memory.save_context(
            {"input": user_input},
            {"output": response.content}
        )

        return response.content

    # æ¨¡æ‹Ÿé¡¹ç›®ç®¡ç†å¯¹è¯
    conversations = [
        "æˆ‘éœ€è¦ç®¡ç†ä¸€ä¸ªè½¯ä»¶å¼€å‘é¡¹ç›®ï¼Œæœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ",
        "å¦‚ä½•åˆ¶å®šé¡¹ç›®è®¡åˆ’å’Œæ—¶é—´çº¿ï¼Ÿ",
        "å›¢é˜Ÿåä½œå·¥å…·æ¨èå“ªäº›ï¼Ÿ",
        "å¦‚ä½•è¿›è¡Œæœ‰æ•ˆçš„è¿›åº¦è·Ÿè¸ªï¼Ÿ",
        "é¡¹ç›®é£é™©ç®¡ç†éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ",
        "æˆ‘ä¹‹å‰é—®çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"  # æµ‹è¯•è®°å¿†
    ]

    print("ğŸš€ å¼€å§‹å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(conversations, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ· [{i}]: {user_input}")

        response = run_with_memory(user_input)
        print(f"ğŸ¤– AI: {response[:100]}...")

        # æ˜¾ç¤ºè®°å¿†çŠ¶æ€
        summary_len = len(memory.buffer) if memory.buffer else 0
        recent_msgs = len(memory.chat_memory.messages)
        print(f"ğŸ“Š æ‘˜è¦é•¿åº¦: {summary_len}, æœ€è¿‘æ¶ˆæ¯æ•°: {recent_msgs}")

    print(f"\nğŸ¯ è®°å¿†çŠ¶æ€:")
    print(f"ğŸ“‹ æ‘˜è¦: {memory.buffer[:200]}...")
    print(f"ğŸ’¬ æœ€è¿‘æ¶ˆæ¯:")
    for msg in memory.chat_memory.messages[-2:]:  # æ˜¾ç¤ºæœ€å2æ¡æ¶ˆæ¯
        msg_type = "ç”¨æˆ·" if isinstance(msg, HumanMessage) else "AI"
        print(f"  {msg_type}: {msg.content[:50]}...")

def memory_comparison():
    """è®°å¿†ç±»å‹æ¯”è¾ƒ"""
    print("\n" + "=" * 60)
    print("âš–ï¸ å¯¹è¯è®°å¿†ç±»å‹æ¯”è¾ƒ")
    print("=" * 60)

    print("""
ğŸ“Š å››ç§å¯¹è¯è®°å¿†ç±»å‹å¯¹æ¯”:

ğŸªŸ ConversationBufferWindowMemory:
âœ… ä¼˜ç‚¹:
   - ä¿æŒå›ºå®šæ•°é‡çš„å¯¹è¯è½®æ•°
   - ç®€å•ç›´è§‚çš„æ»‘åŠ¨çª—å£æœºåˆ¶
   - é€‚åˆçŸ­æœŸå¯¹è¯åœºæ™¯

âŒ ç¼ºç‚¹:
   - å¯èƒ½ä¸¢å¤±é‡è¦çš„æ—©æœŸå¯¹è¯
   - ä¸è€ƒè™‘å†…å®¹é‡è¦æ€§

ğŸª™ ConversationTokenBufferMemory:
âœ… ä¼˜ç‚¹:
   - åŸºäºtokenæ•°é‡ç²¾ç¡®æ§åˆ¶
   - è€ƒè™‘æ¶ˆæ¯é•¿åº¦å·®å¼‚
   - é€‚åˆæœ‰ä¸¥æ ¼tokené™åˆ¶çš„åœºæ™¯

âŒ ç¼ºç‚¹:
   - å¯èƒ½æˆªæ–­é‡è¦ä¿¡æ¯
   - tokenè®¡ç®—å¯èƒ½æœ‰è¯¯å·®

ğŸ“‹ ConversationSummaryMemory:
âœ… ä¼˜ç‚¹:
   - ä¿æŒå¯¹è¯çš„å®Œæ•´æ‘˜è¦
   - èŠ‚çœå­˜å‚¨ç©ºé—´
   - é€‚åˆé•¿æœŸå¯¹è¯

âŒ ç¼ºç‚¹:
   - å¯èƒ½ä¸¢å¤±ç»†èŠ‚ä¿¡æ¯
   - æ‘˜è¦è´¨é‡ä¾èµ–æ¨¡å‹èƒ½åŠ›

ğŸ”„ ConversationSummaryBufferMemory:
âœ… ä¼˜ç‚¹:
   - å¹³è¡¡æ‘˜è¦å’Œè¯¦ç»†è®°å½•
   - ä¿æŒæœ€è¿‘çš„å®Œæ•´å¯¹è¯
   - é€‚åˆå¤æ‚çš„é•¿æœŸå¯¹è¯

âŒ ç¼ºç‚¹:
   - é…ç½®ç›¸å¯¹å¤æ‚
   - éœ€è¦è°ƒä¼˜å‚æ•°

ğŸ¯ ä½¿ç”¨å»ºè®®:
1. çŸ­æœŸå¯¹è¯ â†’ ConversationBufferWindowMemory
2. ä¸¥æ ¼çš„tokené™åˆ¶ â†’ ConversationTokenBufferMemory
3. é•¿æœŸå¯¹è¯ä¸”éœ€è¦æ‘˜è¦ â†’ ConversationSummaryMemory
4. éœ€è¦å¹³è¡¡çš„åœºæ™¯ â†’ ConversationSummaryBufferMemory
    """)

def modern_syntax_memory_example():
    """ç°ä»£è¯­æ³•è®°å¿†ç¤ºä¾‹ - ä½¿ç”¨ RunnablePassthrough"""
    print("\n" + "=" * 60)
    print("ğŸš€ ç°ä»£è¯­æ³•è®°å¿†ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # åˆ›å»ºç®€å•çš„çª—å£è®°å¿†
    memory = ConversationBufferWindowMemory(
        k=2,
        return_messages=True,
        memory_key="chat_history"
    )

    # ç°ä»£è¯­æ³•çš„å¤„ç†é“¾
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä½¿ç”¨ç°ä»£è¯­æ³•çš„AIåŠ©æ‰‹ã€‚"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])

    chain = prompt | model | StrOutputParser()

    # åˆ›å»ºåŒ…è£…å‡½æ•°æ¥é›†æˆè®°å¿†
    def create_memory_chain():
        def run_chain(inputs: Dict[str, Any]) -> str:
            # è·å–å†å²æ¶ˆæ¯
            chat_history = memory.chat_memory.messages

            # å‡†å¤‡è¾“å…¥
            chain_inputs = {
                "input": inputs["input"],
                "chat_history": chat_history
            }

            # è¿è¡Œé“¾
            response = chain.invoke(chain_inputs)

            # ä¿å­˜åˆ°è®°å¿†
            memory.save_context(
                {"input": inputs["input"]},
                {"output": response}
            )

            return response

        return run_chain

    # åˆ›å»ºå¸¦è®°å¿†çš„é“¾
    memory_chain = create_memory_chain()

    # æµ‹è¯•å¯¹è¯
    print("ğŸš€ ä½¿ç”¨ç°ä»£è¯­æ³•æµ‹è¯•å¯¹è¯è®°å¿†...")

    test_inputs = [
        {"input": "ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ LangChain"},
        {"input": "LangChainæœ‰å“ªäº›ä¸»è¦åŠŸèƒ½ï¼Ÿ"},
        {"input": "ä»€ä¹ˆæ˜¯Runnableï¼Ÿ"},
        {"input": "æˆ‘åˆšæ‰ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"}
    ]

    for i, inputs in enumerate(test_inputs, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ· [{i}]: {inputs['input']}")
        response = memory_chain(inputs)
        print(f"ğŸ¤– AI: {response[:100]}...")
        print(f"ğŸ“ è®°å¿†ä¸­æ¶ˆæ¯æ•°: {len(memory.chat_memory.messages)}")

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ GLM-4.6 + LangChain ConversationMemory è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)

    try:
        # è¿è¡Œå„ç§ç¤ºä¾‹
        conversation_buffer_window_memory_example()
        conversation_token_buffer_memory_example()
        conversation_summary_memory_example()
        conversation_summary_buffer_memory_example()
        memory_comparison()
        modern_syntax_memory_example()

        print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ“š æ›´å¤šä¿¡æ¯è¯·å‚è€ƒï¼š")
        print("- LangChainå®˜æ–¹æ–‡æ¡£: https://python.langchain.com/")
        print("- è®°å¿†ç»„ä»¶æŒ‡å—: https://python.langchain.com/docs/modules/memory/")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{e}")

if __name__ == "__main__":
    main()