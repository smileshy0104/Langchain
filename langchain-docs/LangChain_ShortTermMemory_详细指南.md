# LangChain Short-term Memory è¯¦ç»†æŒ‡å—

> åŸºäºå®˜æ–¹æ–‡æ¡£ https://docs.langchain.com/oss/python/langchain/short-term-memory çš„å®Œæ•´ä¸­æ–‡æ€»ç»“

---

## ğŸ“‹ ç›®å½•

- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [ä¸ºä»€ä¹ˆéœ€è¦çŸ­æœŸè®°å¿†](#ä¸ºä»€ä¹ˆéœ€è¦çŸ­æœŸè®°å¿†)
- [çŸ­æœŸè®°å¿†çš„å®ç°æ–¹å¼](#çŸ­æœŸè®°å¿†çš„å®ç°æ–¹å¼)
- [å¸¸è§çš„è®°å¿†ç®¡ç†æ¨¡å¼](#å¸¸è§çš„è®°å¿†ç®¡ç†æ¨¡å¼)
- [æ¶ˆæ¯ä¿®å‰ª (Trim Messages)](#æ¶ˆæ¯ä¿®å‰ª-trim-messages)
- [æ¶ˆæ¯åˆ é™¤ (Delete Messages)](#æ¶ˆæ¯åˆ é™¤-delete-messages)
- [æ¶ˆæ¯æ€»ç»“ (Summarize Messages)](#æ¶ˆæ¯æ€»ç»“-summarize-messages)
- [è‡ªå®šä¹‰ Agent è®°å¿†](#è‡ªå®šä¹‰-agent-è®°å¿†)
- [Checkpointer çš„ä½¿ç”¨](#checkpointer-çš„ä½¿ç”¨)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯çŸ­æœŸè®°å¿† (Short-term Memory)ï¼Ÿ

**çŸ­æœŸè®°å¿†**æ˜¯ä¸€ç§è®©åº”ç”¨ç¨‹åºåœ¨å•ä¸ªçº¿ç¨‹æˆ–å¯¹è¯ä¸­è®°ä½ä¹‹å‰äº¤äº’çš„ç³»ç»Ÿã€‚å®ƒæ˜¯ AI Agent è®°å¿†ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½¿ Agent èƒ½å¤Ÿï¼š

- ğŸ“ è®°ä½ä¹‹å‰çš„å¯¹è¯å†…å®¹
- ğŸ”„ ä»ç”¨æˆ·åé¦ˆä¸­å­¦ä¹ 
- ğŸ¯ é€‚åº”ç”¨æˆ·åå¥½
- ğŸ’¬ ç»´æŒè¿è´¯çš„å¤šè½®å¯¹è¯

### çŸ­æœŸè®°å¿† vs é•¿æœŸè®°å¿†

| ç‰¹æ€§ | çŸ­æœŸè®°å¿† (Short-term) | é•¿æœŸè®°å¿† (Long-term) |
|------|---------------------|---------------------|
| **ä½œç”¨åŸŸ** | å•ä¸ªå¯¹è¯çº¿ç¨‹ (Thread) | è·¨å¤šä¸ªä¼šè¯ |
| **ç”Ÿå‘½å‘¨æœŸ** | ä¼šè¯æœŸé—´ | æ°¸ä¹…å­˜å‚¨ |
| **å­˜å‚¨ä½ç½®** | Graph State | Store |
| **å…¸å‹å†…å®¹** | å¯¹è¯å†å²ã€ä¸´æ—¶æ•°æ® | ç”¨æˆ·åå¥½ã€å†å²äº¤äº’ |
| **ç®¡ç†æ–¹å¼** | Checkpointer | BaseStore |
| **æ›´æ–°æ—¶æœº** | æ¯æ¬¡ invoke/step | æŒ‰éœ€æ›´æ–° |

### çº¿ç¨‹ (Thread) çš„æ¦‚å¿µ

**Threadï¼ˆçº¿ç¨‹ï¼‰** ç»„ç»‡å•ä¸ªä¼šè¯ä¸­çš„å¤šä¸ªäº¤äº’ï¼Œç±»ä¼¼äºç”µå­é‚®ä»¶ä¸­çš„å¯¹è¯çº¿ç¨‹ã€‚

```python
from langchain.agents import create_agent
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
agent = create_agent(model="gpt-5.0", tools=[], checkpointer=checkpointer)

# åŒä¸€çº¿ç¨‹çš„å¤šæ¬¡äº¤äº’
config = {"configurable": {"thread_id": "conversation_1"}}

# ç¬¬ä¸€è½®å¯¹è¯
agent.invoke({"messages": "ä½ å¥½ï¼Œæˆ‘å« Alice"}, config)

# ç¬¬äºŒè½®å¯¹è¯ - Agent è®°ä½ä¹‹å‰çš„å¯¹è¯
agent.invoke({"messages": "æˆ‘å«ä»€ä¹ˆåå­—?"}, config)
# è¾“å‡º: "ä½ å« Alice"
```

---

## ä¸ºä»€ä¹ˆéœ€è¦çŸ­æœŸè®°å¿†

### 1. ä¸Šä¸‹æ–‡çª—å£çš„æŒ‘æˆ˜

å¤§å¤šæ•° LLM éƒ½æœ‰æœ€å¤§ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼š

- **gpt-5.0**: 400K tokens
- **Claude 4.5 Sonnet**: 200K tokens  
- **Gemini 3.0 Pro**: 1M tokens

é•¿å¯¹è¯å¯èƒ½è¶…å‡ºè¿™äº›é™åˆ¶ï¼Œå¯¼è‡´ï¼š
- âŒ ä¸Šä¸‹æ–‡ä¸¢å¤±æˆ–é”™è¯¯
- âŒ å“åº”æ—¶é—´å˜æ…¢
- âŒ API æˆæœ¬å¢åŠ 

### 2. é•¿ä¸Šä¸‹æ–‡æ€§èƒ½é—®é¢˜

å³ä½¿æ¨¡å‹æ”¯æŒé•¿ä¸Šä¸‹æ–‡ï¼Œä¹Ÿå­˜åœ¨é—®é¢˜ï¼š

- **æ³¨æ„åŠ›åˆ†æ•£**: è¢«è¿‡æ—¶æˆ–æ— å…³å†…å®¹å¹²æ‰°
- **æ€§èƒ½ä¸‹é™**: å¤„ç†æ—¶é—´å’Œè´¨é‡éƒ½ä¼šä¸‹é™
- **æˆæœ¬å¢åŠ **: Token ä½¿ç”¨é‡æ¿€å¢

### 3. å®é™…æ¡ˆä¾‹

```python
# é—®é¢˜åœºæ™¯ï¼š100 è½®å¯¹è¯å
messages = [
    SystemMessage("ä½ æ˜¯åŠ©æ‰‹"),
    HumanMessage("é—®é¢˜1"),
    AIMessage("ç­”æ¡ˆ1"),
    # ... 200+ æ¡æ¶ˆæ¯
    HumanMessage("æœ€æ–°é—®é¢˜")
]

# å¯èƒ½é‡åˆ°çš„é—®é¢˜ï¼š
# - è¶…å‡ºä¸Šä¸‹æ–‡çª—å£é™åˆ¶
# - æ¨¡å‹è¢«æ—©æœŸæ— å…³å¯¹è¯å¹²æ‰°
# - Token æˆæœ¬è¿‡é«˜
```

---

## çŸ­æœŸè®°å¿†çš„å®ç°æ–¹å¼

### 1. ä½¿ç”¨ MessagesState

LangChain æä¾›äº†é¢„æ„å»ºçš„ `MessagesState` æ¥ç®¡ç†å¯¹è¯å†å²ã€‚

```python
from langgraph.graph import MessagesState

# MessagesState åŒ…å«ä¸€ä¸ª messages é”®
class State(MessagesState):
    # å¯ä»¥æ·»åŠ å…¶ä»–å­—æ®µ
    documents: list[str]
```

**MessagesState çš„ç‰¹ç‚¹**:
- è‡ªåŠ¨åŒ…å« `messages` é”®
- ä½¿ç”¨ `add_messages` reducer
- æ”¯æŒæ¶ˆæ¯çš„æ·»åŠ ã€æ›´æ–°å’Œåˆ é™¤

### 2. ä½¿ç”¨ add_messages Reducer

`add_messages` æ˜¯ä¸€ä¸ªæ™ºèƒ½ reducerï¼Œèƒ½å¤Ÿï¼š

```python
from langgraph.graph import add_messages
from typing import Annotated, TypedDict
from langchain_core.messages import AnyMessage

class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

# add_messages çš„åŠŸèƒ½ï¼š
# 1. æ·»åŠ æ–°æ¶ˆæ¯åˆ°åˆ—è¡¨
# 2. æ›´æ–°å…·æœ‰ç›¸åŒ ID çš„æ¶ˆæ¯
# 3. ä¿æŒæ¶ˆæ¯é¡ºåº
```

**ç¤ºä¾‹**:

```python
from langgraph.graph import StateGraph, MessagesState
from langchain_core.messages import HumanMessage, AIMessage

def chatbot(state: MessagesState):
    return {"messages": [AIMessage(content="ä½ å¥½ï¼")]}

graph = StateGraph(MessagesState)
graph.add_node("chatbot", chatbot)
graph.set_entry_point("chatbot")

compiled = graph.compile()

# è°ƒç”¨
result = compiled.invoke({
    "messages": [HumanMessage(content="ä½ å¥½")]
})

print(result["messages"])
# [HumanMessage("ä½ å¥½"), AIMessage("ä½ å¥½ï¼")]
```

### 3. å¯ç”¨æŒä¹…åŒ–

ä½¿ç”¨ **Checkpointer** åœ¨å¤šæ¬¡è°ƒç”¨ä¹‹é—´ä¿æŒçŠ¶æ€ï¼š

```python
from langgraph.checkpoint.memory import MemorySaver
from langchain.agents import create_agent

# åˆ›å»ºå†…å­˜ä¿å­˜å™¨
checkpointer = MemorySaver()

# åˆ›å»º Agent
agent = create_agent(
    model="gpt-5.0",
    tools=[],
    checkpointer=checkpointer
)

# é…ç½®çº¿ç¨‹ ID
config = {"configurable": {"thread_id": "1"}}

# ç¬¬ä¸€æ¬¡è°ƒç”¨
agent.invoke({"messages": "æˆ‘å–œæ¬¢ Python"}, config)

# ç¬¬äºŒæ¬¡è°ƒç”¨ - ä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯
agent.invoke({"messages": "æˆ‘å–œæ¬¢ä»€ä¹ˆ?"}, config)
# è¾“å‡º: "ä½ å–œæ¬¢ Python"
```

---

## å¸¸è§çš„è®°å¿†ç®¡ç†æ¨¡å¼

å½“å¯¹è¯å˜é•¿æ—¶ï¼Œéœ€è¦é‡‡ç”¨ç­–ç•¥ç®¡ç†æ¶ˆæ¯å†å²ï¼š

### 1. æ¶ˆæ¯ä¿®å‰ª (Trim Messages)

ä¿ç•™æœ€è¿‘çš„ N æ¡æ¶ˆæ¯ï¼Œåˆ é™¤è¾ƒæ—©çš„æ¶ˆæ¯ã€‚

**ä¼˜ç‚¹**:
- âœ… ç®€å•ç›´æ¥
- âœ… æ€§èƒ½å¼€é”€å°
- âœ… å¯é¢„æµ‹çš„ token ä½¿ç”¨

**ç¼ºç‚¹**:
- âŒ å¯èƒ½ä¸¢å¤±é‡è¦çš„æ—©æœŸä¿¡æ¯
- âŒ ä¸Šä¸‹æ–‡å¯èƒ½ä¸å®Œæ•´

### 2. æ¶ˆæ¯åˆ é™¤ (Delete Messages)

æ°¸ä¹…ä»çŠ¶æ€ä¸­åˆ é™¤ç‰¹å®šæ¶ˆæ¯ã€‚

**ä¼˜ç‚¹**:
- âœ… ç²¾ç¡®æ§åˆ¶ä¿ç•™å†…å®¹
- âœ… å¯ä»¥åˆ é™¤æ•æ„Ÿä¿¡æ¯

**ç¼ºç‚¹**:
- âŒ ä¸å¯æ¢å¤
- âŒ éœ€è¦ä»”ç»†ç®¡ç†æ¶ˆæ¯æœ‰æ•ˆæ€§

### 3. æ¶ˆæ¯æ€»ç»“ (Summarize Messages)

ä½¿ç”¨æ¨¡å‹æ€»ç»“æ—©æœŸæ¶ˆæ¯ï¼Œç”¨æ‘˜è¦æ›¿æ¢åŸæ¶ˆæ¯ã€‚

**ä¼˜ç‚¹**:
- âœ… ä¿ç•™å…³é”®ä¿¡æ¯
- âœ… æ”¯æŒæ›´é•¿çš„æœ‰æ•ˆå¯¹è¯
- âœ… å¹³è¡¡äº†ä¸Šä¸‹æ–‡å’Œé•¿åº¦

**ç¼ºç‚¹**:
- âŒ é¢å¤–çš„ LLM è°ƒç”¨æˆæœ¬
- âŒ å¯èƒ½ä¸¢å¤±ç»†èŠ‚

### 4. è‡ªå®šä¹‰ç­–ç•¥

æ ¹æ®ä¸šåŠ¡éœ€æ±‚å®ç°ç‰¹å®šçš„è¿‡æ»¤æˆ–ç®¡ç†é€»è¾‘ã€‚

**ç¤ºä¾‹**: åªä¿ç•™åŒ…å«ç‰¹å®šå…³é”®è¯çš„æ¶ˆæ¯ã€åŸºäºé‡è¦æ€§è¯„åˆ†ç­‰ã€‚

---

## æ¶ˆæ¯ä¿®å‰ª (Trim Messages)

### åŸºæœ¬ä¿®å‰ªç­–ç•¥

ä½¿ç”¨ `trim_messages` å‡½æ•°æŒ‰ token æ•°é‡ä¿®å‰ªæ¶ˆæ¯ï¼š

```python
from langchain_core.messages import trim_messages
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹"),
    HumanMessage(content="ä½ å¥½"),
    AIMessage(content="ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"),
    HumanMessage(content="æˆ‘æƒ³äº†è§£ AI"),
    AIMessage(content="AI æ˜¯äººå·¥æ™ºèƒ½çš„ç¼©å†™..."),
    HumanMessage(content="å‘Šè¯‰æˆ‘æ›´å¤š"),
]

# ä¿®å‰ªåˆ°æœ€å¤š 1000 tokens
trimmed = trim_messages(
    messages,
    max_tokens=1000,
    strategy="last",  # ä¿ç•™æœ€åçš„æ¶ˆæ¯
    token_counter=len,  # ä½¿ç”¨ç®€å•çš„è®¡æ•°å™¨
    include_system=True  # å§‹ç»ˆä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
)
```

### åœ¨ Agent ä¸­ä½¿ç”¨ä¿®å‰ª

#### æ–¹æ³• 1: ä½¿ç”¨ @before_model ä¸­é—´ä»¶

```python
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import before_model
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.runtime import Runtime

@before_model
def trim_messages(state: AgentState, runtime: Runtime) -> dict | None:
    """ä¿ç•™æœ€è¿‘çš„å‡ æ¡æ¶ˆæ¯ä»¥é€‚åº”ä¸Šä¸‹æ–‡çª—å£"""
    messages = state["messages"]
    
    if len(messages) <= 3:
        return None  # ä¸éœ€è¦ä¿®å‰ª
    
    # ä¿ç•™ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼ˆé€šå¸¸æ˜¯ç³»ç»Ÿæ¶ˆæ¯ï¼‰
    first_msg = messages[0]
    
    # ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]
    new_messages = [first_msg] + recent_messages
    
    return {
        "messages": [
            RemoveMessage(id=REMOVE_ALL_MESSAGES),
            *new_messages
        ]
    }

agent = create_agent(
    model="gpt-5.0",
    tools=[],
    middleware=[trim_messages],
    checkpointer=InMemorySaver(),
)

config = {"configurable": {"thread_id": "1"}}

agent.invoke({"messages": "ä½ å¥½ï¼Œæˆ‘å« Bob"}, config)
agent.invoke({"messages": "å†™ä¸€é¦–å…³äºçŒ«çš„è¯—"}, config)
agent.invoke({"messages": "ç°åœ¨å†™ä¸€é¦–å…³äºç‹—çš„è¯—"}, config)
result = agent.invoke({"messages": "æˆ‘å«ä»€ä¹ˆåå­—?"}, config)

result["messages"][-1].pretty_print()
# è¾“å‡º: "ä½ å« Bobã€‚ä½ ä¹‹å‰å‘Šè¯‰è¿‡æˆ‘ã€‚"
```

#### æ–¹æ³• 2: ä½¿ç”¨ LangGraph æ‰‹åŠ¨ä¿®å‰ª

```python
from langchain_core.messages import trim_messages
from langchain_anthropic import ChatAnthropic
from langgraph.graph import StateGraph, START, MessagesState
from langgraph.checkpoint.memory import MemorySaver

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")

def call_model(state: MessagesState):
    # åœ¨è°ƒç”¨æ¨¡å‹å‰ä¿®å‰ªæ¶ˆæ¯
    trimmed = trim_messages(
        state["messages"],
        max_tokens=128,
        strategy="last",
        token_counter=model,  # ä½¿ç”¨æ¨¡å‹çš„ token è®¡æ•°å™¨
        start_on="human",
        end_on=["human", "tool"],
    )
    response = model.invoke(trimmed)
    return {"messages": [response]}

checkpointer = MemorySaver()
builder = StateGraph(MessagesState)
builder.add_node("call_model", call_model)
builder.add_edge(START, "call_model")
graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}
graph.invoke({"messages": [{"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘å« Bob"}]}, config)
graph.invoke({"messages": [{"role": "user", "content": "å†™ä¸€é¦–å…³äºçŒ«çš„è¯—"}]}, config)
result = graph.invoke({"messages": [{"role": "user", "content": "æˆ‘å«ä»€ä¹ˆåå­—?"}]}, config)

print(result["messages"][-1].content)
```

### trim_messages å‚æ•°è¯¦è§£

```python
trimmed = trim_messages(
    messages,
    
    # åŸºæœ¬å‚æ•°
    max_tokens=1000,        # æœ€å¤§ token æ•°
    strategy="last",         # ç­–ç•¥: "first" æˆ– "last"
    
    # Token è®¡æ•°
    token_counter=model,    # ä½¿ç”¨æ¨¡å‹çš„è®¡æ•°å™¨ï¼Œæˆ–è‡ªå®šä¹‰å‡½æ•°
    
    # æ¶ˆæ¯é€‰æ‹©
    include_system=True,    # å§‹ç»ˆåŒ…å«ç³»ç»Ÿæ¶ˆæ¯
    start_on="human",       # ä»å“ªç§æ¶ˆæ¯ç±»å‹å¼€å§‹
    end_on=["human", "tool"], # åœ¨å“ªç§æ¶ˆæ¯ç±»å‹ç»“æŸ
    
    # å…¶ä»–é€‰é¡¹
    allow_partial=False,    # æ˜¯å¦å…è®¸éƒ¨åˆ†æ¶ˆæ¯
)
```

---

## æ¶ˆæ¯åˆ é™¤ (Delete Messages)

### ä½¿ç”¨ RemoveMessage

`RemoveMessage` å…è®¸ä»çŠ¶æ€ä¸­æ°¸ä¹…åˆ é™¤æ¶ˆæ¯ã€‚

#### åˆ é™¤ç‰¹å®šæ¶ˆæ¯

```python
from langchain.messages import RemoveMessage
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import after_model
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.runtime import Runtime

@after_model
def delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:
    """åˆ é™¤æ—§æ¶ˆæ¯ä»¥ä¿æŒå¯¹è¯å¯ç®¡ç†"""
    messages = state["messages"]
    
    if len(messages) > 2:
        # åˆ é™¤æœ€æ—©çš„ä¸¤æ¡æ¶ˆæ¯
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}
    
    return None

agent = create_agent(
    model="gpt-5.0",
    tools=[],
    middleware=[delete_old_messages],
    checkpointer=InMemorySaver(),
)

config = {"configurable": {"thread_id": "1"}}

agent.invoke({"messages": "ä½ å¥½ï¼Œæˆ‘å« Bob"}, config)
agent.invoke({"messages": "æˆ‘å–œæ¬¢ Python"}, config)
agent.invoke({"messages": "æˆ‘å«ä»€ä¹ˆåå­—?"}, config)
```

#### åˆ é™¤æ‰€æœ‰æ¶ˆæ¯

```python
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langchain.messages import RemoveMessage

def clear_conversation(state):
    """æ¸…ç©ºæ•´ä¸ªå¯¹è¯å†å²"""
    return {"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}
```

#### åœ¨ LangGraph ä¸­åˆ é™¤æ¶ˆæ¯

```python
from langchain.messages import RemoveMessage
from langchain_anthropic import ChatAnthropic
from langgraph.graph import StateGraph, START, MessagesState
from langgraph.checkpoint.memory import MemorySaver

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

def delete_messages(state: MessagesState):
    messages = state["messages"]
    if len(messages) > 2:
        # åˆ é™¤æœ€æ—©çš„ä¸¤æ¡æ¶ˆæ¯
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}
    return {}

def call_model(state: MessagesState):
    response = model.invoke(state["messages"])
    return {"messages": [response]}

checkpointer = MemorySaver()
builder = StateGraph(MessagesState)
builder.add_node("call_model", call_model)
builder.add_node("delete_messages", delete_messages)
builder.add_edge(START, "call_model")
builder.add_edge("call_model", "delete_messages")

graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}
graph.invoke({"messages": [{"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘å« Bob"}]}, config)
graph.invoke({"messages": [{"role": "user", "content": "æˆ‘å«ä»€ä¹ˆåå­—?"}]}, config)
```

### åˆ é™¤æ¶ˆæ¯çš„æ³¨æ„äº‹é¡¹

âš ï¸ **é‡è¦**: åˆ é™¤æ¶ˆæ¯æ—¶ç¡®ä¿ç»“æœæ¶ˆæ¯å†å²æœ‰æ•ˆï¼š

1. **æŸäº›æä¾›å•†æœŸæœ›æ¶ˆæ¯ä»ç”¨æˆ·æ¶ˆæ¯å¼€å§‹**
2. **å¤§å¤šæ•°æä¾›å•†è¦æ±‚å·¥å…·è°ƒç”¨åå¿…é¡»æœ‰å¯¹åº”çš„å·¥å…·ç»“æœæ¶ˆæ¯**

```python
# âœ… æœ‰æ•ˆçš„æ¶ˆæ¯åºåˆ—
[
    HumanMessage("é—®é¢˜"),
    AIMessage("", tool_calls=[...]),
    ToolMessage("ç»“æœ", tool_call_id="..."),
    AIMessage("å›ç­”")
]

# âŒ æ— æ•ˆçš„æ¶ˆæ¯åºåˆ—
[
    AIMessage("", tool_calls=[...]),  # ç¼ºå°‘å¯¹åº”çš„ ToolMessage
    AIMessage("å›ç­”")
]
```

---

## æ¶ˆæ¯æ€»ç»“ (Summarize Messages)

### ä¸ºä»€ä¹ˆä½¿ç”¨æ€»ç»“ï¼Ÿ

ä¿®å‰ªå’Œåˆ é™¤æ¶ˆæ¯ä¼šä¸¢å¤±ä¿¡æ¯ï¼Œè€Œæ€»ç»“å¯ä»¥ï¼š
- âœ… ä¿ç•™å…³é”®ä¿¡æ¯
- âœ… å‹ç¼©å¯¹è¯å†å²
- âœ… æ”¯æŒæ›´é•¿çš„æœ‰æ•ˆå¯¹è¯

### ä½¿ç”¨å†…ç½®çš„ SummarizationMiddleware

#### åœ¨ Agent ä¸­ä½¿ç”¨

```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()

agent = create_agent(
    model="gpt-5.0",
    tools=[],
    middleware=[
        SummarizationMiddleware(
            model="gpt-5.0-mini",              # ç”¨äºæ€»ç»“çš„æ¨¡å‹
            max_tokens_before_summary=4000,   # è§¦å‘æ€»ç»“çš„é˜ˆå€¼
            messages_to_keep=20,              # æ€»ç»“åä¿ç•™çš„æ¶ˆæ¯æ•°
        )
    ],
    checkpointer=checkpointer,
)

config = {"configurable": {"thread_id": "1"}}

agent.invoke({"messages": "ä½ å¥½ï¼Œæˆ‘å« Bob"}, config)
agent.invoke({"messages": "å†™ä¸€é¦–å…³äºçŒ«çš„è¯—"}, config)
agent.invoke({"messages": "ç°åœ¨å†™ä¸€é¦–å…³äºç‹—çš„è¯—"}, config)
result = agent.invoke({"messages": "æˆ‘å«ä»€ä¹ˆåå­—?"}, config)

result["messages"][-1].pretty_print()
# è¾“å‡º: "ä½ å« Bobï¼"
```

### SummarizationMiddleware é…ç½®é€‰é¡¹

```python
SummarizationMiddleware(
    model="gpt-5.0-mini",              # æ€»ç»“æ¨¡å‹
    max_tokens_before_summary=4000,   # Token é˜ˆå€¼
    messages_to_keep=20,              # ä¿ç•™çš„æœ€è¿‘æ¶ˆæ¯æ•°
    token_counter=None,               # è‡ªå®šä¹‰ token è®¡æ•°å™¨
    summary_prompt=None,              # è‡ªå®šä¹‰æ€»ç»“æç¤ºè¯
)
```

### åœ¨ LangGraph ä¸­æ‰‹åŠ¨å®ç°æ€»ç»“

```python
from langchain.chat_models import init_chat_model
from langchain.messages import AnyMessage, SystemMessage, HumanMessage, RemoveMessage
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.memory import InMemorySaver

model = init_chat_model("claude-3-5-sonnet-20241022")

# æ‰©å±•çŠ¶æ€ä»¥åŒ…å«æ€»ç»“
class State(MessagesState):
    summary: str = ""

def call_model(state: State):
    # å¦‚æœæœ‰æ€»ç»“ï¼Œæ·»åŠ ä¸ºç³»ç»Ÿæ¶ˆæ¯
    summary = state.get("summary", "")
    messages = state["messages"]
    
    if summary:
        system_msg = SystemMessage(content=f"ä¹‹å‰å¯¹è¯çš„æ€»ç»“: {summary}")
        messages = [system_msg] + messages
    
    response = model.invoke(messages)
    return {"messages": [response]}

def should_continue(state: State):
    """å†³å®šæ˜¯å¦éœ€è¦æ€»ç»“"""
    if len(state["messages"]) > 6:
        return "summarize"
    return END

def summarize_conversation(state: State):
    """æ€»ç»“å¯¹è¯å†å²"""
    summary = state.get("summary", "")
    messages = state["messages"]
    
    # åˆ›å»ºæ€»ç»“æç¤º
    if summary:
        summary_msg = (
            f"è¿™æ˜¯åˆ°ç›®å‰ä¸ºæ­¢çš„å¯¹è¯æ€»ç»“: {summary}\n\n"
            "è€ƒè™‘ä¸Šé¢çš„æ–°æ¶ˆæ¯ï¼Œæ‰©å±•æ€»ç»“:"
        )
    else:
        summary_msg = "åˆ›å»ºä¸Šè¿°å¯¹è¯çš„æ€»ç»“:"
    
    all_messages = messages + [HumanMessage(content=summary_msg)]
    response = model.invoke(all_messages)
    
    # åˆ é™¤é™¤æœ€åä¸¤æ¡å¤–çš„æ‰€æœ‰æ¶ˆæ¯
    delete_msgs = [RemoveMessage(id=m.id) for m in messages[:-2]]
    
    return {"summary": response.content, "messages": delete_msgs}

# æ„å»ºå›¾
checkpointer = InMemorySaver()
builder = StateGraph(State)
builder.add_node("call_model", call_model)
builder.add_node("summarize", summarize_conversation)
builder.add_edge(START, "call_model")
builder.add_conditional_edges("call_model", should_continue)
builder.add_edge("summarize", END)

graph = builder.compile(checkpointer=checkpointer)

# ä½¿ç”¨
config = {"configurable": {"thread_id": "1"}}
graph.invoke({"messages": "ä½ å¥½ï¼Œæˆ‘å« Alice"}, config)
graph.invoke({"messages": "æˆ‘å–œæ¬¢ç¼–ç¨‹"}, config)
graph.invoke({"messages": "ç‰¹åˆ«æ˜¯ Python"}, config)
graph.invoke({"messages": "æˆ‘è¿˜å–œæ¬¢æœºå™¨å­¦ä¹ "}, config)
result = graph.invoke({"messages": "æˆ‘å«ä»€ä¹ˆåå­—ï¼Œæˆ‘å–œæ¬¢ä»€ä¹ˆ?"}, config)

print(result["messages"][-1].content)
print(f"\næ€»ç»“: {result['summary']}")
```

### ä½¿ç”¨ SummarizationNode (langmem)

å¯¹äºæ›´é«˜çº§çš„æ€»ç»“åŠŸèƒ½ï¼Œå¯ä»¥ä½¿ç”¨ `langmem` åº“ï¼š

```python
from langmem.short_term import SummarizationNode, RunningSummary
from langchain_core.messages.utils import count_tokens_approximately
from typing import TypedDict

model = init_chat_model("claude-3-5-sonnet-20241022")
summarization_model = model.bind(max_tokens=128)

class State(MessagesState):
    context: dict[str, RunningSummary]

class LLMInputState(TypedDict):
    summarized_messages: list[AnyMessage]
    context: dict[str, RunningSummary]

summarization_node = SummarizationNode(
    token_counter=count_tokens_approximately,
    model=summarization_model,
    max_tokens=256,
    max_tokens_before_summary=256,
    max_summary_tokens=128,
)

def call_model(state: LLMInputState):
    response = model.invoke(state["summarized_messages"])
    return {"messages": [response]}

checkpointer = InMemorySaver()
builder = StateGraph(State)
builder.add_node("call_model", call_model)
builder.add_node("summarize", summarization_node)
builder.add_edge(START, "summarize")
builder.add_edge("summarize", "call_model")
graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}
graph.invoke({"messages": "ä½ å¥½ï¼Œæˆ‘å« Bob"}, config)
graph.invoke({"messages": "å†™ä¸€é¦–å…³äºçŒ«çš„è¯—"}, config)
result = graph.invoke({"messages": "æˆ‘å«ä»€ä¹ˆåå­—?"}, config)

print(result["messages"][-1].content)
print(f"\næ€»ç»“: {result['context']['running_summary'].summary}")
```

---

## è‡ªå®šä¹‰ Agent è®°å¿†

### æ‰©å±• AgentState

é»˜è®¤æƒ…å†µä¸‹ï¼ŒAgent ä½¿ç”¨ `AgentState` ç®¡ç†çŸ­æœŸè®°å¿†ã€‚ä½ å¯ä»¥æ‰©å±•å®ƒä»¥æ·»åŠ è‡ªå®šä¹‰å­—æ®µã€‚

#### æ–¹æ³• 1: ä½¿ç”¨ state_schema (ä¼ ç»Ÿæ–¹å¼)

```python
from langchain.agents import create_agent, AgentState
from typing import TypedDict
from langchain.messages import AnyMessage

class CustomState(AgentState):
    user_id: str
    preferences: dict[str, str]

agent = create_agent(
    model="gpt-5.0",
    tools=[],
    state_schema=CustomState,
)

# ä½¿ç”¨è‡ªå®šä¹‰çŠ¶æ€
result = agent.invoke({
    "messages": "ä½ å¥½",
    "user_id": "user_123",
    "preferences": {"theme": "dark"}
})
```

#### æ–¹æ³• 2: ä½¿ç”¨ Middleware (æ¨è)

```python
from langchain.agents import create_agent, create_middleware
from langgraph.checkpoint.memory import MemorySaver
import z from "zod"

# å®šä¹‰è‡ªå®šä¹‰çŠ¶æ€ schema
custom_state_schema = {
    "user_id": str,
    "preferences": dict[str, Any],
}

# åˆ›å»ºçŠ¶æ€æ‰©å±•ä¸­é—´ä»¶
state_extension_middleware = create_middleware(
    name="StateExtension",
    state_schema=custom_state_schema,
)

checkpointer = MemorySaver()
agent = create_agent(
    model="gpt-5.0",
    tools=[],
    middleware=[state_extension_middleware],
    checkpointer=checkpointer,
)

# è°ƒç”¨æ—¶ä¼ é€’è‡ªå®šä¹‰çŠ¶æ€
result = agent.invoke({
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "user_id": "user_123",
    "preferences": {"theme": "dark"},
})
```

### åœ¨å·¥å…·ä¸­è®¿é—®è‡ªå®šä¹‰çŠ¶æ€

```python
from langchain_core.tools import tool
from langchain.tools import ToolRuntime

@tool
def get_user_preference(
    preference_name: str,
    runtime: ToolRuntime
) -> str:
    """è·å–ç”¨æˆ·åå¥½è®¾ç½®"""
    preferences = runtime.state.get("preferences", {})
    return preferences.get(preference_name, "æœªè®¾ç½®")

agent = create_agent(
    model="gpt-5.0",
    tools=[get_user_preference],
    middleware=[state_extension_middleware],
    checkpointer=checkpointer,
)
```

---

## Checkpointer çš„ä½¿ç”¨

### ä»€ä¹ˆæ˜¯ Checkpointerï¼Ÿ

**Checkpointer** è´Ÿè´£æŒä¹…åŒ– Agent çš„çŠ¶æ€ï¼Œä½¿å¯¹è¯èƒ½å¤Ÿè·¨å¤šæ¬¡è°ƒç”¨ä¿æŒè¿ç»­ã€‚

### å†…å­˜ Checkpointer

ç”¨äºå¼€å‘å’Œæµ‹è¯•ï¼Œæ•°æ®å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼š

```python
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
```

**ç‰¹ç‚¹**:
- âœ… å¿«é€Ÿã€ç®€å•
- âœ… æ— éœ€å¤–éƒ¨ä¾èµ–
- âŒ è¿›ç¨‹ç»“æŸæ—¶æ•°æ®ä¸¢å¤±

### SQLite Checkpointer

å°†çŠ¶æ€æŒä¹…åŒ–åˆ° SQLite æ•°æ®åº“ï¼š

```python
from langgraph.checkpoint.sqlite import SqliteSaver

checkpointer = SqliteSaver.from_conn_string("checkpoints.db")
```

**ç‰¹ç‚¹**:
- âœ… æŒä¹…åŒ–å­˜å‚¨
- âœ… è½»é‡çº§
- âŒ ä¸é€‚åˆé«˜å¹¶å‘

### PostgreSQL Checkpointer

ç”¨äºç”Ÿäº§ç¯å¢ƒçš„å¼ºå¤§é€‰æ‹©ï¼š

```python
from langgraph.checkpoint.postgres import PostgresSaver

checkpointer = PostgresSaver.from_conn_string(
    "postgresql://user:password@localhost/dbname"
)
```

**ç‰¹ç‚¹**:
- âœ… ç”Ÿäº§å°±ç»ª
- âœ… æ”¯æŒé«˜å¹¶å‘
- âœ… å¯æ‰©å±•

### ä½¿ç”¨ Checkpointer

```python
from langchain.agents import create_agent
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()

agent = create_agent(
    model="gpt-5.0",
    tools=[],
    checkpointer=checkpointer
)

# ä½¿ç”¨çº¿ç¨‹ ID è¿›è¡Œå¯¹è¯
config_1 = {"configurable": {"thread_id": "conversation_1"}}
config_2 = {"configurable": {"thread_id": "conversation_2"}}

# å¯¹è¯ 1
agent.invoke({"messages": "æˆ‘å« Alice"}, config_1)
agent.invoke({"messages": "æˆ‘å«ä»€ä¹ˆ?"}, config_1)  # "Alice"

# å¯¹è¯ 2 (ç‹¬ç«‹çš„çº¿ç¨‹)
agent.invoke({"messages": "æˆ‘å« Bob"}, config_2)
agent.invoke({"messages": "æˆ‘å«ä»€ä¹ˆ?"}, config_2)  # "Bob"
```

### æŸ¥çœ‹çŠ¶æ€å†å²

```python
# è·å–å½“å‰çŠ¶æ€
state = graph.get_state(config)
print(state.values)
print(state.next)  # ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹

# è·å–çŠ¶æ€å†å²
history = list(graph.get_state_history(config))
for snapshot in history:
    print(f"Step {snapshot.metadata['step']}")
    print(f"Messages: {len(snapshot.values['messages'])}")
```

---

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„è®°å¿†ç®¡ç†ç­–ç•¥

```python
# çŸ­å¯¹è¯ (< 10 è½®) - ä¸éœ€è¦ç‰¹æ®Šå¤„ç†
agent = create_agent(model="gpt-5.0", tools=[], checkpointer=checkpointer)

# ä¸­ç­‰å¯¹è¯ (10-50 è½®) - ä½¿ç”¨æ¶ˆæ¯ä¿®å‰ª
agent = create_agent(
    model="gpt-5.0",
    tools=[],
    middleware=[trim_messages_middleware],
    checkpointer=checkpointer,
)

# é•¿å¯¹è¯ (> 50 è½®) - ä½¿ç”¨æ¶ˆæ¯æ€»ç»“
agent = create_agent(
    model="gpt-5.0",
    tools=[],
    middleware=[
        SummarizationMiddleware(
            model="gpt-5.0-mini",
            max_tokens_before_summary=4000,
            messages_to_keep=20,
        )
    ],
    checkpointer=checkpointer,
)
```

### 2. å§‹ç»ˆä¿ç•™ç³»ç»Ÿæ¶ˆæ¯

```python
@before_model
def trim_messages(state: AgentState, runtime: Runtime):
    messages = state["messages"]
    
    # âœ… ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    system_msg = messages[0]
    recent_msgs = messages[-10:]
    
    return {
        "messages": [
            RemoveMessage(id=REMOVE_ALL_MESSAGES),
            system_msg,
            *recent_msgs
        ]
    }
```

### 3. ä½¿ç”¨é€‚å½“çš„ Token è®¡æ•°å™¨

```python
from langchain_core.messages import trim_messages
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")

# âœ… ä½¿ç”¨æ¨¡å‹çš„ token è®¡æ•°å™¨
trimmed = trim_messages(
    messages,
    max_tokens=1000,
    token_counter=model,  # å‡†ç¡®çš„ token è®¡æ•°
)

# âš ï¸ ä½¿ç”¨ç®€å•è®¡æ•°å™¨ï¼ˆå¿«é€Ÿä½†ä¸å‡†ç¡®ï¼‰
trimmed = trim_messages(
    messages,
    max_tokens=1000,
    token_counter=len,  # å­—ç¬¦æ•°ï¼Œä¸æ˜¯ token æ•°
)
```

### 4. ç›‘æ§å’Œè®°å½•è®°å¿†ä½¿ç”¨

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@before_model
def trim_with_logging(state: AgentState, runtime: Runtime):
    messages = state["messages"]
    original_count = len(messages)
    
    # æ‰§è¡Œä¿®å‰ª
    if original_count > 10:
        trimmed_messages = messages[-10:]
        logger.info(f"ä¿®å‰ªæ¶ˆæ¯: {original_count} -> {len(trimmed_messages)}")
        
        return {
            "messages": [
                RemoveMessage(id=REMOVE_ALL_MESSAGES),
                *trimmed_messages
            ]
        }
    
    return None
```

### 5. å¤„ç†å¤šæ¨¡æ€å†…å®¹

```python
from langchain_core.messages import trim_messages

# å¤šæ¨¡æ€æ¶ˆæ¯å¯èƒ½æ¶ˆè€—å¤§é‡ tokens
trimmed = trim_messages(
    messages,
    max_tokens=2000,  # ä¸ºå›¾åƒç­‰ç•™å‡ºæ›´å¤šç©ºé—´
    token_counter=model,
    include_system=True,
)
```

### 6. å®šæœŸæ¸…ç†è¿‡æœŸçº¿ç¨‹

```python
from datetime import datetime, timedelta

def cleanup_old_threads(checkpointer, max_age_days=30):
    """æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„çº¿ç¨‹"""
    cutoff_date = datetime.now() - timedelta(days=max_age_days)
    
    # å®ç°å–å†³äº checkpointer ç±»å‹
    # è¿™æ˜¯ä¸€ä¸ªæ¦‚å¿µç¤ºä¾‹
    for thread_id in checkpointer.list_threads():
        last_update = checkpointer.get_last_update(thread_id)
        if last_update < cutoff_date:
            checkpointer.delete_thread(thread_id)

# å®šæœŸè¿è¡Œ
cleanup_old_threads(checkpointer)
```

### 7. æµ‹è¯•ä¸åŒçš„ç­–ç•¥

```python
import pytest
from langchain.agents import create_agent

@pytest.fixture
def agent_with_trim():
    return create_agent(
        model="gpt-5.0",
        tools=[],
        middleware=[trim_middleware],
        checkpointer=MemorySaver(),
    )

@pytest.fixture
def agent_with_summary():
    return create_agent(
        model="gpt-5.0",
        tools=[],
        middleware=[SummarizationMiddleware(...)],
        checkpointer=MemorySaver(),
    )

def test_long_conversation_trim(agent_with_trim):
    config = {"configurable": {"thread_id": "test"}}
    
    # æ¨¡æ‹Ÿé•¿å¯¹è¯
    for i in range(20):
        agent_with_trim.invoke({"messages": f"æ¶ˆæ¯ {i}"}, config)
    
    # éªŒè¯è®°å¿†ç®¡ç†
    state = agent_with_trim.get_state(config)
    assert len(state.values["messages"]) <= 10

def test_long_conversation_summary(agent_with_summary):
    # ç±»ä¼¼çš„æµ‹è¯•...
    pass
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. ä½¿ç”¨è½»é‡çº§æ¨¡å‹è¿›è¡Œæ€»ç»“

```python
# âœ… å¥½çš„åšæ³•
SummarizationMiddleware(
    model="gpt-5.0-mini",  # å¿«é€Ÿã€ä¾¿å®œçš„æ¨¡å‹
    max_tokens_before_summary=4000,
)

# âŒ é¿å…
SummarizationMiddleware(
    model="gpt-5.0",  # æ…¢ã€è´µ
    max_tokens_before_summary=4000,
)
```

### 2. æ‰¹é‡å¤„ç†æ¶ˆæ¯åˆ é™¤

```python
# âœ… ä¸€æ¬¡æ€§åˆ é™¤å¤šæ¡æ¶ˆæ¯
@after_model
def batch_delete(state: AgentState, runtime: Runtime):
    messages = state["messages"]
    if len(messages) > 20:
        to_delete = messages[:10]  # åˆ é™¤æœ€æ—©çš„ 10 æ¡
        return {"messages": [RemoveMessage(id=m.id) for m in to_delete]}
    return None

# âŒ é¿å…é¢‘ç¹çš„å°æ‰¹é‡åˆ é™¤
@after_model
def frequent_delete(state: AgentState, runtime: Runtime):
    messages = state["messages"]
    if len(messages) > 5:
        return {"messages": [RemoveMessage(id=messages[0].id)]}  # æ¯æ¬¡åªåˆ ä¸€æ¡
    return None
```

### 3. ç¼“å­˜ Token è®¡æ•°

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def count_tokens(text: str) -> int:
    """ç¼“å­˜çš„ token è®¡æ•°"""
    return len(text) // 4  # ç®€åŒ–ç¤ºä¾‹

# ä½¿ç”¨ç¼“å­˜çš„è®¡æ•°å™¨
trimmed = trim_messages(
    messages,
    max_tokens=1000,
    token_counter=lambda msgs: sum(count_tokens(m.content) for m in msgs),
)
```

### 4. å¼‚æ­¥å¤„ç†æ€»ç»“

```python
import asyncio

async def async_summarize(messages, model):
    """å¼‚æ­¥æ€»ç»“æ¶ˆæ¯"""
    summary_prompt = "æ€»ç»“ä»¥ä¸‹å¯¹è¯:"
    all_msgs = messages + [HumanMessage(content=summary_prompt)]
    response = await model.ainvoke(all_msgs)
    return response.content

# åœ¨åå°æ€»ç»“ï¼Œä¸é˜»å¡ä¸»æµç¨‹
```

### 5. ç›‘æ§ Token ä½¿ç”¨

```python
from langchain_core.messages import count_tokens_approximately

def monitor_token_usage(state: AgentState):
    """ç›‘æ§å½“å‰å¯¹è¯çš„ token ä½¿ç”¨"""
    messages = state["messages"]
    total_tokens = count_tokens_approximately(messages)
    
    logger.info(f"å½“å‰ tokens: {total_tokens}")
    
    if total_tokens > 8000:
        logger.warning("Token ä½¿ç”¨æ¥è¿‘ä¸Šé™ï¼Œè€ƒè™‘æ€»ç»“")
    
    return {"token_usage": total_tokens}
```

---

## ğŸ¯ å¿«é€Ÿå‚è€ƒ

### è®°å¿†ç®¡ç†ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| **ä¿®å‰ª** | ç®€å•ã€å¿«é€Ÿã€å¯é¢„æµ‹ | ä¸¢å¤±æ—©æœŸä¿¡æ¯ | çŸ­æœŸå¯¹è¯ (< 20 è½®) |
| **åˆ é™¤** | ç²¾ç¡®æ§åˆ¶ | ä¸å¯æ¢å¤ | åˆ é™¤æ•æ„Ÿä¿¡æ¯ |
| **æ€»ç»“** | ä¿ç•™å…³é”®ä¿¡æ¯ | é¢å¤–æˆæœ¬ | é•¿æœŸå¯¹è¯ (> 50 è½®) |
| **æ··åˆ** | å¹³è¡¡å„æ–¹é¢ | å¤æ‚åº¦é«˜ | å¤æ‚åº”ç”¨ |

### å¸¸ç”¨ä»£ç ç‰‡æ®µ

```python
# 1. å¯ç”¨åŸºæœ¬è®°å¿†
from langgraph.checkpoint.memory import MemorySaver
checkpointer = MemorySaver()
agent = create_agent(model="gpt-5.0", tools=[], checkpointer=checkpointer)

# 2. ä¿®å‰ªæ¶ˆæ¯
from langchain.agents.middleware import before_model
@before_model
def trim(state, runtime):
    messages = state["messages"]
    return {"messages": [messages[0]] + messages[-10:]} if len(messages) > 10 else None

# 3. æ€»ç»“æ¶ˆæ¯
from langchain.agents.middleware import SummarizationMiddleware
middleware = [SummarizationMiddleware(model="gpt-5.0-mini", max_tokens_before_summary=4000)]

# 4. åˆ é™¤æ¶ˆæ¯
from langchain.messages import RemoveMessage
return {"messages": [RemoveMessage(id=m.id) for m in old_messages]}

# 5. æŸ¥çœ‹çŠ¶æ€
state = graph.get_state(config)
history = list(graph.get_state_history(config))
```

---

## ğŸ”— ç›¸å…³èµ„æº

- **å®˜æ–¹æ–‡æ¡£**: https://docs.langchain.com/oss/python/langchain/short-term-memory
- **é…å¥—æ–‡æ¡£**:
  - [LangChain Messages è¯¦ç»†æŒ‡å—](./LangChain_Messages_è¯¦ç»†æŒ‡å—.md)
  - [LangChain Models è¯¦ç»†æŒ‡å—](./LangChain_Models_è¯¦ç»†æŒ‡å—.md)
  - [LangChain Agents è¯¦ç»†æ€»ç»“](./LangChain_Agents_è¯¦ç»†æ€»ç»“.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ  
**åŸºäº**: LangChain v0.3+, Python 3.9+

æœ¬æ–‡æ¡£æ¶µç›–äº† LangChain Short-term Memory çš„æ ¸å¿ƒæ¦‚å¿µã€å®ç°æ–¹å¼ã€ç®¡ç†ç­–ç•¥å’Œæœ€ä½³å®è·µï¼ŒåŒ…å« 60+ å®ç”¨ä»£ç ç¤ºä¾‹ã€‚
