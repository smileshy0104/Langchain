# LangChain Streaming è¯¦ç»†æŒ‡å—

## ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [ä¸ºä»€ä¹ˆéœ€è¦ Streaming](#ä¸ºä»€ä¹ˆéœ€è¦-streaming)
3. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
4. [LangChain ä¸­çš„ Streaming](#langchain-ä¸­çš„-streaming)
   - [Model Streaming](#model-streaming)
   - [Agent Streaming](#agent-streaming)
   - [è‡ªåŠ¨æµå¼ä¼ è¾“](#è‡ªåŠ¨æµå¼ä¼ è¾“)
5. [LangGraph ä¸­çš„ Streaming](#langgraph-ä¸­çš„-streaming)
   - [Stream æ¨¡å¼è¯¦è§£](#stream-æ¨¡å¼è¯¦è§£)
   - [æµå¼ä¼ è¾“å›¾çŠ¶æ€](#æµå¼ä¼ è¾“å›¾çŠ¶æ€)
   - [æµå¼ä¼ è¾“ LLM Tokens](#æµå¼ä¼ è¾“-llm-tokens)
   - [æµå¼ä¼ è¾“è‡ªå®šä¹‰æ•°æ®](#æµå¼ä¼ è¾“è‡ªå®šä¹‰æ•°æ®)
   - [æµå¼ä¼ è¾“å­å›¾è¾“å‡º](#æµå¼ä¼ è¾“å­å›¾è¾“å‡º)
   - [è°ƒè¯•æ¨¡å¼](#è°ƒè¯•æ¨¡å¼)
6. [é«˜çº§ç‰¹æ€§](#é«˜çº§ç‰¹æ€§)
   - [å¤šæ¨¡å¼ç»„åˆ](#å¤šæ¨¡å¼ç»„åˆ)
   - [ç¦ç”¨ Streaming](#ç¦ç”¨-streaming)
   - [è¿‡æ»¤ç‰¹å®šèŠ‚ç‚¹](#è¿‡æ»¤ç‰¹å®šèŠ‚ç‚¹)
7. [æ¶ˆæ¯å—å¤„ç†](#æ¶ˆæ¯å—å¤„ç†)
8. [å®é™…åº”ç”¨åœºæ™¯](#å®é™…åº”ç”¨åœºæ™¯)
9. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
10. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
11. [å¿«é€Ÿå‚è€ƒ](#å¿«é€Ÿå‚è€ƒ)

---

## æ¦‚è¿°

LangChain å®ç°äº†ä¸€ä¸ªå¼ºå¤§çš„æµå¼ä¼ è¾“ç³»ç»Ÿï¼Œç”¨äºå®æ—¶æ˜¾ç¤ºæ›´æ–°ã€‚æµå¼ä¼ è¾“å¯¹äºå¢å¼ºåŸºäº LLM çš„åº”ç”¨ç¨‹åºçš„å“åº”æ€§è‡³å…³é‡è¦ã€‚é€šè¿‡é€æ­¥æ˜¾ç¤ºè¾“å‡ºï¼ˆå³ä½¿åœ¨å®Œæ•´å“åº”å‡†å¤‡å¥½ä¹‹å‰ï¼‰ï¼Œæµå¼ä¼ è¾“æ˜¾è‘—æ”¹å–„äº†ç”¨æˆ·ä½“éªŒï¼ˆUXï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç† LLM çš„å»¶è¿Ÿæ—¶ã€‚

**å…³é”®ä¼˜åŠ¿ï¼š**
- ğŸš€ å®æ—¶åé¦ˆ - ç”¨æˆ·ç«‹å³çœ‹åˆ°è¿›å±•
- âš¡ æ”¹å–„æ„ŸçŸ¥æ€§èƒ½ - å³ä½¿æ€»æ—¶é—´ç›¸åŒï¼Œæµå¼ä¼ è¾“è®©åº”ç”¨æ„Ÿè§‰æ›´å¿«
- ğŸ“Š è¿›åº¦å¯è§†åŒ– - å¯ä»¥æ˜¾ç¤ºä¸­é—´æ­¥éª¤å’ŒçŠ¶æ€
- ğŸ¯ æ›´å¥½çš„ UX - ç‰¹åˆ«æ˜¯å¯¹äºé•¿å“åº”

---

## ä¸ºä»€ä¹ˆéœ€è¦ Streaming

### 1. **LLM å»¶è¿Ÿé—®é¢˜**
å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå“åº”éœ€è¦æ—¶é—´ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿è¾“å‡ºï¼š
- GPT-4 ç”Ÿæˆ 500 å­—å¯èƒ½éœ€è¦ 10-20 ç§’
- ç”¨æˆ·æœŸæœ›å³æ—¶åé¦ˆ
- æµå¼ä¼ è¾“è®©ç­‰å¾…è¿‡ç¨‹æ›´åŠ å¯æ§

### 2. **ç”¨æˆ·ä½“éªŒ**
```python
# éæµå¼ï¼šç”¨æˆ·ç­‰å¾… 15 ç§’ï¼Œç„¶åçœ‹åˆ°å®Œæ•´å“åº”
response = model.invoke("å†™ä¸€ç¯‡å…³äº AI çš„æ–‡ç« ")
print(response.content)  # 15 ç§’åæ˜¾ç¤º

# æµå¼ï¼šç”¨æˆ·ç«‹å³çœ‹åˆ°æ–‡å­—é€æ¸å‡ºç°
for chunk in model.stream("å†™ä¸€ç¯‡å…³äº AI çš„æ–‡ç« "):
    print(chunk.text, end="", flush=True)  # å®æ—¶æ˜¾ç¤º
```

### 3. **Agent å¯è§‚å¯Ÿæ€§**
åœ¨å¤æ‚çš„ Agent ç³»ç»Ÿä¸­ï¼Œæµå¼ä¼ è¾“è®©ä½ çœ‹åˆ°ï¼š
- Agent æ­£åœ¨è°ƒç”¨å“ªä¸ªå·¥å…·
- å·¥å…·æ‰§è¡Œçš„è¿›åº¦
- ä¸­é—´æ€è€ƒè¿‡ç¨‹

---

## æ ¸å¿ƒæ¦‚å¿µ

### Stream Modeï¼ˆæµå¼æ¨¡å¼ï¼‰

LangChain æ”¯æŒå¤šç§æµå¼æ¨¡å¼ï¼Œæ¯ç§æ¨¡å¼æä¾›ä¸åŒçº§åˆ«çš„ä¿¡æ¯ï¼š

| æ¨¡å¼ | æè¿° | ç”¨é€” |
|------|------|------|
| `values` | æ¯æ­¥åçš„å®Œæ•´çŠ¶æ€ | æŸ¥çœ‹å®Œæ•´çš„å›¾çŠ¶æ€ |
| `updates` | æ¯æ­¥çš„çŠ¶æ€æ›´æ–°ï¼ˆå¢é‡ï¼‰ | åªçœ‹å˜åŒ–éƒ¨åˆ† |
| `messages` | LLM token æµ + å…ƒæ•°æ® | æµå¼æ˜¾ç¤º LLM è¾“å‡º |
| `custom` | è‡ªå®šä¹‰ç”¨æˆ·æ•°æ® | è¿›åº¦æ›´æ–°ã€æ—¥å¿—ç­‰ |
| `debug` | è¯¦ç»†çš„æ‰§è¡Œä¿¡æ¯ | è°ƒè¯•å’Œæ•…éšœæ’é™¤ |

### æµå¼è¾“å‡ºç±»å‹

1. **Token-level streaming** - é€ä¸ª token è¾“å‡º
2. **Step-level streaming** - æ¯ä¸ªèŠ‚ç‚¹/æ­¥éª¤çš„è¾“å‡º
3. **Event streaming** - è¯­ä¹‰äº‹ä»¶ï¼ˆå¼€å§‹ã€æµå¼ã€ç»“æŸï¼‰

---

## LangChain ä¸­çš„ Streaming

### Model Streaming

#### åŸºæœ¬ç”¨æ³•

```python
from langchain.chat_models import init_chat_model

model = init_chat_model(model="glm-4.5-air")

# æµå¼è¾“å‡º tokens
for chunk in model.stream("ä»€ä¹ˆé¢œè‰²æ˜¯å¤©ç©ºï¼Ÿ"):
    print(chunk.text, end="", flush=True)
```

**è¾“å‡ºï¼š**
```
å¤©
å¤©ç©º
å¤©ç©ºé€šå¸¸
å¤©ç©ºé€šå¸¸æ˜¯
å¤©ç©ºé€šå¸¸æ˜¯è“è‰²
...
```

#### ç´¯ç§¯æ¶ˆæ¯å—

```python
from langchain_core.messages import AIMessageChunk

# ç´¯ç§¯å®Œæ•´æ¶ˆæ¯
full = None
for chunk in model.stream("ä½ å¥½"):
    full = chunk if full is None else full + chunk
    print(full.text)

# æœ€ç»ˆçš„ full æ˜¯ä¸€ä¸ªå®Œæ•´çš„ AIMessage
print(full.content_blocks)
# [{"type": "text", "text": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"}]
```

#### æµå¼å·¥å…·è°ƒç”¨

```python
from langchain.tools import tool

@tool
def get_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°”"""
    return f"{city} ä»Šå¤©æ™´å¤©"

model_with_tools = model.bind_tools([get_weather])

# å·¥å…·è°ƒç”¨åœ¨æµå¼ä¼ è¾“ä¸­é€æ­¥æ„å»º
for chunk in model_with_tools.stream("æ³¢å£«é¡¿å¤©æ°”å¦‚ä½•ï¼Ÿ"):
    if chunk.tool_call_chunks:
        for tool_chunk in chunk.tool_call_chunks:
            print(f"Tool: {tool_chunk.get('name', '')}")
            print(f"Args: {tool_chunk.get('args', '')}")
```

**è¾“å‡ºï¼š**
```
Tool: get_weather
Args:
Tool:
Args: {"city
Tool:
Args: ": "Boston"}
```

### Agent Streaming

#### æµå¼ Agent è¿›åº¦

```python
from langchain.agents import create_agent

agent = create_agent(
    model="glm-4.5-air",
    tools=[get_weather],
)

# stream_mode="updates" æ˜¾ç¤ºæ¯ä¸ªæ­¥éª¤åçš„æ›´æ–°
for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "SF çš„å¤©æ°”"}]},
    stream_mode="updates"
):
    print(chunk)
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```python
# LLM èŠ‚ç‚¹è¾“å‡ºï¼ˆå¸¦å·¥å…·è°ƒç”¨ï¼‰
{'model': {'messages': [AIMessage(tool_calls=[...])]}}

# å·¥å…·èŠ‚ç‚¹è¾“å‡º
{'tools': {'messages': [ToolMessage(content="æ™´å¤©")]}}

# LLM æœ€ç»ˆå“åº”
{'model': {'messages': [AIMessage(content="æ—§é‡‘å±±ä»Šå¤©æ™´å¤©")]}}
```

### è‡ªåŠ¨æµå¼ä¼ è¾“ï¼ˆ`invoke()`ï¼‰

LangChain çš„ä¸€ä¸ªå¼ºå¤§ç‰¹æ€§æ˜¯**è‡ªåŠ¨æµå¼ä¼ è¾“**ï¼šå³ä½¿åœ¨èŠ‚ç‚¹å†…ä½¿ç”¨ `invoke()`ï¼Œå¦‚æœæ•´ä¸ªåº”ç”¨ç¨‹åºå¤„äºæµå¼æ¨¡å¼ï¼ŒLangChain ä¹Ÿä¼šè‡ªåŠ¨åˆ‡æ¢åˆ°æµå¼ä¼ è¾“ã€‚

```python
from langgraph.graph import StateGraph, START

def my_node(state):
    # ä½¿ç”¨ invoke()ï¼Œä½†åœ¨æµå¼ä¸Šä¸‹æ–‡ä¸­ä¼šè‡ªåŠ¨æµå¼ä¼ è¾“
    response = model.invoke(state["messages"])
    return {"messages": [response]}

graph = StateGraph(MessagesState).add_node("model", my_node).compile()

# graph.stream() ä¼šè‡ªåŠ¨è§¦å‘ model çš„æµå¼ä¼ è¾“
for chunk in graph.stream(
    {"messages": [{"role": "user", "content": "ä½ å¥½"}]},
    stream_mode="messages"  # æµå¼ä¼ è¾“ LLM tokens
):
    print(chunk)
```

**å·¥ä½œåŸç†ï¼š**
- å½“æ£€æµ‹åˆ°æ•´ä½“åº”ç”¨ç¨‹åºåœ¨æµå¼æ¨¡å¼æ—¶ï¼Œ`invoke()` è‡ªåŠ¨åˆ‡æ¢åˆ°å†…éƒ¨æµå¼æ¨¡å¼
- LangChain è§¦å‘ `on_llm_new_token` å›è°ƒäº‹ä»¶
- LangGraph çš„ `stream()` å’Œ `astream_events()` å®æ—¶æ˜¾ç¤ºè¾“å‡º

---

## LangGraph ä¸­çš„ Streaming

LangGraph æä¾›æ›´å¼ºå¤§çš„æµå¼ä¼ è¾“åŠŸèƒ½ï¼Œé€‚ç”¨äºå¤æ‚çš„ Agent å·¥ä½œæµã€‚

### Stream æ¨¡å¼è¯¦è§£

#### 1. Values Mode - å®Œæ•´çŠ¶æ€

æ¯ä¸ªè¶…æ­¥éª¤åæµå¼ä¼ è¾“å®Œæ•´çš„å›¾çŠ¶æ€ã€‚

```python
from langgraph.graph import StateGraph, START
from typing import TypedDict

class State(TypedDict):
    topic: str
    joke: str

def generate_joke(state: State):
    return {"joke": f"å…³äº {state['topic']} çš„ç¬‘è¯"}

graph = StateGraph(State).add_node("generate", generate_joke).compile()

for chunk in graph.stream(
    {"topic": "å†°æ·‡æ·‹"},
    stream_mode="values"  # å®Œæ•´çŠ¶æ€
):
    print(chunk)
```

**è¾“å‡ºï¼š**
```python
{'topic': 'å†°æ·‡æ·‹', 'joke': ''}  # åˆå§‹çŠ¶æ€
{'topic': 'å†°æ·‡æ·‹', 'joke': 'å…³äº å†°æ·‡æ·‹ çš„ç¬‘è¯'}  # æ‰§è¡Œå
```

#### 2. Updates Mode - å¢é‡æ›´æ–°

åªæµå¼ä¼ è¾“çŠ¶æ€çš„æ›´æ–°éƒ¨åˆ†ã€‚

```python
for chunk in graph.stream(
    {"topic": "å†°æ·‡æ·‹"},
    stream_mode="updates"  # åªæœ‰æ›´æ–°
):
    print(chunk)
```

**è¾“å‡ºï¼š**
```python
{'generate': {'joke': 'å…³äº å†°æ·‡æ·‹ çš„ç¬‘è¯'}}  # åªæ˜¾ç¤ºæ›´æ–°
```

### æµå¼ä¼ è¾“å›¾çŠ¶æ€

```python
from typing import Annotated
from langgraph.graph import StateGraph, START, END

class GraphState(TypedDict):
    count: Annotated[int, lambda x, y: x + y]  # reducer
    data: str

def node_a(state: GraphState):
    return {"count": 1, "data": "A"}

def node_b(state: GraphState):
    return {"count": 1, "data": "B"}

graph = (
    StateGraph(GraphState)
    .add_node("a", node_a)
    .add_node("b", node_b)
    .add_edge(START, "a")
    .add_edge("a", "b")
    .compile()
)

# æµå¼ä¼ è¾“çŠ¶æ€æ›´æ–°
for chunk in graph.stream(
    {"count": 0, "data": ""},
    stream_mode="updates"
):
    print(chunk)
```

**è¾“å‡ºï¼š**
```python
{'a': {'count': 1, 'data': 'A'}}
{'b': {'count': 1, 'data': 'B'}}
```

### æµå¼ä¼ è¾“ LLM Tokens

ä½¿ç”¨ `stream_mode="messages"` å®æ—¶æµå¼ä¼ è¾“ LLM ç”Ÿæˆçš„ tokensã€‚

```python
from dataclasses import dataclass
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START

@dataclass
class MyState:
    topic: str
    joke: str = ""

model = init_chat_model(model="glm-4.5-air-mini")

def call_model(state: MyState):
    """è°ƒç”¨ LLM ç”Ÿæˆç¬‘è¯"""
    # å³ä½¿ä½¿ç”¨ invokeï¼Œåœ¨ messages æ¨¡å¼ä¸‹ä¹Ÿä¼šæµå¼ä¼ è¾“
    response = model.invoke([
        {"role": "user", "content": f"ç”Ÿæˆä¸€ä¸ªå…³äº {state.topic} çš„ç¬‘è¯"}
    ])
    return {"joke": response.content}

graph = (
    StateGraph(MyState)
    .add_node("call_model", call_model)
    .add_edge(START, "call_model")
    .compile()
)

# æµå¼ä¼ è¾“ LLM tokens
for message_chunk, metadata in graph.stream(
    {"topic": "å†°æ·‡æ·‹"},
    stream_mode="messages"  # Token æµå¼ä¼ è¾“
):
    if message_chunk.content:
        print(message_chunk.content, end="|", flush=True)
```

**è¾“å‡ºï¼š**
```
ä¸º|ä»€ä¹ˆ|å†°æ·‡æ·‹|ä»ä¸|æ„Ÿåˆ°|å­¤ç‹¬|ï¼Ÿ|å› ä¸º|å®ƒ|æ€»æ˜¯|å’Œ|å®ƒçš„|æœ‹å‹|åœ¨|ä¸€èµ·|ï¼|
```

**å…ƒæ•°æ®åŒ…å«ï¼š**
```python
{
    'langgraph_node': 'call_model',  # èŠ‚ç‚¹åç§°
    'langgraph_triggers': [...],     # è§¦å‘å™¨
    'langgraph_path': [...],         # æ‰§è¡Œè·¯å¾„
}
```

#### è¿‡æ»¤ç‰¹å®šèŠ‚ç‚¹çš„ Tokens

```python
from langgraph.graph import StateGraph, START

def write_joke(state):
    response = model.invoke([
        {"role": "user", "content": f"å†™ä¸€ä¸ªå…³äº {state['topic']} çš„ç¬‘è¯"}
    ])
    return {"joke": response.content}

def write_poem(state):
    response = model.invoke([
        {"role": "user", "content": f"å†™ä¸€é¦–å…³äº {state['topic']} çš„è¯—"}
    ])
    return {"poem": response.content}

graph = (
    StateGraph(State)
    .add_node("write_joke", write_joke)
    .add_node("write_poem", write_poem)
    .add_edge(START, "write_joke")
    .add_edge(START, "write_poem")
    .compile()
)

# åªæµå¼ä¼ è¾“ write_poem èŠ‚ç‚¹çš„è¾“å‡º
for msg, metadata in graph.stream(
    {"topic": "çŒ«"},
    stream_mode="messages"
):
    # æŒ‰èŠ‚ç‚¹è¿‡æ»¤
    if msg.content and metadata.get("langgraph_node") == "write_poem":
        print(msg.content, end="|", flush=True)
```

### æµå¼ä¼ è¾“è‡ªå®šä¹‰æ•°æ®

ä»å·¥å…·æˆ–èŠ‚ç‚¹å†…éƒ¨å‘é€è‡ªå®šä¹‰è¿›åº¦æ›´æ–°ã€‚

#### ä½¿ç”¨ get_stream_writer

```python
from langgraph.config import get_stream_writer
from langgraph.graph import StateGraph, START

class State(TypedDict):
    query: str
    answer: str

def node(state: State):
    # è·å–æµå†™å…¥å™¨
    writer = get_stream_writer()
    
    # å‘é€è‡ªå®šä¹‰è¿›åº¦æ›´æ–°
    writer({"progress": "å¼€å§‹å¤„ç†æŸ¥è¯¢..."})
    
    # æ¨¡æ‹Ÿå¤„ç†
    import time
    time.sleep(1)
    
    writer({"progress": "æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ..."})
    time.sleep(1)
    
    writer({"progress": "å®Œæˆï¼"})
    
    return {"answer": "ç­”æ¡ˆå†…å®¹"}

graph = (
    StateGraph(State)
    .add_node("process", node)
    .add_edge(START, "process")
    .compile()
)

# stream_mode="custom" æ¥æ”¶è‡ªå®šä¹‰æ•°æ®
for chunk in graph.stream(
    {"query": "ç¤ºä¾‹æŸ¥è¯¢"},
    stream_mode="custom"
):
    print(chunk)
```

**è¾“å‡ºï¼š**
```python
{'progress': 'å¼€å§‹å¤„ç†æŸ¥è¯¢...'}
{'progress': 'æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...'}
{'progress': 'å®Œæˆï¼'}
```

#### åœ¨å·¥å…·ä¸­ä½¿ç”¨æµå¼ä¼ è¾“

```python
from langchain.tools import tool
from langgraph.config import get_stream_writer

@tool
def query_database(query: str) -> str:
    """æŸ¥è¯¢æ•°æ®åº“"""
    writer = get_stream_writer()
    
    # å‘é€è¿›åº¦æ›´æ–°
    writer({"data": "å·²æ£€ç´¢ 0/100 æ¡è®°å½•", "type": "progress"})
    
    # æ¨¡æ‹ŸæŸ¥è¯¢
    import time
    for i in range(0, 101, 20):
        time.sleep(0.5)
        writer({"data": f"å·²æ£€ç´¢ {i}/100 æ¡è®°å½•", "type": "progress"})
    
    return "æŸ¥è¯¢ç»“æœ"

# åœ¨ agent ä¸­ä½¿ç”¨
agent = create_agent(model="glm-4.5-air", tools=[query_database])

for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "æŸ¥è¯¢æ•°æ®åº“"}]},
    stream_mode="custom"
):
    print(chunk)
```

**è¾“å‡ºï¼š**
```python
{'data': 'å·²æ£€ç´¢ 0/100 æ¡è®°å½•', 'type': 'progress'}
{'data': 'å·²æ£€ç´¢ 20/100 æ¡è®°å½•', 'type': 'progress'}
{'data': 'å·²æ£€ç´¢ 40/100 æ¡è®°å½•', 'type': 'progress'}
...
```

#### Python < 3.11 å¼‚æ­¥æ³¨æ„äº‹é¡¹

åœ¨ Python < 3.11 çš„å¼‚æ­¥ä»£ç ä¸­ï¼Œ`get_stream_writer` ä¸å¯ç”¨ï¼Œéœ€è¦ç›´æ¥ä½¿ç”¨ `StreamWriter`ï¼š

```python
from langgraph.types import StreamWriter

async def async_node(state: State, writer: StreamWriter):
    writer({"status": "å¼€å§‹"})
    # ... å¼‚æ­¥å¤„ç†
    writer({"status": "å®Œæˆ"})
    return {"result": "data"}
```

### æµå¼ä¼ è¾“å­å›¾è¾“å‡º

å½“ä½¿ç”¨åµŒå¥—å­å›¾æ—¶ï¼Œå¯ä»¥æµå¼ä¼ è¾“æ¥è‡ªçˆ¶å›¾å’Œå­å›¾çš„è¾“å‡ºã€‚

```python
from langgraph.graph import START, StateGraph
from typing import TypedDict

# å®šä¹‰å­å›¾
class SubgraphState(TypedDict):
    foo: str  # ä¸çˆ¶å›¾å…±äº«çš„é”®
    bar: str

def subgraph_node_1(state: SubgraphState):
    return {"bar": "bar"}

def subgraph_node_2(state: SubgraphState):
    return {"foo": state["foo"] + state["bar"]}

subgraph = (
    StateGraph(SubgraphState)
    .add_node("subgraph_node_1", subgraph_node_1)
    .add_node("subgraph_node_2", subgraph_node_2)
    .add_edge(START, "subgraph_node_1")
    .add_edge("subgraph_node_1", "subgraph_node_2")
    .compile()
)

# å®šä¹‰çˆ¶å›¾
class ParentState(TypedDict):
    foo: str

def node_1(state: ParentState):
    return {"foo": "hi! " + state["foo"]}

graph = (
    StateGraph(ParentState)
    .add_node("node_1", node_1)
    .add_node("node_2", subgraph)  # å­å›¾ä½œä¸ºèŠ‚ç‚¹
    .add_edge(START, "node_1")
    .add_edge("node_1", "node_2")
    .compile()
)

# æµå¼ä¼ è¾“æ—¶åŒ…å«å­å›¾è¾“å‡º
for chunk in graph.stream(
    {"foo": "foo"},
    stream_mode="updates",
    subgraphs=True  # å¯ç”¨å­å›¾æµå¼ä¼ è¾“
):
    print(chunk)
```

**è¾“å‡ºï¼š**
```python
((), {'node_1': {'foo': 'hi! foo'}})  # çˆ¶å›¾
(('node_2:UUID',), {'subgraph_node_1': {'bar': 'bar'}})  # å­å›¾èŠ‚ç‚¹ 1
(('node_2:UUID',), {'subgraph_node_2': {'foo': 'hi! foobar'}})  # å­å›¾èŠ‚ç‚¹ 2
((), {'node_2': {'foo': 'hi! foobar'}})  # çˆ¶å›¾
```

**å‘½åç©ºé—´è¯´æ˜ï¼š**
- `()` - çˆ¶å›¾
- `('node_2:UUID',)` - å­å›¾è·¯å¾„
- å¯ä»¥é€šè¿‡å‘½åç©ºé—´è¯†åˆ«æ¥è‡ªå“ªä¸ªå›¾/å­å›¾

### è°ƒè¯•æ¨¡å¼

`debug` æ¨¡å¼æµå¼ä¼ è¾“å°½å¯èƒ½å¤šçš„æ‰§è¡Œä¿¡æ¯ï¼ŒåŒ…æ‹¬å®Œæ•´çŠ¶æ€å’Œæ‰€æœ‰ä¸­é—´æ­¥éª¤ã€‚

```python
for chunk in graph.stream(
    {"topic": "å†°æ·‡æ·‹"},
    stream_mode="debug"
):
    print(chunk)
```

**è¾“å‡ºåŒ…å«ï¼š**
- èŠ‚ç‚¹åç§°
- å®Œæ•´çŠ¶æ€
- æ‰§è¡Œå…ƒæ•°æ®
- æ—¶é—´æˆ³
- æ‰€æœ‰ä¸­é—´ç»“æœ

---

## é«˜çº§ç‰¹æ€§

### å¤šæ¨¡å¼ç»„åˆ

å¯ä»¥åŒæ—¶ä½¿ç”¨å¤šä¸ªæµå¼æ¨¡å¼ï¼š

```python
from langchain.agents import create_agent
from langgraph.config import get_stream_writer

@tool
def get_weather(city: str) -> str:
    """è·å–åŸå¸‚å¤©æ°”"""
    writer = get_stream_writer()
    writer(f"æ­£åœ¨æŸ¥æ‰¾ {city} çš„æ•°æ®")
    writer(f"å·²è·å– {city} çš„æ•°æ®")
    return f"{city} æ€»æ˜¯æ™´å¤©ï¼"

agent = create_agent(
    model="glm-4.5-air-mini",
    tools=[get_weather],
)

# åŒæ—¶ä½¿ç”¨ updatesã€messages å’Œ custom æ¨¡å¼
for stream_mode, chunk in agent.stream(
    {"messages": [{"role": "user", "content": "æ—§é‡‘å±±å¤©æ°”å¦‚ä½•ï¼Ÿ"}]},
    stream_mode=["updates", "messages", "custom"]
):
    print(f"{stream_mode}: {chunk}\n")
```

**è¾“å‡ºï¼š**
```
updates: {'model': {'messages': [AIMessage(tool_calls=[...])]}}

custom: æ­£åœ¨æŸ¥æ‰¾ San Francisco çš„æ•°æ®

custom: å·²è·å– San Francisco çš„æ•°æ®

updates: {'tools': {'messages': [ToolMessage(...)]}}

messages: (AIMessageChunk(content='æ—§é‡‘å±±'), {...})

updates: {'model': {'messages': [AIMessage(content='...')]}}
```

### ç¦ç”¨ Streaming

åœ¨æŸäº›åœºæ™¯ï¼ˆå¦‚å¤š Agent ç³»ç»Ÿï¼‰ï¼Œå¯èƒ½éœ€è¦ç¦ç”¨ç‰¹å®šæ¨¡å‹çš„æµå¼ä¼ è¾“ã€‚

#### Python

```python
from langchain_openai import ChatOpenAI

# ç¦ç”¨æµå¼ä¼ è¾“
model = ChatOpenAI(
    model="o1-preview",
    disable_streaming=True  # æ˜¾å¼ç¦ç”¨
)
```

æˆ–ä½¿ç”¨ `init_chat_model`ï¼š

```python
from langchain.chat_models import init_chat_model

model = init_chat_model(
    "claude-sonnet-4-5-20250929",
    disable_streaming=True
)
```

#### JavaScript

```javascript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "o1-preview",
  streaming: false  // ç¦ç”¨æµå¼ä¼ è¾“
});
```

**ä½¿ç”¨åœºæ™¯ï¼š**
- æŸäº›æ¨¡å‹ä¸æ”¯æŒæµå¼ä¼ è¾“ï¼ˆå¦‚ OpenAI o1 ç³»åˆ—ï¼‰
- å¤š Agent ç³»ç»Ÿä¸­æ§åˆ¶å“ªäº› Agent æµå¼ä¼ è¾“
- éœ€è¦ç­‰å¾…å®Œæ•´å“åº”å†å¤„ç†

### è¿‡æ»¤ç‰¹å®šèŠ‚ç‚¹

```python
# åªæ˜¾ç¤ºç‰¹å®šèŠ‚ç‚¹çš„æµå¼è¾“å‡º
for msg, metadata in graph.stream(
    inputs,
    stream_mode="messages"
):
    # è¿‡æ»¤æŒ‡å®šèŠ‚ç‚¹
    if msg.content and metadata.get("langgraph_node") == "target_node":
        print(msg.content, end="", flush=True)
```

---

## æ¶ˆæ¯å—å¤„ç†

### AIMessageChunk

åœ¨æµå¼ä¼ è¾“è¿‡ç¨‹ä¸­ï¼Œä½ ä¼šæ”¶åˆ° `AIMessageChunk` å¯¹è±¡ï¼Œå¯ä»¥ç´¯ç§¯æˆå®Œæ•´æ¶ˆæ¯ã€‚

#### Python

```python
from langchain_core.messages import AIMessageChunk

chunks = []
full_message = None

for chunk in model.stream("ä½ å¥½"):
    chunks.append(chunk)
    print(chunk.text)
    
    # ç´¯ç§¯å®Œæ•´æ¶ˆæ¯
    full_message = chunk if full_message is None else full_message + chunk

# full_message ç°åœ¨æ˜¯å®Œæ•´çš„ AIMessage
print(full_message.content_blocks)
```

#### JavaScript

```javascript
import { AIMessageChunk } from "langchain";

let finalChunk = undefined;

for (const chunk of chunks) {
  finalChunk = finalChunk ? finalChunk.concat(chunk) : chunk;
}

console.log(finalChunk.contentBlocks);
```

### æµå¼ä¼ è¾“å·¥å…·è°ƒç”¨

```python
# å·¥å…·è°ƒç”¨é€æ­¥æ„å»º
full = None
for chunk in model_with_tools.stream("æ³¢å£«é¡¿å¤©æ°”ï¼Ÿ"):
    full = chunk if full is None else full + chunk
    print(full.content_blocks)
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```python
[{"type": "tool_call_chunk", "name": "get_weather", "args": ""}]
[{"type": "tool_call_chunk", "name": "get_weather", "args": "{\"city"}]
[{"type": "tool_call_chunk", "name": "get_weather", "args": "\": \"Boston\"}"}]
# ... æœ€ç»ˆå˜ä¸ºå®Œæ•´çš„ tool_call
```

---

## å®é™…åº”ç”¨åœºæ™¯

### 1. èŠå¤©åº”ç”¨

```python
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent

model = init_chat_model("glm-4.5-air")
agent = create_agent(model=model, tools=[])

def chat_stream(user_message: str):
    """æµå¼èŠå¤©å“åº”"""
    for chunk in agent.stream(
        {"messages": [{"role": "user", "content": user_message}]},
        stream_mode="messages"
    ):
        msg, metadata = chunk
        if msg.content:
            yield msg.content

# ä½¿ç”¨
for text in chat_stream("ä»‹ç»ä¸€ä¸‹ LangChain"):
    print(text, end="", flush=True)
```

### 2. è¿›åº¦è·Ÿè¸ªçš„æ–‡æ¡£å¤„ç†

```python
@tool
def process_document(file_path: str) -> str:
    """å¤„ç†æ–‡æ¡£"""
    writer = get_stream_writer()
    
    writer({"stage": "reading", "progress": 0})
    # è¯»å–æ–‡æ¡£
    
    writer({"stage": "analyzing", "progress": 30})
    # åˆ†æå†…å®¹
    
    writer({"stage": "summarizing", "progress": 70})
    # ç”Ÿæˆæ‘˜è¦
    
    writer({"stage": "complete", "progress": 100})
    return "å¤„ç†å®Œæˆ"

agent = create_agent(model="glm-4.5-air", tools=[process_document])

for chunk in agent.stream(
    {"messages": [{"role": "user", "content": "å¤„ç† report.pdf"}]},
    stream_mode="custom"
):
    # æ›´æ–° UI è¿›åº¦æ¡
    update_progress_bar(chunk["progress"])
```

### 3. å¤šæ­¥éª¤ Agent å·¥ä½œæµ

```python
from langgraph.graph import StateGraph, START, END

class WorkflowState(TypedDict):
    query: str
    search_results: list
    analysis: str
    summary: str

def search(state: WorkflowState):
    writer = get_stream_writer()
    writer({"step": "æœç´¢ä¸­..."})
    # æ‰§è¡Œæœç´¢
    return {"search_results": [...]}

def analyze(state: WorkflowState):
    writer = get_stream_writer()
    writer({"step": "åˆ†æç»“æœ..."})
    # åˆ†æ
    return {"analysis": "..."}

def summarize(state: WorkflowState):
    writer = get_stream_writer()
    writer({"step": "ç”Ÿæˆæ‘˜è¦..."})
    # æ€»ç»“
    return {"summary": "..."}

graph = (
    StateGraph(WorkflowState)
    .add_node("search", search)
    .add_node("analyze", analyze)
    .add_node("summarize", summarize)
    .add_edge(START, "search")
    .add_edge("search", "analyze")
    .add_edge("analyze", "summarize")
    .compile()
)

# åŒæ—¶æµå¼ä¼ è¾“æ­¥éª¤å’Œè‡ªå®šä¹‰è¿›åº¦
for mode, chunk in graph.stream(
    {"query": "LangChain æ˜¯ä»€ä¹ˆï¼Ÿ"},
    stream_mode=["updates", "custom"]
):
    if mode == "custom":
        print(f"è¿›åº¦: {chunk['step']}")
    elif mode == "updates":
        print(f"å®ŒæˆèŠ‚ç‚¹: {list(chunk.keys())[0]}")
```

### 4. å®æ—¶æ•°æ®å¤„ç†

```python
@tool
def fetch_realtime_data(source: str) -> str:
    """è·å–å®æ—¶æ•°æ®"""
    writer = get_stream_writer()
    
    for i in range(10):
        # æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµ
        data = fetch_batch(i)
        writer({
            "batch": i,
            "data": data,
            "timestamp": datetime.now()
        })
        time.sleep(0.5)
    
    return "æ•°æ®è·å–å®Œæˆ"

# å®æ—¶æ˜¾ç¤ºæ•°æ®
for chunk in agent.stream(..., stream_mode="custom"):
    plot_data(chunk["data"])  # å®æ—¶ç»˜å›¾
```

---

## æœ€ä½³å®è·µï¼ˆé‡ç‚¹ï¼‰

### 1. **é€‰æ‹©åˆé€‚çš„æµå¼æ¨¡å¼**

```python
# èŠå¤©åº”ç”¨ - ä½¿ç”¨ messages æ¨¡å¼
for msg, _ in agent.stream(..., stream_mode="messages"):
    display_to_user(msg.content)

# è°ƒè¯• - ä½¿ç”¨ debug æ¨¡å¼
for chunk in graph.stream(..., stream_mode="debug"):
    log_to_console(chunk)

# è¿›åº¦è·Ÿè¸ª - ä½¿ç”¨ custom æ¨¡å¼
for progress in agent.stream(..., stream_mode="custom"):
    update_progress_bar(progress)

# çŠ¶æ€ç›‘æ§ - ä½¿ç”¨ updates æ¨¡å¼
for update in graph.stream(..., stream_mode="updates"):
    update_state_display(update)
```

### 2. **æ­£ç¡®å¤„ç†æ¶ˆæ¯å—ç´¯ç§¯**

```python
# âœ… æ­£ç¡®ï¼šç´¯ç§¯å—
full = None
for chunk in model.stream("é•¿æ–‡æœ¬"):
    full = chunk if full is None else full + chunk
    # å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨ full

# âŒ é”™è¯¯ï¼šä¸ç´¯ç§¯ï¼Œä¸¢å¤±ä¸Šä¸‹æ–‡
for chunk in model.stream("é•¿æ–‡æœ¬"):
    print(chunk.text)  # åªæ˜¾ç¤ºéƒ¨åˆ†å†…å®¹
```

### 3. **ä½¿ç”¨ flush ç«‹å³æ˜¾ç¤º**

```python
# âœ… æ­£ç¡®ï¼šç«‹å³åˆ·æ–°è¾“å‡º
for chunk in model.stream("ä½ å¥½"):
    print(chunk.text, end="", flush=True)

# âŒ é”™è¯¯ï¼šå¯èƒ½è¢«ç¼“å†²
for chunk in model.stream("ä½ å¥½"):
    print(chunk.text, end="")  # æ²¡æœ‰ flush
```

### 4. **é”™è¯¯å¤„ç†**

```python
from langgraph.errors import GraphRecursionError

try:
    for chunk in graph.stream(
        inputs,
        stream_mode="updates",
        config={"recursion_limit": 10}
    ):
        process_chunk(chunk)
except GraphRecursionError:
    print("è¾¾åˆ°é€’å½’é™åˆ¶ï¼Œä½†è·å¾—äº†éƒ¨åˆ†ç»“æœ")
except Exception as e:
    print(f"æµå¼ä¼ è¾“é”™è¯¯: {e}")
```

### 5. **ç»„åˆå¤šç§æ¨¡å¼**

```python
# åŒæ—¶è·å–çŠ¶æ€æ›´æ–°å’Œè‡ªå®šä¹‰è¿›åº¦
for mode, chunk in agent.stream(
    inputs,
    stream_mode=["updates", "custom", "messages"]
):
    if mode == "updates":
        log_state_change(chunk)
    elif mode == "custom":
        update_ui_progress(chunk)
    elif mode == "messages":
        display_llm_output(chunk[0].content)
```

### 6. **é¿å…é˜»å¡**

```python
import asyncio

# âœ… å¼‚æ­¥æµå¼ä¼ è¾“ï¼ˆéé˜»å¡ï¼‰
async def async_stream():
    async for chunk in agent.astream(inputs):
        await process_chunk(chunk)

# âŒ åŒæ­¥æµå¼ä¼ è¾“ï¼ˆé˜»å¡ï¼‰
def sync_stream():
    for chunk in agent.stream(inputs):
        process_chunk(chunk)  # é˜»å¡ä¸»çº¿ç¨‹
```

### 7. **å†…å­˜ç®¡ç†**

```python
# å¯¹äºé•¿æ—¶é—´è¿è¡Œçš„æµï¼Œå®šæœŸæ¸…ç†
chunks_buffer = []
MAX_BUFFER_SIZE = 100

for chunk in model.stream("è¶…é•¿æ–‡æœ¬"):
    chunks_buffer.append(chunk)
    
    # å®šæœŸå¤„ç†å¹¶æ¸…ç©ºç¼“å†²åŒº
    if len(chunks_buffer) >= MAX_BUFFER_SIZE:
        process_chunks(chunks_buffer)
        chunks_buffer = []
```

### 8. **å­å›¾æµå¼ä¼ è¾“**

```python
# âœ… å¯ç”¨å­å›¾æµå¼ä¼ è¾“ä»¥è·å¾—å®Œæ•´å¯è§æ€§
for chunk in graph.stream(
    inputs,
    stream_mode="updates",
    subgraphs=True  # åŒ…å«å­å›¾è¾“å‡º
):
    process_chunk(chunk)

# âŒ ä¸å¯ç”¨å­å›¾æµå¼ä¼ è¾“å¯èƒ½ä¸¢å¤±ä¿¡æ¯
for chunk in graph.stream(inputs, stream_mode="updates"):
    # åªèƒ½çœ‹åˆ°çˆ¶å›¾è¾“å‡º
    process_chunk(chunk)
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. **ä½¿ç”¨æµå¼ä¼ è¾“å‡å°‘æ„ŸçŸ¥å»¶è¿Ÿ**

```python
import time

# éæµå¼ï¼šç”¨æˆ·ç­‰å¾…å®Œæ•´å“åº”
start = time.time()
response = model.invoke("å†™ä¸€ç¯‡é•¿æ–‡ç« ")
print(response.content)  # 15 ç§’åæ˜¾ç¤º
print(f"æ€»æ—¶é—´: {time.time() - start}s")

# æµå¼ï¼šç«‹å³å¼€å§‹æ˜¾ç¤º
start = time.time()
first_token_time = None
for chunk in model.stream("å†™ä¸€ç¯‡é•¿æ–‡ç« "):
    if first_token_time is None:
        first_token_time = time.time()
        print(f"é¦–ä¸ª token æ—¶é—´: {first_token_time - start}s")  # ~0.5s
    print(chunk.text, end="", flush=True)
print(f"\næ€»æ—¶é—´: {time.time() - start}s")  # æ€»æ—¶é—´ç›¸åŒï¼Œä½†ä½“éªŒæ›´å¥½
```

### 2. **æ‰¹é‡å¤„ç†**

```python
# âœ… æ‰¹é‡å¤„ç†æµå¼å—
batch = []
BATCH_SIZE = 10

for chunk in model.stream("é•¿æ–‡æœ¬"):
    batch.append(chunk)
    if len(batch) >= BATCH_SIZE:
        process_batch(batch)
        batch = []

# å¤„ç†å‰©ä½™
if batch:
    process_batch(batch)
```

### 3. **å¼‚æ­¥å¹¶å‘**

```python
import asyncio

async def stream_multiple_agents():
    """å¹¶å‘è¿è¡Œå¤šä¸ª agent æµ"""
    tasks = [
        agent1.astream(input1, stream_mode="messages"),
        agent2.astream(input2, stream_mode="messages"),
        agent3.astream(input3, stream_mode="messages"),
    ]
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰æµ
    results = await asyncio.gather(*tasks)
    return results
```

### 4. **é€‰æ‹©æ€§æµå¼ä¼ è¾“**

```python
# åªæµå¼ä¼ è¾“éœ€è¦çš„èŠ‚ç‚¹
for msg, metadata in graph.stream(
    inputs,
    stream_mode="messages"
):
    # åªå¤„ç†ç‰¹å®šèŠ‚ç‚¹
    if metadata.get("langgraph_node") in ["important_node_1", "important_node_2"]:
        process_message(msg)
    # å¿½ç•¥å…¶ä»–èŠ‚ç‚¹ï¼Œå‡å°‘å¤„ç†å¼€é”€
```

### 5. **ç¼“å­˜å’Œå¤ç”¨**

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_cached_stream_result(query: str):
    """ç¼“å­˜æµå¼ç»“æœ"""
    result = []
    for chunk in model.stream(query):
        result.append(chunk)
    return result

# å¤ç”¨ç¼“å­˜çš„æµ
cached = get_cached_stream_result("å¸¸è§é—®é¢˜")
for chunk in cached:
    print(chunk.text, end="")
```

---

## å¿«é€Ÿå‚è€ƒ

### Stream æ¨¡å¼å¯¹æ¯”è¡¨

| æ¨¡å¼ | è¿”å›å†…å®¹ | ä½¿ç”¨åœºæ™¯ | ç¤ºä¾‹è¾“å‡º |
|------|---------|---------|----------|
| `values` | å®Œæ•´çŠ¶æ€ | æŸ¥çœ‹æ‰€æœ‰çŠ¶æ€ | `{"topic": "AI", "joke": "..."}` |
| `updates` | å¢é‡æ›´æ–° | åªå…³å¿ƒå˜åŒ– | `{"node": {"field": "value"}}` |
| `messages` | LLM tokens + å…ƒæ•°æ® | æµå¼èŠå¤© | `(AIMessageChunk("Hi"), {...})` |
| `custom` | è‡ªå®šä¹‰æ•°æ® | è¿›åº¦/æ—¥å¿— | `{"progress": 50, "status": "..."}` |
| `debug` | è¯¦ç»†ä¿¡æ¯ | è°ƒè¯• | `{node, state, metadata, ...}` |

### å¸¸ç”¨ä»£ç ç‰‡æ®µ

#### åŸºæœ¬æµå¼ä¼ è¾“

```python
# Model streaming
for chunk in model.stream("query"):
    print(chunk.text, end="", flush=True)

# Agent streaming
for chunk in agent.stream({"messages": [...]}, stream_mode="updates"):
    print(chunk)

# Graph streaming
for chunk in graph.stream(inputs, stream_mode="values"):
    print(chunk)
```

#### ç´¯ç§¯æ¶ˆæ¯

```python
full = None
for chunk in model.stream("query"):
    full = chunk if full is None else full + chunk
```

#### è‡ªå®šä¹‰è¿›åº¦

```python
from langgraph.config import get_stream_writer

def my_node(state):
    writer = get_stream_writer()
    writer({"progress": "å¼€å§‹"})
    # ... å¤„ç†
    writer({"progress": "å®Œæˆ"})
    return state

# æµå¼ä¼ è¾“
for chunk in graph.stream(inputs, stream_mode="custom"):
    print(chunk["progress"])
```

#### å¤šæ¨¡å¼

```python
for mode, chunk in agent.stream(
    inputs,
    stream_mode=["updates", "messages", "custom"]
):
    if mode == "updates":
        print(f"State: {chunk}")
    elif mode == "messages":
        print(f"Token: {chunk[0].content}")
    elif mode == "custom":
        print(f"Progress: {chunk}")
```

#### å­å›¾æµå¼ä¼ è¾“

```python
for chunk in graph.stream(
    inputs,
    stream_mode="updates",
    subgraphs=True  # åŒ…å«å­å›¾
):
    namespace, data = chunk
    if namespace == ():
        print("çˆ¶å›¾:", data)
    else:
        print(f"å­å›¾ {namespace}:", data)
```

### äº‹ä»¶ç±»å‹ï¼ˆastream_eventsï¼‰

```python
async for event in model.astream_events("Hello"):
    if event["event"] == "on_chat_model_start":
        print(f"å¼€å§‹: {event['data']['input']}")
    
    elif event["event"] == "on_chat_model_stream":
        print(f"Token: {event['data']['chunk'].text}")
    
    elif event["event"] == "on_chat_model_end":
        print(f"å®Œæ•´æ¶ˆæ¯: {event['data']['output'].text}")
```

### ç¦ç”¨æµå¼ä¼ è¾“

```python
# Python
model = ChatOpenAI(model="o1-preview", disable_streaming=True)

# JavaScript
const model = new ChatOpenAI({ model: "o1-preview", streaming: false });
```

---

## æ€»ç»“

LangChain å’Œ LangGraph çš„æµå¼ä¼ è¾“ç³»ç»Ÿæä¾›äº†å¼ºå¤§è€Œçµæ´»çš„å®æ—¶æ•°æ®ä¼ è¾“èƒ½åŠ›ï¼š

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**
âœ… æ”¹å–„ç”¨æˆ·ä½“éªŒ - å³æ—¶åé¦ˆå’Œè¿›åº¦å¯è§†åŒ–  
âœ… å¤šç§æµå¼æ¨¡å¼ - é€‚åº”ä¸åŒåœºæ™¯éœ€æ±‚  
âœ… è‡ªåŠ¨æµå¼ä¼ è¾“ - ç®€åŒ–å¼€å‘å¤æ‚åº¦  
âœ… å­å›¾æ”¯æŒ - å¤æ‚å·¥ä½œæµçš„å®Œæ•´å¯è§æ€§  
âœ… è‡ªå®šä¹‰æ•°æ® - çµæ´»çš„è¿›åº¦å’Œæ—¥å¿—ä¼ è¾“  

**å…³é”®è¦ç‚¹ï¼š**
- ä½¿ç”¨ `stream()` æˆ– `astream()` æ–¹æ³•å¯åŠ¨æµå¼ä¼ è¾“
- é€šè¿‡ `stream_mode` å‚æ•°é€‰æ‹©åˆé€‚çš„æµå¼æ¨¡å¼
- ä½¿ç”¨ `get_stream_writer()` å‘é€è‡ªå®šä¹‰è¿›åº¦æ›´æ–°
- æ­£ç¡®ç´¯ç§¯ `AIMessageChunk` ä»¥è·å¾—å®Œæ•´æ¶ˆæ¯
- å¯¹äºå¤æ‚å·¥ä½œæµï¼Œå¯ç”¨ `subgraphs=True`
- ä½¿ç”¨å¤šæ¨¡å¼ç»„åˆè·å¾—å…¨é¢çš„å¯è§‚å¯Ÿæ€§

é€šè¿‡åˆç†ä½¿ç”¨æµå¼ä¼ è¾“ï¼Œä½ å¯ä»¥æ„å»ºå“åº”è¿…é€Ÿã€ç”¨æˆ·ä½“éªŒä¼˜ç§€çš„ LLM åº”ç”¨ç¨‹åºï¼
