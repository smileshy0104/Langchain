# LangChain Middleware è¯¦ç»†æŒ‡å—

## ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [ä¸ºä»€ä¹ˆéœ€è¦ Middleware](#ä¸ºä»€ä¹ˆéœ€è¦-middleware)
3. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
4. [Middleware ç”Ÿå‘½å‘¨æœŸé’©å­](#middleware-ç”Ÿå‘½å‘¨æœŸé’©å­)
5. [æ‰§è¡Œé¡ºåº](#æ‰§è¡Œé¡ºåº)
6. [è£…é¥°å™¨å¼ Middleware](#è£…é¥°å™¨å¼-middleware)
7. [ç±»å¼ Middleware](#ç±»å¼-middleware)
8. [é¢„ç½® Middleware](#é¢„ç½®-middleware)
9. [è‡ªå®šä¹‰ Middleware å®æˆ˜](#è‡ªå®šä¹‰-middleware-å®æˆ˜)
10. [è‡ªå®šä¹‰çŠ¶æ€ç®¡ç†](#è‡ªå®šä¹‰çŠ¶æ€ç®¡ç†)
11. [Runtime è®¿é—®](#runtime-è®¿é—®)
12. [å®é™…åº”ç”¨åœºæ™¯](#å®é™…åº”ç”¨åœºæ™¯)
13. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
14. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
15. [å¿«é€Ÿå‚è€ƒ](#å¿«é€Ÿå‚è€ƒ)

---

## æ¦‚è¿°

Middlewareï¼ˆä¸­é—´ä»¶ï¼‰æ˜¯ LangChain `create_agent` çš„æ ¸å¿ƒç‰¹æ€§ï¼Œæä¾›äº†ä¸€ç§åœ¨ Agent æ‰§è¡Œçš„ä¸åŒé˜¶æ®µç²¾ç¡®æ§åˆ¶è¡Œä¸ºçš„æ–¹æ³•ã€‚é€šè¿‡ Middlewareï¼Œä½ å¯ä»¥æ‹¦æˆªå¹¶ä¿®æ”¹ Agent æ‰§è¡Œæµç¨‹ä¸­çš„æ•°æ®ï¼Œè€Œæ— éœ€æ”¹å˜æ ¸å¿ƒ Agent é€»è¾‘ã€‚

**æ ¸å¿ƒ Agent å¾ªç¯ï¼š**
```
å¼€å§‹ â†’ è°ƒç”¨æ¨¡å‹ â†’ æ¨¡å‹é€‰æ‹©å·¥å…· â†’ æ‰§è¡Œå·¥å…· â†’ ç»§ç»­æˆ–ç»“æŸ
```

**Middleware æä¾›çš„é’©å­ï¼š**
- âœ… **before_agent** - Agent å¼€å§‹å‰
- âœ… **before_model** - æ¯æ¬¡è°ƒç”¨æ¨¡å‹å‰
- âœ… **wrap_model_call** - åŒ…è£…æ¨¡å‹è°ƒç”¨
- âœ… **after_model** - æ¯æ¬¡æ¨¡å‹å“åº”å
- âœ… **wrap_tool_call** - åŒ…è£…å·¥å…·è°ƒç”¨
- âœ… **after_agent** - Agent å®Œæˆå

---

## ä¸ºä»€ä¹ˆéœ€è¦ Middleware

### 1. **ä¸Šä¸‹æ–‡å·¥ç¨‹ (Context Engineering)**

ä¼˜ç§€çš„ Agent éœ€è¦åœ¨æ­£ç¡®çš„æ—¶é—´è·å¾—æ­£ç¡®çš„ä¿¡æ¯ã€‚Middleware å¸®åŠ©ä½ ï¼š
- åŠ¨æ€è°ƒæ•´ prompt
- æ€»ç»“å¯¹è¯å†å²
- é€‰æ‹©æ€§å·¥å…·è®¿é—®
- çŠ¶æ€ç®¡ç†

### 2. **å¯ç»„åˆæ€§**

```python
from langchain.agents import create_agent
from langchain.agents.middleware import (
    SummarizationMiddleware,
    PIIMiddleware,
    HumanInTheLoopMiddleware
)

agent = create_agent(
    model="gpt-4o",
    tools=[read_email, send_email],
    middleware=[
        PIIMiddleware("email", strategy="redact"),           # PII ä¿æŠ¤
        SummarizationMiddleware(                              # å¯¹è¯æ€»ç»“
            model="gpt-4o-mini",
            max_tokens_before_summary=500
        ),
        HumanInTheLoopMiddleware(                             # äººå·¥å®¡æ ¸
            interrupt_on={"send_email": {"allowed_decisions": ["approve", "edit", "reject"]}}
        ),
    ]
)
```

### 3. **å…³æ³¨ç‚¹åˆ†ç¦»**

Middleware è®©ä½ å°†æ¨ªåˆ‡å…³æ³¨ç‚¹ï¼ˆæ—¥å¿—ã€ç›‘æ§ã€å®‰å…¨ï¼‰ä»æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ä¸­åˆ†ç¦»å‡ºæ¥ï¼š
- ğŸ“Š ç›‘æ§ - è¿½è¸ª Agent è¡Œä¸ºã€æ—¥å¿—ã€åˆ†æ
- ğŸ”„ ä¿®æ”¹ - è½¬æ¢ promptsã€å·¥å…·é€‰æ‹©ã€è¾“å‡ºæ ¼å¼
- ğŸ® æ§åˆ¶ - æ·»åŠ é‡è¯•ã€å›é€€ã€æå‰ç»ˆæ­¢é€»è¾‘
- ğŸ›¡ï¸ æ‰§è¡Œ - åº”ç”¨é€Ÿç‡é™åˆ¶ã€æŠ¤æ ã€PII æ£€æµ‹

---

## æ ¸å¿ƒæ¦‚å¿µ

### Agent æ‰§è¡Œæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ before_agent()                                          â”‚
â”‚ â†“                                                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Agent Loop                                          â”‚ â”‚
â”‚ â”‚                                                     â”‚ â”‚
â”‚ â”‚ before_model()                                      â”‚ â”‚
â”‚ â”‚ â†“                                                   â”‚ â”‚
â”‚ â”‚ wrap_model_call() â†’ æ¨¡å‹è°ƒç”¨                         â”‚ â”‚
â”‚ â”‚ â†“                                                   â”‚ â”‚
â”‚ â”‚ after_model()                                       â”‚ â”‚
â”‚ â”‚ â†“                                                   â”‚ â”‚
â”‚ â”‚ wrap_tool_call() â†’ å·¥å…·æ‰§è¡Œ (å¦‚æœæœ‰å·¥å…·è°ƒç”¨)         â”‚ â”‚
â”‚ â”‚ â†“                                                   â”‚ â”‚
â”‚ â”‚ (å¾ªç¯ç›´åˆ°æ— å·¥å…·è°ƒç”¨)                                 â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â†“                                                       â”‚
â”‚ after_agent()                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Middleware ç±»å‹

#### 1. **è£…é¥°å™¨å¼ Middleware**
å¿«é€Ÿæ·»åŠ å•ä¸€é’©å­åŠŸèƒ½ï¼š

```python
from langchain.agents.middleware import before_model, after_model

@before_model
def log_before_model(state, runtime):
    print(f"è°ƒç”¨æ¨¡å‹ï¼Œæ¶ˆæ¯æ•°: {len(state['messages'])}")
    return None
```

#### 2. **ç±»å¼ Middleware**
å®ç°å¤šä¸ªé’©å­çš„å¤æ‚é€»è¾‘ï¼š

```python
from langchain.agents.middleware import AgentMiddleware

class CustomMiddleware(AgentMiddleware):
    def before_model(self, state, runtime):
        # æ¨¡å‹è°ƒç”¨å‰çš„é€»è¾‘
        return None
    
    def after_model(self, state, runtime):
        # æ¨¡å‹å“åº”åçš„é€»è¾‘
        return None
```

---

## Middleware ç”Ÿå‘½å‘¨æœŸé’©å­

### 1. before_agent

**æ—¶æœºï¼š** Agent å¼€å§‹æ‰§è¡Œå‰è¿è¡Œä¸€æ¬¡  
**ç”¨é€”ï¼š** åŠ è½½å†…å­˜ã€éªŒè¯è¾“å…¥ã€åˆå§‹åŒ–èµ„æº

```python
from langchain.agents.middleware import before_agent
from langgraph.runtime import Runtime

@before_agent
def load_user_memory(state: AgentState, runtime: Runtime):
    """ä»æ•°æ®åº“åŠ è½½ç”¨æˆ·å†å²"""
    user_id = runtime.context.user_id
    history = database.load_history(user_id)
    
    return {"messages": history + state["messages"]}
```

### 2. before_model

**æ—¶æœºï¼š** æ¯æ¬¡è°ƒç”¨ LLM å‰æ‰§è¡Œ  
**ç”¨é€”ï¼š** æ›´æ–° promptsã€ä¿®å‰ªæ¶ˆæ¯ã€æ³¨å…¥ä¸Šä¸‹æ–‡

```python
from langchain.agents.middleware import before_model

@before_model
def trim_messages(state: AgentState, runtime: Runtime):
    """ä¿ç•™æœ€åå‡ æ¡æ¶ˆæ¯ä»¥é€‚åº”ä¸Šä¸‹æ–‡çª—å£"""
    messages = state["messages"]
    
    if len(messages) <= 5:
        return None  # æ— éœ€ä¿®æ”¹
    
    # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘çš„æ¶ˆæ¯
    system_msg = messages[0]
    recent_msgs = messages[-4:]
    
    return {"messages": [system_msg] + recent_msgs}
```

**è¿”å›å€¼ï¼š**
- `None` - ä¸ä¿®æ”¹çŠ¶æ€
- `dict` - æ›´æ–°çŠ¶æ€ï¼ˆä¸ç°æœ‰çŠ¶æ€åˆå¹¶ï¼‰
- `{"jump_to": "node_name"}` - è·³è½¬åˆ°æŒ‡å®šèŠ‚ç‚¹

### 3. wrap_model_call

**æ—¶æœºï¼š** åŒ…è£…æ¨¡å‹è°ƒç”¨  
**ç”¨é€”ï¼š** æ‹¦æˆªè¯·æ±‚/å“åº”ã€é‡è¯•é€»è¾‘ã€åŠ¨æ€æ¨¡å‹é€‰æ‹©

```python
from langchain.agents.middleware import wrap_model_call
from langchain.agents.middleware import ModelRequest, ModelResponse
from typing import Callable

@wrap_model_call
def retry_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    """æ·»åŠ é‡è¯•é€»è¾‘"""
    for attempt in range(3):
        try:
            return handler(request)
        except Exception as e:
            if attempt == 2:
                raise
            print(f"é‡è¯• {attempt + 1}/3ï¼Œé”™è¯¯: {e}")
```

**åŠ¨æ€æ¨¡å‹é€‰æ‹©ï¼š**
```python
@wrap_model_call
def dynamic_model_selection(request: ModelRequest, handler):
    """æ ¹æ®æ¶ˆæ¯æ•°é‡é€‰æ‹©æ¨¡å‹"""
    message_count = len(request.messages)
    
    if message_count > 10:
        # å¤æ‚å¯¹è¯ä½¿ç”¨å¼ºå¤§æ¨¡å‹
        request.model = ChatOpenAI(model="gpt-4o")
    else:
        # ç®€å•å¯¹è¯ä½¿ç”¨è½»é‡æ¨¡å‹
        request.model = ChatOpenAI(model="gpt-4o-mini")
    
    return handler(request)
```

### 4. after_model

**æ—¶æœºï¼š** æ¯æ¬¡ LLM å“åº”åæ‰§è¡Œ  
**ç”¨é€”ï¼š** éªŒè¯è¾“å‡ºã€åº”ç”¨æŠ¤æ ã€å†…å®¹è¿‡æ»¤

```python
from langchain.agents.middleware import after_model
from langchain.messages import AIMessage

@after_model(can_jump_to=["end"])
def validate_output(state: AgentState, runtime: Runtime):
    """éªŒè¯æ¨¡å‹è¾“å‡ºå¹¶åº”ç”¨å†…å®¹è¿‡æ»¤"""
    last_message = state["messages"][-1]
    
    # æ£€æŸ¥è¢«é˜»æ­¢çš„å†…å®¹
    if "BLOCKED" in last_message.content:
        return {
            "messages": [AIMessage("æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å“åº”è¯¥è¯·æ±‚ã€‚")],
            "jump_to": "end"
        }
    
    return None
```

### 5. wrap_tool_call

**æ—¶æœºï¼š** åŒ…è£…å·¥å…·æ‰§è¡Œ  
**ç”¨é€”ï¼š** é”™è¯¯å¤„ç†ã€æƒé™æ£€æŸ¥ã€æ—¥å¿—è®°å½•

```python
from langchain.agents.middleware import wrap_tool_call
from langchain.messages import ToolMessage

@wrap_tool_call
def handle_tool_errors(request, handler):
    """è‡ªå®šä¹‰å·¥å…·é”™è¯¯å¤„ç†"""
    try:
        return handler(request)
    except Exception as e:
        # è¿”å›å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
        return ToolMessage(
            content=f"å·¥å…·é”™è¯¯ï¼šè¯·æ£€æŸ¥è¾“å…¥å¹¶é‡è¯•ã€‚({e})",
            tool_call_id=request.tool_call.id
        )
```

### 6. after_agent

**æ—¶æœºï¼š** Agent å®Œæˆæ‰§è¡Œåè¿è¡Œä¸€æ¬¡  
**ç”¨é€”ï¼š** ä¿å­˜ç»“æœã€æ¸…ç†èµ„æºã€è®°å½•åˆ†æ

```python
from langchain.agents.middleware import after_agent

@after_agent
def save_results(state: AgentState, runtime: Runtime):
    """ä¿å­˜å¯¹è¯å†å²åˆ°æ•°æ®åº“"""
    user_id = runtime.context.user_id
    database.save_conversation(user_id, state["messages"])
    
    # ä¸ä¿®æ”¹çŠ¶æ€
    return None
```

---

## æ‰§è¡Œé¡ºåº

### å¤šä¸ª Middleware çš„æ‰§è¡Œé¡ºåº

å½“ä½¿ç”¨å¤šä¸ª Middleware æ—¶ï¼Œæ‰§è¡Œé¡ºåºéµå¾ªç‰¹å®šè§„åˆ™ï¼š

```python
agent = create_agent(
    model="gpt-4o",
    middleware=[middleware1, middleware2, middleware3],
    tools=[...]
)
```

**æ‰§è¡Œæµç¨‹ï¼š**

```
Before é’©å­ï¼ˆæŒ‰é¡ºåºï¼‰:
  middleware1.before_agent()
  middleware2.before_agent()
  middleware3.before_agent()

Agent å¾ªç¯å¼€å§‹:
  middleware1.before_model()
  middleware2.before_model()
  middleware3.before_model()

Wrap é’©å­ï¼ˆåµŒå¥—è°ƒç”¨ï¼‰:
  middleware1.wrap_model_call() â†’
    middleware2.wrap_model_call() â†’
      middleware3.wrap_model_call() â†’
        æ¨¡å‹è°ƒç”¨

After é’©å­ï¼ˆåå‘é¡ºåºï¼‰:
  middleware3.after_model()
  middleware2.after_model()
  middleware1.after_model()

Agent å¾ªç¯ç»“æŸ:
  middleware3.after_agent()
  middleware2.after_agent()
  middleware1.after_agent()
```

**å…³é”®è§„åˆ™ï¼š**
- âœ… `before_*` é’©å­ï¼šä»ç¬¬ä¸€ä¸ªåˆ°æœ€åä¸€ä¸ª
- âœ… `after_*` é’©å­ï¼šä»æœ€åä¸€ä¸ªåˆ°ç¬¬ä¸€ä¸ªï¼ˆåå‘ï¼‰
- âœ… `wrap_*` é’©å­ï¼šåµŒå¥—è°ƒç”¨ï¼ˆç¬¬ä¸€ä¸ªåŒ…è£…æ‰€æœ‰å…¶ä»–çš„ï¼‰

**ç¤ºä¾‹ï¼š**

```python
from langchain.agents.middleware import before_model, after_model

@before_model
def middleware_a(state, runtime):
    print("A: before")
    return None

@before_model
def middleware_b(state, runtime):
    print("B: before")
    return None

@after_model
def middleware_c(state, runtime):
    print("C: after")
    return None

agent = create_agent(
    model="gpt-4o",
    middleware=[middleware_a, middleware_b, middleware_c],
    tools=[]
)
```

**è¾“å‡ºï¼š**
```
A: before
B: before
(æ¨¡å‹è°ƒç”¨)
C: after
```

---

## è£…é¥°å™¨å¼ Middleware

è£…é¥°å™¨æä¾›æœ€å¿«æ·çš„æ–¹å¼æ·»åŠ å•ä¸€é’©å­åŠŸèƒ½ã€‚

### 1. @before_model - èŠ‚ç‚¹é£æ ¼

```python
from langchain.agents.middleware import before_model, AgentState
from langgraph.runtime import Runtime
from typing import Any

@before_model
def log_and_trim(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """æ—¥å¿—è®°å½•å¹¶ä¿®å‰ªæ¶ˆæ¯"""
    print(f"å°†è¦è°ƒç”¨æ¨¡å‹ï¼Œæ¶ˆæ¯æ•°: {len(state['messages'])}")
    
    # ä¿®å‰ªæ¶ˆæ¯
    if len(state["messages"]) > 10:
        return {"messages": state["messages"][-10:]}
    
    return None
```

### 2. @after_model - èŠ‚ç‚¹é£æ ¼

```python
from langchain.agents.middleware import after_model

@after_model(can_jump_to=["end"])
def check_safety(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """å®‰å…¨æ£€æŸ¥"""
    last_msg = state["messages"][-1]
    
    if contains_unsafe_content(last_msg.content):
        return {
            "messages": [AIMessage("å†…å®¹å·²è¢«è¿‡æ»¤")],
            "jump_to": "end"
        }
    
    return None
```

### 3. @wrap_model_call - åŒ…è£…é£æ ¼

```python
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable

@wrap_model_call
def add_caching(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse]
) -> ModelResponse:
    """æ·»åŠ ç¼“å­˜å±‚"""
    cache_key = hash_request(request)
    
    # æ£€æŸ¥ç¼“å­˜
    if cache_key in cache:
        return cache[cache_key]
    
    # è°ƒç”¨æ¨¡å‹
    response = handler(request)
    
    # å­˜å…¥ç¼“å­˜
    cache[cache_key] = response
    return response
```

### 4. @dynamic_prompt - åŠ¨æ€æç¤ºè¯

```python
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dynamic_prompt
def personalized_prompt(request: ModelRequest) -> str:
    """æ ¹æ®ç”¨æˆ·è§’è‰²ç”ŸæˆåŠ¨æ€æç¤ºè¯"""
    user_role = request.runtime.context.get("user_role", "user")
    
    base_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"
    
    if user_role == "expert":
        return f"{base_prompt} æä¾›è¯¦ç»†çš„æŠ€æœ¯å›ç­”ã€‚"
    elif user_role == "beginner":
        return f"{base_prompt} ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šæ¦‚å¿µï¼Œé¿å…æœ¯è¯­ã€‚"
    
    return base_prompt
```

### 5. @wrap_tool_call - å·¥å…·åŒ…è£…

```python
from langchain.agents.middleware import wrap_tool_call

@wrap_tool_call
def log_tool_usage(request, handler):
    """è®°å½•å·¥å…·ä½¿ç”¨æƒ…å†µ"""
    tool_name = request.tool_call.name
    
    print(f"æ‰§è¡Œå·¥å…·: {tool_name}")
    start_time = time.time()
    
    try:
        result = handler(request)
        elapsed = time.time() - start_time
        print(f"å·¥å…· {tool_name} æˆåŠŸï¼Œè€—æ—¶ {elapsed:.2f}s")
        return result
    except Exception as e:
        print(f"å·¥å…· {tool_name} å¤±è´¥: {e}")
        raise
```

---

## ç±»å¼ Middleware

ç±»å¼ Middleware é€‚ç”¨äºéœ€è¦å®ç°å¤šä¸ªé’©å­æˆ–ç»´æŠ¤çŠ¶æ€çš„å¤æ‚åœºæ™¯ã€‚

### åŸºæœ¬ç»“æ„

```python
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Any, Callable

class CustomMiddleware(AgentMiddleware):
    """è‡ªå®šä¹‰ä¸­é—´ä»¶æ¨¡æ¿"""
    
    def __init__(self, config: dict):
        self.config = config
    
    def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        """Agent å¼€å§‹å‰"""
        return None
    
    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        """æ¨¡å‹è°ƒç”¨å‰"""
        return None
    
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        """åŒ…è£…æ¨¡å‹è°ƒç”¨"""
        return handler(request)
    
    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        """æ¨¡å‹å“åº”å"""
        return None
    
    def wrap_tool_call(self, request, handler):
        """åŒ…è£…å·¥å…·è°ƒç”¨"""
        return handler(request)
    
    def after_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        """Agent å®Œæˆå"""
        return None
```

### å®ä¾‹ï¼šè¯·æ±‚è®¡æ•°ä¸­é—´ä»¶

```python
from dataclasses import dataclass

@dataclass
class Context:
    user_id: str
    max_requests: int = 10

class RequestCounterMiddleware(AgentMiddleware):
    """é™åˆ¶æ¯ä¸ªç”¨æˆ·çš„è¯·æ±‚æ¬¡æ•°"""
    
    def __init__(self):
        self.request_counts = {}
    
    def before_agent(self, state: AgentState, runtime: Runtime[Context]) -> dict | None:
        """æ£€æŸ¥è¯·æ±‚é™åˆ¶"""
        user_id = runtime.context.user_id
        max_req = runtime.context.max_requests
        
        # å¢åŠ è®¡æ•°
        count = self.request_counts.get(user_id, 0) + 1
        self.request_counts[user_id] = count
        
        # æ£€æŸ¥æ˜¯å¦è¶…é™
        if count > max_req:
            raise ValueError(f"ç”¨æˆ· {user_id} è¶…è¿‡è¯·æ±‚é™åˆ¶ ({max_requests})")
        
        print(f"ç”¨æˆ· {user_id} è¯·æ±‚ {count}/{max_req}")
        return None
    
    def after_agent(self, state: AgentState, runtime: Runtime[Context]) -> dict | None:
        """è®°å½•å®Œæˆ"""
        user_id = runtime.context.user_id
        print(f"ç”¨æˆ· {user_id} è¯·æ±‚å®Œæˆ")
        return None

# ä½¿ç”¨
agent = create_agent(
    model="gpt-4o",
    middleware=[RequestCounterMiddleware()],
    tools=[],
    context_schema=Context
)

agent.invoke(
    {"messages": [{"role": "user", "content": "ä½ å¥½"}]},
    context=Context(user_id="alice", max_requests=5)
)
```

---

## é¢„ç½® Middleware

LangChain æä¾›äº†å¸¸ç”¨æ¨¡å¼çš„é¢„ç½® Middlewareã€‚

### 1. SummarizationMiddleware - å¯¹è¯æ€»ç»“

å½“å¯¹è¯å†å²è¿‡é•¿æ—¶ï¼Œè‡ªåŠ¨æ€»ç»“å¹¶å‹ç¼©ã€‚

```python
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",              # ç”¨äºæ€»ç»“çš„æ¨¡å‹
            max_tokens_before_summary=4000,   # è§¦å‘æ€»ç»“çš„é˜ˆå€¼
            messages_to_keep=20,              # æ€»ç»“åä¿ç•™çš„æ¶ˆæ¯æ•°
            summary_prompt="è‡ªå®šä¹‰æ€»ç»“æç¤ºè¯...",  # å¯é€‰
        )
    ]
)
```

**å·¥ä½œåŸç†ï¼š**
1. ç›‘æ§æ¶ˆæ¯å†å²çš„ token æ•°é‡
2. è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œä½¿ç”¨å•ç‹¬çš„ LLM è°ƒç”¨ç”Ÿæˆæ€»ç»“
3. ç”¨æ€»ç»“æ¶ˆæ¯**æ°¸ä¹…æ›¿æ¢**æ—§æ¶ˆæ¯ï¼ˆæŒä¹…åŒ–æ›´æ–°çŠ¶æ€ï¼‰
4. ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯ä»¥ç»´æŒä¸Šä¸‹æ–‡

**ç¤ºä¾‹ï¼š**

```python
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()

agent = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",
            max_tokens_before_summary=4000,
            messages_to_keep=20,
        )
    ],
    checkpointer=checkpointer,
)

config = {"configurable": {"thread_id": "1"}}

agent.invoke({"messages": "ä½ å¥½ï¼Œæˆ‘å« Bob"}, config)
agent.invoke({"messages": "å†™ä¸€é¦–å…³äºçŒ«çš„è¯—"}, config)
agent.invoke({"messages": "ç°åœ¨å†™ä¸€é¦–å…³äºç‹—çš„"}, config)
# ... è®¸å¤šè½®å¯¹è¯å ...
result = agent.invoke({"messages": "æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"}, config)

print(result["messages"][-1].content)
# "ä½ çš„åå­—æ˜¯ Bobï¼"
```

### 2. PIIMiddleware - PII ä¿æŠ¤

è‡ªåŠ¨æ£€æµ‹å’Œå¤„ç†ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰ã€‚

```python
from langchain.agents.middleware import PIIMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[send_email],
    middleware=[
        # ç¼–è¾‘ç”µå­é‚®ä»¶åœ°å€
        PIIMiddleware("email", strategy="redact", apply_to_input=True),
        
        # é˜»æ­¢åŒ…å«ç”µè¯å·ç çš„è¯·æ±‚
        PIIMiddleware(
            "phone_number",
            detector=r"(?:\+?\d{1,3}[\s.-]?)?(?:\(?\d{2,4}\)?[\s.-]?)?\d{3,4}[\s.-]?\d{4}",
            strategy="block"
        ),
    ]
)
```

**ç­–ç•¥ï¼š**
- `redact` - ç¼–è¾‘ PIIï¼ˆæ›¿æ¢ä¸º `[REDACTED]`ï¼‰
- `block` - é˜»æ­¢åŒ…å« PII çš„è¯·æ±‚

### 3. HumanInTheLoopMiddleware - äººå·¥å®¡æ ¸

è¦æ±‚äººå·¥æ‰¹å‡†æ•æ„Ÿçš„å·¥å…·è°ƒç”¨ã€‚

```python
from langchain.agents.middleware import HumanInTheLoopMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[read_email, send_email, delete_file],
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={
                "send_email": {
                    "allowed_decisions": ["approve", "edit", "reject"]
                },
                "delete_file": {
                    "allowed_decisions": ["approve", "reject"]
                }
            }
        )
    ]
)
```

**å·¥ä½œæµç¨‹ï¼š**
1. Agent æè®®è°ƒç”¨å·¥å…·ï¼ˆå¦‚ `send_email`ï¼‰
2. Middleware æ‹¦æˆªå¹¶å‘å‡ºä¸­æ–­ï¼ˆinterruptï¼‰
3. æ‰§è¡Œæš‚åœï¼Œç­‰å¾…äººå·¥å†³ç­–
4. äººå·¥åšå‡ºå†³ç­–ï¼š
   - `approve` - æ‰¹å‡†æ‰§è¡Œ
   - `edit` - ä¿®æ”¹å‚æ•°åæ‰§è¡Œ
   - `reject` - æ‹’ç»å¹¶æä¾›åé¦ˆ
5. æ ¹æ®å†³ç­–æ¢å¤æ‰§è¡Œ

### 4. ContextEditingMiddleware - ä¸Šä¸‹æ–‡ç¼–è¾‘

ç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå®šæœŸæ¸…ç†å·¥å…·ä½¿ç”¨è®°å½•ã€‚

```python
from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[
        ContextEditingMiddleware(
            edits=[
                ClearToolUsesEdit(trigger=1000),  # æ¯ 1000 tokens æ¸…ç†å·¥å…·ä½¿ç”¨
            ]
        )
    ]
)
```

---

## è‡ªå®šä¹‰ Middleware å®æˆ˜

### å®ä¾‹ 1ï¼šåŸºäºä¸“ä¸šçŸ¥è¯†çš„åŠ¨æ€å·¥å…·é€‰æ‹©

æ ¹æ®ç”¨æˆ·ä¸“ä¸šæ°´å¹³åŠ¨æ€é€‰æ‹©å·¥å…·å’Œæ¨¡å‹ã€‚

```python
from dataclasses import dataclass
from typing import Callable
from langchain_openai import ChatOpenAI
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse

@dataclass
class Context:
    user_expertise: str = "beginner"  # "beginner" æˆ– "expert"

class ExpertiseBasedMiddleware(AgentMiddleware):
    """æ ¹æ®ç”¨æˆ·ä¸“ä¸šæ°´å¹³é€‰æ‹©å·¥å…·å’Œæ¨¡å‹"""
    
    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        user_level = request.runtime.context.user_expertise
        
        if user_level == "expert":
            # ä¸“å®¶çº§ç”¨æˆ·ï¼šå¼ºå¤§æ¨¡å‹ + é«˜çº§å·¥å…·
            request.model = ChatOpenAI(model="gpt-4o")
            request.tools = [advanced_search, data_analysis, code_execution]
        else:
            # åˆå­¦è€…ï¼šè½»é‡æ¨¡å‹ + åŸºç¡€å·¥å…·
            request.model = ChatOpenAI(model="gpt-4o-mini")
            request.tools = [simple_search, basic_calculator]
        
        return handler(request)

# ä½¿ç”¨
agent = create_agent(
    model="gpt-4o",  # åŸºç¡€æ¨¡å‹
    tools=[simple_search, advanced_search, basic_calculator, data_analysis, code_execution],
    middleware=[ExpertiseBasedMiddleware()],
    context_schema=Context
)

# åˆå­¦è€…æ¨¡å¼
result_beginner = agent.invoke(
    {"messages": [{"role": "user", "content": "è§£é‡Šæœºå™¨å­¦ä¹ "}]},
    context=Context(user_expertise="beginner")
)

# ä¸“å®¶æ¨¡å¼
result_expert = agent.invoke(
    {"messages": [{"role": "user", "content": "è§£é‡Š Transformer æ¶æ„"}]},
    context=Context(user_expertise="expert")
)
```

### å®ä¾‹ 2ï¼šæ™ºèƒ½é‡è¯•ä¸å›é€€

æ·»åŠ é‡è¯•é€»è¾‘å’Œæ¨¡å‹å›é€€ç­–ç•¥ã€‚

```python
import time
from langchain.agents.middleware import wrap_model_call

@wrap_model_call
def retry_with_fallback(request: ModelRequest, handler):
    """é‡è¯•å¤±è´¥çš„è°ƒç”¨ï¼Œå¹¶åœ¨å¿…è¦æ—¶é™çº§åˆ°å¤‡ç”¨æ¨¡å‹"""
    primary_model = request.model
    fallback_model = ChatOpenAI(model="gpt-4o-mini")
    
    # å°è¯•ä¸»æ¨¡å‹ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰
    for attempt in range(3):
        try:
            return handler(request)
        except Exception as e:
            if attempt == 2:
                # ä¸»æ¨¡å‹å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹
                print(f"ä¸»æ¨¡å‹å¤±è´¥ï¼Œåˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹")
                request.model = fallback_model
                try:
                    return handler(request)
                except Exception as fallback_e:
                    raise Exception(f"ä¸»æ¨¡å‹å’Œå¤‡ç”¨æ¨¡å‹éƒ½å¤±è´¥: {e}, {fallback_e}")
            
            # æŒ‡æ•°é€€é¿
            wait_time = 2 ** attempt
            print(f"é‡è¯• {attempt + 1}/3ï¼Œ{wait_time}ç§’åé‡è¯•...")
            time.sleep(wait_time)
```

### å®ä¾‹ 3ï¼šå†…å®¹å®¡æ ¸ä¸æŠ¤æ 

å®æ–½å†…å®¹å®¡æ ¸å’Œå®‰å…¨æŠ¤æ ã€‚

```python
from langchain.agents.middleware import before_model, after_model
from langchain.messages import AIMessage

# è¾“å…¥å®¡æ ¸
@before_model
def moderate_input(state: AgentState, runtime: Runtime):
    """å®¡æ ¸ç”¨æˆ·è¾“å…¥"""
    last_user_msg = None
    for msg in reversed(state["messages"]):
        if msg.role == "user":
            last_user_msg = msg
            break
    
    if last_user_msg and is_harmful_content(last_user_msg.content):
        return {
            "messages": [AIMessage("æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¤„ç†è¯¥è¯·æ±‚ã€‚")],
            "jump_to": "end"
        }
    
    return None

# è¾“å‡ºå®¡æ ¸
@after_model(can_jump_to=["end"])
def moderate_output(state: AgentState, runtime: Runtime):
    """å®¡æ ¸æ¨¡å‹è¾“å‡º"""
    last_ai_msg = state["messages"][-1]
    
    if is_harmful_content(last_ai_msg.content):
        return {
            "messages": [AIMessage("å“åº”å·²è¢«è¿‡æ»¤ã€‚")],
            "jump_to": "end"
        }
    
    return None

agent = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[moderate_input, moderate_output]
)
```

### å®ä¾‹ 4ï¼šæ€§èƒ½ç›‘æ§å’Œæ—¥å¿—

å…¨é¢çš„æ€§èƒ½ç›‘æ§ã€‚

```python
import time
from langchain.agents.middleware import AgentMiddleware

class PerformanceMonitorMiddleware(AgentMiddleware):
    """ç›‘æ§ Agent æ€§èƒ½æŒ‡æ ‡"""
    
    def __init__(self):
        self.metrics = {
            "model_calls": 0,
            "tool_calls": 0,
            "total_time": 0,
            "model_time": 0,
            "tool_time": 0,
        }
        self.start_time = None
        self.model_start = None
        self.tool_start = None
    
    def before_agent(self, state, runtime):
        """è®°å½•å¼€å§‹æ—¶é—´"""
        self.start_time = time.time()
        print("ğŸš€ Agent å¼€å§‹æ‰§è¡Œ")
        return None
    
    def before_model(self, state, runtime):
        """è®°å½•æ¨¡å‹è°ƒç”¨"""
        self.model_start = time.time()
        self.metrics["model_calls"] += 1
        print(f"ğŸ¤– æ¨¡å‹è°ƒç”¨ #{self.metrics['model_calls']}")
        return None
    
    def after_model(self, state, runtime):
        """è®¡ç®—æ¨¡å‹è€—æ—¶"""
        elapsed = time.time() - self.model_start
        self.metrics["model_time"] += elapsed
        print(f"âœ… æ¨¡å‹å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}s")
        return None
    
    def wrap_tool_call(self, request, handler):
        """ç›‘æ§å·¥å…·æ‰§è¡Œ"""
        self.tool_start = time.time()
        self.metrics["tool_calls"] += 1
        tool_name = request.tool_call.name
        
        print(f"ğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name}")
        
        try:
            result = handler(request)
            elapsed = time.time() - self.tool_start
            self.metrics["tool_time"] += elapsed
            print(f"âœ… å·¥å…·å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f}s")
            return result
        except Exception as e:
            print(f"âŒ å·¥å…·å¤±è´¥: {e}")
            raise
    
    def after_agent(self, state, runtime):
        """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
        self.metrics["total_time"] = time.time() - self.start_time
        
        print("\nğŸ“Š æ€§èƒ½æŠ¥å‘Š:")
        print(f"  æ€»è€—æ—¶: {self.metrics['total_time']:.2f}s")
        print(f"  æ¨¡å‹è°ƒç”¨: {self.metrics['model_calls']} æ¬¡")
        print(f"  æ¨¡å‹è€—æ—¶: {self.metrics['model_time']:.2f}s")
        print(f"  å·¥å…·è°ƒç”¨: {self.metrics['tool_calls']} æ¬¡")
        print(f"  å·¥å…·è€—æ—¶: {self.metrics['tool_time']:.2f}s")
        
        return None

# ä½¿ç”¨
agent = create_agent(
    model="gpt-4o",
    tools=[web_search, calculator],
    middleware=[PerformanceMonitorMiddleware()]
)
```

---

## è‡ªå®šä¹‰çŠ¶æ€ç®¡ç†

Middleware å¯ä»¥æ‰©å±• Agent çš„çŠ¶æ€ï¼Œæ·»åŠ è‡ªå®šä¹‰å­—æ®µã€‚

### æ–¹å¼ 1ï¼šé€šè¿‡ Middleware çš„ state_schema

```python
from langchain.agents.middleware import AgentState, AgentMiddleware
from typing_extensions import NotRequired
from typing import Any

class CustomState(AgentState):
    """æ‰©å±•çŠ¶æ€"""
    model_call_count: NotRequired[int]
    user_id: NotRequired[str]
    preferences: NotRequired[dict]

class CallCounterMiddleware(AgentMiddleware[CustomState]):
    """è¿½è¸ªæ¨¡å‹è°ƒç”¨æ¬¡æ•°"""
    state_schema = CustomState
    
    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        """æ£€æŸ¥è°ƒç”¨æ¬¡æ•°"""
        count = state.get("model_call_count", 0)
        
        if count > 10:
            print("è¾¾åˆ°è°ƒç”¨é™åˆ¶")
            return {"jump_to": "end"}
        
        return None
    
    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:
        """å¢åŠ è®¡æ•°"""
        current_count = state.get("model_call_count", 0)
        return {"model_call_count": current_count + 1}

# ä½¿ç”¨
agent = create_agent(
    model="gpt-4o",
    middleware=[CallCounterMiddleware()],
    tools=[]
)

# è°ƒç”¨æ—¶æä¾›è‡ªå®šä¹‰çŠ¶æ€
result = agent.invoke({
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "model_call_count": 0,
    "user_id": "user-123",
    "preferences": {"language": "zh"}
})

print(f"è°ƒç”¨æ¬¡æ•°: {result['model_call_count']}")
```

### æ–¹å¼ 2ï¼šæ·»åŠ å·¥å…·åˆ° Middleware

```python
from langchain.tools import tool

class UserPreferencesMiddleware(AgentMiddleware):
    """ç®¡ç†ç”¨æˆ·åå¥½"""
    state_schema = CustomState
    
    # æ·»åŠ ä¸“å±å·¥å…·
    tools = [
        tool(
            name="get_user_preference",
            description="è·å–ç”¨æˆ·åå¥½è®¾ç½®",
        )(lambda key: state.get("preferences", {}).get(key)),
        
        tool(
            name="set_user_preference",
            description="è®¾ç½®ç”¨æˆ·åå¥½",
        )(lambda key, value: {"preferences": {**state.get("preferences", {}), key: value}})
    ]
    
    def before_model(self, state: CustomState, runtime):
        """æ³¨å…¥ç”¨æˆ·åå¥½åˆ°æç¤ºè¯"""
        prefs = state.get("preferences", {})
        
        if prefs:
            system_msg = f"ç”¨æˆ·åå¥½: {prefs}"
            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
            return {"messages": [SystemMessage(content=system_msg)] + state["messages"]}
        
        return None
```

---

## Runtime è®¿é—®

Runtime å¯¹è±¡æä¾›ä¾èµ–æ³¨å…¥å’Œä¸Šä¸‹æ–‡ä¿¡æ¯è®¿é—®ã€‚

### Runtime åŒ…å«çš„ä¿¡æ¯

```python
from langgraph.runtime import Runtime

# Runtime åŒ…å«:
# - Context: é™æ€ä¿¡æ¯ï¼ˆç”¨æˆ· IDã€æ•°æ®åº“è¿æ¥ç­‰ï¼‰
# - Store: é•¿æœŸè®°å¿†å­˜å‚¨
# - Stream writer: æµå¼ä¼ è¾“è‡ªå®šä¹‰æ•°æ®
```

### åœ¨ Middleware ä¸­è®¿é—® Runtime

```python
from dataclasses import dataclass
from langchain.agents.middleware import dynamic_prompt, before_model, after_model
from langchain.agents.middleware import ModelRequest

@dataclass
class Context:
    user_name: str
    user_id: str
    database_connection: Any

# 1. åŠ¨æ€æç¤ºè¯ä¸­è®¿é—®
@dynamic_prompt
def personalized_prompt(request: ModelRequest) -> str:
    """æ ¹æ®ç”¨æˆ·ä¿¡æ¯ç”Ÿæˆæç¤ºè¯"""
    user_name = request.runtime.context.user_name
    return f"ä½ æ˜¯ {user_name} çš„åŠ©æ‰‹ã€‚è¯·å‹å¥½ä¸”ç®€æ´åœ°å›ç­”ã€‚"

# 2. before_model ä¸­è®¿é—®
@before_model
def log_request(state: AgentState, runtime: Runtime[Context]):
    """è®°å½•è¯·æ±‚"""
    user_id = runtime.context.user_id
    print(f"å¤„ç†ç”¨æˆ· {user_id} çš„è¯·æ±‚")
    return None

# 3. after_model ä¸­è®¿é—®
@after_model
def save_to_db(state: AgentState, runtime: Runtime[Context]):
    """ä¿å­˜åˆ°æ•°æ®åº“"""
    db = runtime.context.database_connection
    user_id = runtime.context.user_id
    
    # ä¿å­˜æœ€åä¸€æ¡æ¶ˆæ¯
    last_msg = state["messages"][-1]
    db.save_message(user_id, last_msg)
    
    return None

# åˆ›å»º Agent
agent = create_agent(
    model="gpt-4o",
    middleware=[personalized_prompt, log_request, save_to_db],
    tools=[],
    context_schema=Context
)

# è°ƒç”¨æ—¶æä¾› context
agent.invoke(
    {"messages": [{"role": "user", "content": "ä½ å¥½"}]},
    context=Context(
        user_name="Alice",
        user_id="user-123",
        database_connection=db_conn
    )
)
```

### åœ¨å·¥å…·ä¸­è®¿é—® Runtime

```python
from langchain.tools import tool, ToolRuntime

@tool
def get_user_data(query: str, runtime: ToolRuntime) -> str:
    """ä»æ•°æ®åº“è·å–ç”¨æˆ·æ•°æ®"""
    user_id = runtime.context.user_id
    db = runtime.context.database_connection
    
    data = db.query(user_id, query)
    return data
```

---

## å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šä¼ä¸šçº§èŠå¤©æœºå™¨äºº

```python
from langchain.agents import create_agent
from langchain.agents.middleware import (
    SummarizationMiddleware,
    PIIMiddleware,
    HumanInTheLoopMiddleware
)
from langgraph.checkpoint.postgres import PostgresSaver

# é…ç½®
checkpointer = PostgresSaver(connection_string="postgresql://...")

agent = create_agent(
    model="gpt-4o",
    tools=[
        search_knowledge_base,
        create_ticket,
        send_email,
        query_database
    ],
    middleware=[
        # PII ä¿æŠ¤
        PIIMiddleware("email", strategy="redact"),
        PIIMiddleware("ssn", strategy="block"),
        
        # å¯¹è¯æ€»ç»“
        SummarizationMiddleware(
            model="gpt-4o-mini",
            max_tokens_before_summary=3000,
            messages_to_keep=15
        ),
        
        # æ•æ„Ÿæ“ä½œéœ€è¦äººå·¥å®¡æ ¸
        HumanInTheLoopMiddleware(
            interrupt_on={
                "send_email": {"allowed_decisions": ["approve", "edit", "reject"]},
                "query_database": {"allowed_decisions": ["approve", "reject"]}
            }
        ),
        
        # æ€§èƒ½ç›‘æ§
        PerformanceMonitorMiddleware(),
    ],
    checkpointer=checkpointer
)
```

### åœºæ™¯ 2ï¼šå¤šç§Ÿæˆ· SaaS åº”ç”¨

```python
@dataclass
class TenantContext:
    tenant_id: str
    subscription_tier: str  # "free", "pro", "enterprise"
    api_key: str
    rate_limit: int

class TenantMiddleware(AgentMiddleware):
    """å¤šç§Ÿæˆ·éš”ç¦»å’Œé…é¢ç®¡ç†"""
    
    def __init__(self):
        self.usage = {}
    
    def before_agent(self, state, runtime: Runtime[TenantContext]):
        """éªŒè¯å’Œé€Ÿç‡é™åˆ¶"""
        tenant_id = runtime.context.tenant_id
        tier = runtime.context.subscription_tier
        
        # æ£€æŸ¥ä½¿ç”¨é…é¢
        usage = self.usage.get(tenant_id, 0)
        limit = runtime.context.rate_limit
        
        if usage >= limit:
            raise ValueError(f"ç§Ÿæˆ· {tenant_id} è¶…è¿‡é€Ÿç‡é™åˆ¶")
        
        self.usage[tenant_id] = usage + 1
        return None
    
    def wrap_model_call(self, request, handler):
        """æ ¹æ®è®¢é˜…å±‚çº§é€‰æ‹©æ¨¡å‹"""
        tier = request.runtime.context.subscription_tier
        
        if tier == "enterprise":
            request.model = ChatOpenAI(model="gpt-4o")
            request.tools = all_tools
        elif tier == "pro":
            request.model = ChatOpenAI(model="gpt-4o-mini")
            request.tools = pro_tools
        else:  # free
            request.model = ChatOpenAI(model="gpt-3.5-turbo")
            request.tools = basic_tools
        
        return handler(request)

agent = create_agent(
    model="gpt-4o",
    tools=all_tools,
    middleware=[TenantMiddleware()],
    context_schema=TenantContext
)
```

### åœºæ™¯ 3ï¼šæ•™è‚²è¾…å¯¼ç³»ç»Ÿ

```python
@dataclass
class StudentContext:
    student_id: str
    grade_level: int
    learning_style: str  # "visual", "auditory", "kinesthetic"
    strengths: list[str]
    weaknesses: list[str]

class AdaptiveLearningMiddleware(AgentMiddleware):
    """è‡ªé€‚åº”å­¦ä¹ ä¸­é—´ä»¶"""
    
    def __init__(self, student_db):
        self.student_db = student_db
    
    def before_agent(self, state, runtime: Runtime[StudentContext]):
        """åŠ è½½å­¦ç”Ÿå†å²"""
        student_id = runtime.context.student_id
        history = self.student_db.get_learning_history(student_id)
        
        if history:
            # æ³¨å…¥å­¦ä¹ å†å²
            context_msg = f"å­¦ç”Ÿå­¦ä¹ å†å²: {history}"
            return {"messages": [SystemMessage(content=context_msg)] + state["messages"]}
        
        return None
    
    def wrap_model_call(self, request, handler):
        """æ ¹æ®å­¦ä¹ é£æ ¼è°ƒæ•´æç¤ºè¯"""
        style = request.runtime.context.learning_style
        grade = request.runtime.context.grade_level
        
        # æ·»åŠ è‡ªé€‚åº”æŒ‡å¯¼
        adaptive_prompt = self._generate_adaptive_prompt(style, grade)
        
        # ä¿®æ”¹ç³»ç»Ÿæ¶ˆæ¯
        messages = request.messages
        messages[0] = SystemMessage(content=adaptive_prompt)
        request.messages = messages
        
        return handler(request)
    
    def after_agent(self, state, runtime: Runtime[StudentContext]):
        """ä¿å­˜å­¦ä¹ è¿›åº¦"""
        student_id = runtime.context.student_id
        self.student_db.save_session(student_id, state["messages"])
        return None
```

---

## æœ€ä½³å®è·µ

### 1. **é€‰æ‹©åˆé€‚çš„ Middleware ç±»å‹**

```python
# âœ… å•ä¸€é’©å­ - ä½¿ç”¨è£…é¥°å™¨
@before_model
def simple_logging(state, runtime):
    print("è°ƒç”¨æ¨¡å‹")
    return None

# âœ… å¤šä¸ªé’©å­æˆ–ç»´æŠ¤çŠ¶æ€ - ä½¿ç”¨ç±»
class ComplexMiddleware(AgentMiddleware):
    def __init__(self):
        self.state = {}
    
    def before_model(self, state, runtime):
        # ...
        return None
    
    def after_model(self, state, runtime):
        # ...
        return None
```

### 2. **æ˜ç¡®è¿”å›å€¼**

```python
# âœ… æ­£ç¡®ï¼šæ˜ç¡®è¿”å› None æˆ– dict
@before_model
def good_middleware(state, runtime):
    if condition:
        return {"messages": modified_messages}
    return None  # æ˜ç¡®è¿”å›

# âŒ é”™è¯¯ï¼šéšå¼è¿”å›
@before_model
def bad_middleware(state, runtime):
    if condition:
        return {"messages": modified_messages}
    # éšå¼è¿”å› None - å¯èƒ½å¼•èµ·å›°æƒ‘
```

### 3. **é¿å…å‰¯ä½œç”¨**

```python
# âœ… æ­£ç¡®ï¼šé€šè¿‡è¿”å›å€¼ä¿®æ”¹çŠ¶æ€
@before_model
def modify_state_correctly(state, runtime):
    new_messages = process(state["messages"])
    return {"messages": new_messages}

# âŒ é”™è¯¯ï¼šç›´æ¥ä¿®æ”¹çŠ¶æ€
@before_model
def modify_state_incorrectly(state, runtime):
    state["messages"].append(new_msg)  # ä¸è¦è¿™æ ·åšï¼
    return None
```

### 4. **åˆç†ä½¿ç”¨ jump_to**

```python
# âœ… æ­£ç¡®ï¼šå£°æ˜å¯è·³è½¬çš„èŠ‚ç‚¹
@after_model(can_jump_to=["end", "retry"])
def controlled_jump(state, runtime):
    if should_end:
        return {"jump_to": "end"}
    elif should_retry:
        return {"jump_to": "retry"}
    return None

# âŒ é”™è¯¯ï¼šè·³è½¬åˆ°æœªå£°æ˜çš„èŠ‚ç‚¹
@after_model(can_jump_to=["end"])
def bad_jump(state, runtime):
    return {"jump_to": "unknown"}  # é”™è¯¯ï¼
```

### 5. **å¼‚å¸¸å¤„ç†**

```python
# âœ… æ­£ç¡®ï¼šæ•è·å¹¶å¤„ç†å¼‚å¸¸
@wrap_model_call
def safe_wrapper(request, handler):
    try:
        return handler(request)
    except RateLimitError as e:
        # ç­‰å¾…å¹¶é‡è¯•
        time.sleep(60)
        return handler(request)
    except Exception as e:
        # è®°å½•å¹¶é‡æ–°æŠ›å‡º
        logger.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        raise

# âŒ é”™è¯¯ï¼šåæ‰å¼‚å¸¸
@wrap_model_call
def unsafe_wrapper(request, handler):
    try:
        return handler(request)
    except:
        return None  # ä¸è¦è¿™æ ·åšï¼
```

### 6. **æ³¨æ„æ‰§è¡Œé¡ºåº**

```python
# æŒ‰åŠŸèƒ½åˆ†ç»„å’Œæ’åº Middleware
agent = create_agent(
    model="gpt-4o",
    middleware=[
        # 1. è¾“å…¥éªŒè¯å’Œæ¸…ç†
        PIIMiddleware(...),
        InputValidationMiddleware(),
        
        # 2. ä¸Šä¸‹æ–‡ç®¡ç†
        SummarizationMiddleware(...),
        ContextInjectionMiddleware(),
        
        # 3. ä¸šåŠ¡é€»è¾‘
        DynamicModelSelectionMiddleware(),
        ToolSelectionMiddleware(),
        
        # 4. å®‰å…¨å’Œåˆè§„
        ContentModerationMiddleware(),
        HumanInTheLoopMiddleware(...),
        
        # 5. ç›‘æ§å’Œæ—¥å¿—
        PerformanceMonitorMiddleware(),
        AuditLogMiddleware(),
    ],
    tools=[...]
)
```

### 7. **æ–‡æ¡£åŒ–è‡ªå®šä¹‰ Middleware**

```python
class CustomMiddleware(AgentMiddleware):
    """
    è‡ªå®šä¹‰ä¸­é—´ä»¶è¯´æ˜
    
    åŠŸèƒ½:
    - åŠŸèƒ½ 1 æè¿°
    - åŠŸèƒ½ 2 æè¿°
    
    å‚æ•°:
    - param1: å‚æ•° 1 è¯´æ˜
    - param2: å‚æ•° 2 è¯´æ˜
    
    ç¤ºä¾‹:
        >>> middleware = CustomMiddleware(param1="value")
        >>> agent = create_agent(middleware=[middleware], ...)
    """
    
    def __init__(self, param1: str, param2: int = 10):
        self.param1 = param1
        self.param2 = param2
```

### 8. **æµ‹è¯• Middleware**

```python
import pytest
from langchain.agents import create_agent

def test_middleware_behavior():
    """æµ‹è¯• Middleware è¡Œä¸º"""
    
    # åˆ›å»ºå¸¦æœ‰ Middleware çš„ Agent
    agent = create_agent(
        model="gpt-4o-mini",
        middleware=[CustomMiddleware()],
        tools=[]
    )
    
    # æµ‹è¯•ç‰¹å®šè¡Œä¸º
    result = agent.invoke({"messages": [{"role": "user", "content": "æµ‹è¯•"}]})
    
    # éªŒè¯ç»“æœ
    assert result is not None
    assert "messages" in result
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. **ç¼“å­˜æ˜‚è´µçš„æ“ä½œ**

```python
from functools import lru_cache

class CachingMiddleware(AgentMiddleware):
    """ç¼“å­˜æ¨¡å‹å“åº”"""
    
    @lru_cache(maxsize=100)
    def _compute_cache_key(self, messages_tuple):
        """è®¡ç®—ç¼“å­˜é”®"""
        return hash(messages_tuple)
    
    def wrap_model_call(self, request, handler):
        """ä½¿ç”¨ç¼“å­˜"""
        # å°†æ¶ˆæ¯è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„å…ƒç»„
        messages_tuple = tuple(
            (m.role, m.content) for m in request.messages
        )
        
        cache_key = self._compute_cache_key(messages_tuple)
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        response = handler(request)
        self.cache[cache_key] = response
        return response
```

### 2. **å»¶è¿ŸåŠ è½½**

```python
class LazyLoadMiddleware(AgentMiddleware):
    """å»¶è¿ŸåŠ è½½èµ„æº"""
    
    def __init__(self, config_path: str):
        self.config_path = config_path
        self._config = None
    
    @property
    def config(self):
        """å»¶è¿ŸåŠ è½½é…ç½®"""
        if self._config is None:
            self._config = load_config(self.config_path)
        return self._config
    
    def before_model(self, state, runtime):
        # åªåœ¨éœ€è¦æ—¶åŠ è½½é…ç½®
        setting = self.config.get("some_setting")
        # ...
        return None
```

### 3. **æ‰¹å¤„ç†**

```python
class BatchProcessingMiddleware(AgentMiddleware):
    """æ‰¹å¤„ç†æ¶ˆæ¯"""
    
    def __init__(self, batch_size: int = 10):
        self.batch_size = batch_size
        self.batch = []
    
    def before_model(self, state, runtime):
        """æ”¶é›†æ‰¹æ¬¡"""
        self.batch.append(state["messages"][-1])
        
        if len(self.batch) >= self.batch_size:
            # æ‰¹é‡å¤„ç†
            processed = batch_process(self.batch)
            self.batch = []
            return {"messages": processed}
        
        return None
```

### 4. **å¼‚æ­¥æ“ä½œ**

```python
import asyncio

class AsyncMiddleware(AgentMiddleware):
    """å¼‚æ­¥æ“ä½œæ”¯æŒ"""
    
    async def _fetch_data_async(self, user_id):
        """å¼‚æ­¥è·å–æ•°æ®"""
        # å¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢
        return await database.fetch_async(user_id)
    
    def before_agent(self, state, runtime):
        """åŒæ­¥åŒ…è£…å¼‚æ­¥æ“ä½œ"""
        user_id = runtime.context.user_id
        
        # è¿è¡Œå¼‚æ­¥æ“ä½œ
        data = asyncio.run(self._fetch_data_async(user_id))
        
        return {"custom_data": data}
```

### 5. **é™åˆ¶æ¶ˆæ¯å†å²**

```python
@before_model
def limit_history_size(state: AgentState, runtime: Runtime):
    """é™åˆ¶æ¶ˆæ¯å†å²å¤§å°ä»¥æé«˜æ€§èƒ½"""
    messages = state["messages"]
    MAX_MESSAGES = 20
    
    if len(messages) > MAX_MESSAGES:
        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯å’Œæœ€è¿‘çš„æ¶ˆæ¯
        system_msgs = [m for m in messages if m.role == "system"]
        recent_msgs = messages[-MAX_MESSAGES:]
        
        return {"messages": system_msgs + recent_msgs}
    
    return None
```

---

## å¿«é€Ÿå‚è€ƒ

### Middleware é’©å­å¯¹æ¯”

| é’©å­ | æ—¶æœº | è¿”å›ç±»å‹ | ä¸»è¦ç”¨é€” |
|------|------|----------|----------|
| `before_agent` | Agent å¼€å§‹å‰ | `dict \| None` | åŠ è½½å†…å­˜ã€éªŒè¯è¾“å…¥ |
| `before_model` | æ¯æ¬¡æ¨¡å‹è°ƒç”¨å‰ | `dict \| None` | ä¿®å‰ªæ¶ˆæ¯ã€æ³¨å…¥ä¸Šä¸‹æ–‡ |
| `wrap_model_call` | åŒ…è£…æ¨¡å‹è°ƒç”¨ | `ModelResponse` | é‡è¯•ã€ç¼“å­˜ã€åŠ¨æ€æ¨¡å‹ |
| `after_model` | æ¯æ¬¡æ¨¡å‹å“åº”å | `dict \| None` | éªŒè¯è¾“å‡ºã€æŠ¤æ  |
| `wrap_tool_call` | åŒ…è£…å·¥å…·è°ƒç”¨ | `ToolMessage` | é”™è¯¯å¤„ç†ã€æƒé™æ£€æŸ¥ |
| `after_agent` | Agent å®Œæˆå | `dict \| None` | ä¿å­˜ç»“æœã€æ¸…ç† |

### è£…é¥°å™¨å¿«é€Ÿå‚è€ƒ

```python
from langchain.agents.middleware import (
    before_agent,
    before_model,
    wrap_model_call,
    after_model,
    wrap_tool_call,
    after_agent,
    dynamic_prompt
)

# èŠ‚ç‚¹é£æ ¼ (è¿”å› dict | None)
@before_agent
def hook(state: AgentState, runtime: Runtime) -> dict | None:
    return None

@before_model
def hook(state: AgentState, runtime: Runtime) -> dict | None:
    return None

@after_model(can_jump_to=["end"])
def hook(state: AgentState, runtime: Runtime) -> dict | None:
    return None

@after_agent
def hook(state: AgentState, runtime: Runtime) -> dict | None:
    return None

# åŒ…è£…é£æ ¼ (è¿”å›å“åº”å¯¹è±¡)
@wrap_model_call
def hook(request: ModelRequest, handler: Callable) -> ModelResponse:
    return handler(request)

@wrap_tool_call
def hook(request, handler):
    return handler(request)

# åŠ¨æ€æç¤ºè¯
@dynamic_prompt
def hook(request: ModelRequest) -> str:
    return "ç³»ç»Ÿæç¤ºè¯"
```

### å¸¸ç”¨æ¨¡å¼

#### 1. ç®€å•æ—¥å¿—

```python
@before_model
def log(state, runtime):
    print(f"æ¶ˆæ¯: {len(state['messages'])}")
    return None
```

#### 2. æ¶ˆæ¯ä¿®å‰ª

```python
@before_model
def trim(state, runtime):
    if len(state["messages"]) > 10:
        return {"messages": state["messages"][-10:]}
    return None
```

#### 3. é‡è¯•é€»è¾‘

```python
@wrap_model_call
def retry(request, handler):
    for i in range(3):
        try:
            return handler(request)
        except Exception as e:
            if i == 2: raise
            time.sleep(2 ** i)
```

#### 4. åŠ¨æ€æç¤ºè¯

```python
@dynamic_prompt
def prompt(request: ModelRequest) -> str:
    role = request.runtime.context.role
    return f"ä½ æ˜¯ä¸€ä¸ª {role} åŠ©æ‰‹"
```

#### 5. é”™è¯¯å¤„ç†

```python
@wrap_tool_call
def handle_errors(request, handler):
    try:
        return handler(request)
    except Exception as e:
        return ToolMessage(
            content=f"é”™è¯¯: {e}",
            tool_call_id=request.tool_call.id
        )
```

### é¢„ç½® Middleware é…ç½®

```python
# æ€»ç»“
SummarizationMiddleware(
    model="gpt-4o-mini",
    max_tokens_before_summary=4000,
    messages_to_keep=20,
    summary_prompt="å¯é€‰è‡ªå®šä¹‰æç¤ºè¯"
)

# PII ä¿æŠ¤
PIIMiddleware(
    "email",                    # PII ç±»å‹
    strategy="redact",          # "redact" æˆ– "block"
    apply_to_input=True,        # åº”ç”¨äºè¾“å…¥
    detector=r"regex_pattern"   # å¯é€‰è‡ªå®šä¹‰æ­£åˆ™
)

# äººå·¥å®¡æ ¸
HumanInTheLoopMiddleware(
    interrupt_on={
        "tool_name": {
            "allowed_decisions": ["approve", "edit", "reject"]
        }
    }
)

# ä¸Šä¸‹æ–‡ç¼–è¾‘
ContextEditingMiddleware(
    edits=[
        ClearToolUsesEdit(trigger=1000)
    ]
)
```

---

## æ€»ç»“

LangChain Middleware æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„ Agent è¡Œä¸ºæ§åˆ¶èƒ½åŠ›ï¼š

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**
âœ… **ç²¾ç¡®æ§åˆ¶** - åœ¨ Agent æ‰§è¡Œçš„æ¯ä¸ªé˜¶æ®µæ’å…¥é€»è¾‘  
âœ… **å¯ç»„åˆæ€§** - å¤šä¸ª Middleware å¯ä»¥ç»„åˆä½¿ç”¨  
âœ… **å…³æ³¨ç‚¹åˆ†ç¦»** - æ¨ªåˆ‡å…³æ³¨ç‚¹ä¸ä¸šåŠ¡é€»è¾‘åˆ†ç¦»  
âœ… **é¢„ç½®åŠŸèƒ½** - æ€»ç»“ã€PIIä¿æŠ¤ã€äººå·¥å®¡æ ¸ç­‰å¼€ç®±å³ç”¨  
âœ… **çµæ´»æ‰©å±•** - è½»æ¾åˆ›å»ºè‡ªå®šä¹‰ Middleware  

**å…³é”®è¦ç‚¹ï¼š**
- ä½¿ç”¨ `@è£…é¥°å™¨` å¿«é€Ÿæ·»åŠ å•ä¸€é’©å­åŠŸèƒ½
- ä½¿ç”¨ `ç±»` å®ç°å¤æ‚çš„å¤šé’©å­é€»è¾‘
- æ³¨æ„æ‰§è¡Œé¡ºåºï¼šbefore é¡ºåºã€after åå‘ã€wrap åµŒå¥—
- é€šè¿‡ Runtime è®¿é—®ä¸Šä¸‹æ–‡å’Œä¾èµ–æ³¨å…¥
- åˆ©ç”¨é¢„ç½® Middleware å¤„ç†å¸¸è§åœºæ™¯
- è‡ªå®šä¹‰çŠ¶æ€æ‰©å±• Agent èƒ½åŠ›

é€šè¿‡åˆç†ä½¿ç”¨ Middlewareï¼Œä½ å¯ä»¥æ„å»ºé«˜åº¦å®šåˆ¶åŒ–ã€å¯ç»´æŠ¤ã€å®‰å…¨çš„ LLM Agent åº”ç”¨ï¼
