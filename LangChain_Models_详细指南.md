# LangChain Models è¯¦ç»†æŒ‡å—

> åŸºäºå®˜æ–¹æ–‡æ¡£ https://docs.langchain.com/oss/python/langchain/models çš„å®Œæ•´ä¸­æ–‡æ€»ç»“

---

## ğŸ“‹ ç›®å½•

- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [Chat Models vs LLMs](#chat-models-vs-llms)
- [æ¨¡å‹åˆå§‹åŒ–](#æ¨¡å‹åˆå§‹åŒ–)
- [æ”¯æŒçš„æ¨¡å‹æä¾›å•†](#æ”¯æŒçš„æ¨¡å‹æä¾›å•†)
- [æ¨¡å‹å‚æ•°é…ç½®](#æ¨¡å‹å‚æ•°é…ç½®)
- [å·¥å…·è°ƒç”¨ (Tool Calling)](#å·¥å…·è°ƒç”¨-tool-calling)
- [ç»“æ„åŒ–è¾“å‡º](#ç»“æ„åŒ–è¾“å‡º)
- [æµå¼å¤„ç†](#æµå¼å¤„ç†)
- [å¤šæ¨¡æ€æ”¯æŒ](#å¤šæ¨¡æ€æ”¯æŒ)
- [Token ä½¿ç”¨å’Œå…ƒæ•°æ®](#token-ä½¿ç”¨å’Œå…ƒæ•°æ®)
- [é”™è¯¯å¤„ç†å’Œé‡è¯•](#é”™è¯¯å¤„ç†å’Œé‡è¯•)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)

---

## æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯ Modelsï¼Ÿ

åœ¨ LangChain ä¸­ï¼Œ**Modelsï¼ˆæ¨¡å‹ï¼‰** æ˜¯ Agent çš„æ¨ç†å¼•æ“ã€‚å®ƒä»¬è´Ÿè´£ç†è§£ç”¨æˆ·è¾“å…¥ã€ç”Ÿæˆå“åº”ã€å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ä»¥åŠå¦‚ä½•ä½¿ç”¨å·¥å…·è¿”å›çš„ç»“æœã€‚

### Models çš„æ ¸å¿ƒèƒ½åŠ›

1. **æ¨ç† (Reasoning)**: ç†è§£å¤æ‚é—®é¢˜å¹¶ç”Ÿæˆåˆç†çš„å›ç­”
2. **å·¥å…·è°ƒç”¨ (Tool Calling)**: å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨å¤–éƒ¨å·¥å…·
3. **ç»“æ„åŒ–è¾“å‡º (Structured Output)**: æŒ‰ç…§é¢„å®šä¹‰çš„ schema ç”Ÿæˆæ ¼å¼åŒ–æ•°æ®
4. **å¤šæ¨¡æ€ (Multimodality)**: å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§è¾“å…¥ç±»å‹
5. **æµå¼å¤„ç† (Streaming)**: å®æ—¶ç”Ÿæˆå’Œä¼ è¾“å“åº”

### Models åœ¨ Agent ä¸­çš„è§’è‰²

```
ç”¨æˆ·è¾“å…¥ â†’ Model (æ¨ç†) â†’ å†³ç­– â†’ è°ƒç”¨å·¥å…· â†’ Model (ç»¼åˆ) â†’ æœ€ç»ˆå“åº”
```

Models æ˜¯æ•´ä¸ª Agent ç³»ç»Ÿçš„å¤§è„‘ï¼Œè´Ÿè´£ï¼š
- ç†è§£ç”¨æˆ·æ„å›¾
- åˆ¶å®šæ‰§è¡Œè®¡åˆ’
- é€‰æ‹©åˆé€‚çš„å·¥å…·
- ç»¼åˆä¿¡æ¯ç”Ÿæˆå›ç­”

---

## Chat Models vs LLMs

### Chat Models

**Chat Models** æ˜¯ä¸“é—¨ä¸ºå¯¹è¯åœºæ™¯è®¾è®¡çš„æ¨¡å‹ï¼Œæ¥å—æ¶ˆæ¯åˆ—è¡¨ä½œä¸ºè¾“å…¥å¹¶è¿”å›æ¶ˆæ¯ã€‚

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage

# åˆå§‹åŒ– Chat Model
model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨è°ƒç”¨
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—?")
]

response = model.invoke(messages)
print(response.content)
```

**ç‰¹ç‚¹**:
- æ¥å— `List[Message]` ä½œä¸ºè¾“å…¥
- è¿”å› `Message` å¯¹è±¡
- æ”¯æŒç³»ç»Ÿæ¶ˆæ¯ã€ç”¨æˆ·æ¶ˆæ¯ã€AI æ¶ˆæ¯ç­‰å¤šç§æ¶ˆæ¯ç±»å‹
- åŸç”Ÿæ”¯æŒå·¥å…·è°ƒç”¨
- æ›´å¥½çš„å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†

### LLMs (ä¼ ç»Ÿè¯­è¨€æ¨¡å‹)

**LLMs** æ˜¯æ›´é€šç”¨çš„è¯­è¨€æ¨¡å‹ï¼Œæ¥å—å­—ç¬¦ä¸²è¾“å…¥å¹¶è¿”å›å­—ç¬¦ä¸²ã€‚

```python
from langchain_openai import OpenAI

# åˆå§‹åŒ– LLM
llm = OpenAI(model="gpt-3.5-turbo-instruct")

# ä½¿ç”¨å­—ç¬¦ä¸²è°ƒç”¨
response = llm.invoke("ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—?")
print(response)
```

**ç‰¹ç‚¹**:
- æ¥å— `str` ä½œä¸ºè¾“å…¥
- è¿”å› `str` 
- æ›´ç®€å•çš„æ¥å£
- é€‚åˆå•è½®æ–‡æœ¬ç”Ÿæˆ

### é€‰æ‹©å»ºè®®

**ä¼˜å…ˆä½¿ç”¨ Chat Models**ï¼Œç°ä»£åº”ç”¨æ¨èä½¿ç”¨ Chat Modelsï¼Œå› ä¸ºå®ƒä»¬ï¼š
- âœ… æ”¯æŒæ›´ä¸°å¯Œçš„å¯¹è¯ä¸Šä¸‹æ–‡
- âœ… åŸç”Ÿæ”¯æŒå·¥å…·è°ƒç”¨
- âœ… æ›´å¥½çš„ç³»ç»Ÿæç¤ºè¯æ§åˆ¶
- âœ… æ›´é€‚åˆ Agent åº”ç”¨
- âœ… æ”¯æŒå¤šæ¨¡æ€è¾“å…¥

---

## æ¨¡å‹åˆå§‹åŒ–

### ä½¿ç”¨ init_chat_model (æ¨è)

`init_chat_model` æ˜¯ LangChain æ¨èçš„ç»Ÿä¸€æ¨¡å‹åˆå§‹åŒ–æ–¹å¼ï¼Œæ”¯æŒå¤šä¸ªæä¾›å•†ã€‚

```python
from langchain.chat_models import init_chat_model

# Anthropic Claude
model = init_chat_model(
    model="claude-3-5-sonnet-20241022",
    model_provider="anthropic",
    temperature=0.7
)

# OpenAI GPT
openai_model = init_chat_model(
    model="gpt-4o",
    model_provider="openai",
    temperature=0.5,
    max_tokens=1000
)

# Google Gemini
google_model = init_chat_model(
    model="gemini-2.0-flash-exp",
    model_provider="google_genai",
    temperature=0
)
```

**ä¼˜åŠ¿**:
- ç»Ÿä¸€çš„ API æ¥å£
- è½»æ¾åˆ‡æ¢ä¸åŒæä¾›å•†
- ç®€åŒ–é…ç½®ç®¡ç†

### ç›´æ¥ä½¿ç”¨æä¾›å•†ç±»

æ¯ä¸ªæä¾›å•†éƒ½æœ‰è‡ªå·±çš„ç±»ï¼Œå¯ä»¥ç›´æ¥å¯¼å…¥ä½¿ç”¨ã€‚

#### Anthropic Claude
```bash
pip install -U "langchain[anthropic]"
```

```python
from langchain_community.chat_models import ChatAnthropic
# from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    api_key="your-api-key",  # æˆ–ä»ç¯å¢ƒå˜é‡è¯»å–
    temperature=0.7,
    max_tokens=1024
)
```

#### OpenAI GPT
```bash
pip install -U "langchain[openai]"
```

```python
from langchain_community.chat_models import ChatOpenAI
# from langchain_openai import ChatOpenAI

model = ChatOpenAI(
    model="gpt-4o",
    api_key="your-api-key",
    temperature=0.5
)
```

#### Google Gemini
```bash
pip install -U "langchain[google-genai]"
```

```python
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    google_api_key="your-api-key"
)
```

#### Azure OpenAI
```bash
pip install -U "langchain[openai]"
```

```python
from langchain_openai import AzureChatOpenAI

model = AzureChatOpenAI(
    azure_endpoint="https://your-endpoint.openai.azure.com/",
    azure_deployment="your-deployment-name",
    api_version="2024-02-15-preview"
)
```

#### AWS Bedrock
```bash
pip install -U "langchain[aws]"
```

```python
from langchain_aws import ChatBedrock

model = ChatBedrock(
    model_id="anthropic.claude-3-sonnet-20240229-v1:0",
    region_name="us-east-1"
)
```

### ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®

```python
import os
from langchain_anthropic import ChatAnthropic

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["ANTHROPIC_API_KEY"] = "your-api-key"

# api_key ä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–
model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022"
)
```

**æ”¯æŒçš„ç¯å¢ƒå˜é‡**:
- `ANTHROPIC_API_KEY` - Anthropic Claude
- `OPENAI_API_KEY` - OpenAI GPT
- `GOOGLE_API_KEY` - Google Gemini
- `AZURE_OPENAI_API_KEY` - Azure OpenAI
- `AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY` - AWS Bedrock

---

## æ”¯æŒçš„æ¨¡å‹æä¾›å•†(ä¹‹å‰çš„ç‰ˆæœ¬)

### Anthropic Claude

```python
from langchain_anthropic import ChatAnthropic

# Claude 3.5 Sonnet (æ¨èç”¨äº Agents)
model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# Claude 3 Opus (æœ€å¼ºå¤§)
model = ChatAnthropic(model="claude-3-opus-20240229")

# Claude 3 Haiku (æœ€å¿«é€Ÿ)
model = ChatAnthropic(model="claude-3-haiku-20240307")
```

**ç‰¹ç‚¹**:
- âœ¨ å‡ºè‰²çš„æ¨ç†èƒ½åŠ›
- ğŸ› ï¸ åŸç”Ÿå·¥å…·è°ƒç”¨æ”¯æŒ
- ğŸ“š é•¿ä¸Šä¸‹æ–‡çª—å£ (200K tokens)
- ğŸ¯ å¼ºå¤§çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›
- ğŸ’¡ é€‚åˆå¤æ‚çš„ Agent ä»»åŠ¡

**æœ€ä½³ç”¨é€”**: Agent åº”ç”¨ã€å¤æ‚æ¨ç†ã€å·¥å…·è°ƒç”¨

### OpenAI GPT

```python
from langchain_openai import ChatOpenAI

# GPT-4o (å¤šæ¨¡æ€)
model = ChatOpenAI(model="gpt-4o")

# GPT-4 Turbo
model = ChatOpenAI(model="gpt-4-turbo")

# GPT-3.5 Turbo (ç»æµå‹)
model = ChatOpenAI(model="gpt-3.5-turbo")
```

**ç‰¹ç‚¹**:
- ğŸŒ å¹¿æ³›é‡‡ç”¨å’Œæˆç†Ÿ
- ğŸ”§ å¼ºå¤§çš„å·¥å…·è°ƒç”¨
- ğŸ–¼ï¸ å¤šæ¨¡æ€æ”¯æŒ (GPT-4o)
- ğŸŒ ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿ

**æœ€ä½³ç”¨é€”**: é€šç”¨ä»»åŠ¡ã€å¤šæ¨¡æ€åº”ç”¨ã€æˆç†Ÿçš„ç”Ÿäº§ç¯å¢ƒ

### Google Gemini

```python
from langchain_google_genai import ChatGoogleGenerativeAI

# Gemini 2.0 Flash
model = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp")

# Gemini 1.5 Pro
model = ChatGoogleGenerativeAI(model="gemini-1.5-pro")
```

**ç‰¹ç‚¹**:
- ğŸ“– æé•¿ä¸Šä¸‹æ–‡ (1M+ tokens)
- ğŸ¨ å¤šæ¨¡æ€èƒ½åŠ›
- âš¡ å¿«é€Ÿå“åº”
- ğŸ’° æˆæœ¬æ•ˆç›Šé«˜

**æœ€ä½³ç”¨é€”**: é•¿æ–‡æ¡£åˆ†æã€è¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€ç»æµå‹åº”ç”¨

### Azure OpenAI

```python
from langchain_openai import AzureChatOpenAI

model = AzureChatOpenAI(
    azure_endpoint="https://your-endpoint.openai.azure.com/",
    azure_deployment="your-gpt4-deployment",
    api_version="2024-02-15-preview",
    temperature=0.7
)
```

**ç‰¹ç‚¹**:
- ğŸ”’ ä¼ä¸šçº§å®‰å…¨å’Œåˆè§„
- â˜ï¸ ä¸ Azure ç”Ÿæ€é›†æˆ
- ğŸ›¡ï¸ æ•°æ®éšç§ä¿è¯
- ğŸ“ SLA æ”¯æŒ

**æœ€ä½³ç”¨é€”**: ä¼ä¸šåº”ç”¨ã€åˆè§„è¦æ±‚é«˜çš„åœºæ™¯

### AWS Bedrock

```python
from langchain_aws import ChatBedrock

# Claude 3 on Bedrock
model = ChatBedrock(
    model_id="anthropic.claude-3-sonnet-20240229-v1:0",
    region_name="us-east-1"
)

# Llama 2 on Bedrock
model = ChatBedrock(
    model_id="meta.llama2-70b-chat-v1",
    region_name="us-west-2"
)
```

**ç‰¹ç‚¹**:
- ğŸ¯ å¤šç§æ¨¡å‹é€‰æ‹©
- ğŸŒ AWS åŸºç¡€è®¾æ–½é›†æˆ
- ğŸ¢ ä¼ä¸šçº§éƒ¨ç½²
- ğŸ’³ æŒ‰éœ€ä»˜è´¹

**æœ€ä½³ç”¨é€”**: AWS ç”Ÿæ€å†…çš„åº”ç”¨ã€å¤šæ¨¡å‹å®éªŒ

---

## æ¨¡å‹å‚æ•°é…ç½®

### æ ¸å¿ƒå‚æ•°

```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    
    # API å¯†é’¥: ä»ç¯å¢ƒå˜é‡è¯»å–
    api_key="your-api-key",

    # æ¸©åº¦ (0-1): æ§åˆ¶è¾“å‡ºéšæœºæ€§
    # 0 = ç¡®å®šæ€§è¾“å‡º, 1 = é«˜åº¦éšæœº
    temperature=0.7,
    
    # æœ€å¤§ tokens: é™åˆ¶å“åº”é•¿åº¦
    max_tokens=1024,
    
    # Top P: æ ¸é‡‡æ ·å‚æ•° (0-1)
    top_p=0.9,
    
    # Top K: é™åˆ¶å€™é€‰ tokens æ•°é‡
    top_k=40,
    
    # Stop sequences: é‡åˆ°è¿™äº›åºåˆ—æ—¶åœæ­¢ç”Ÿæˆ
    stop=["\\n\\nHuman:", "\\n\\nAssistant:"],
    
    # æµå¼å¤„ç†
    streaming=True,
    
    # è¶…æ—¶è®¾ç½® (ç§’)
    timeout=60,
    
    # æœ€å¤§é‡è¯•æ¬¡æ•°
    max_retries=3
)
```

### Temperature (æ¸©åº¦) ä½¿ç”¨æŒ‡å—

Temperature æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§å’Œåˆ›é€ æ€§:

```python
# temperature = 0: ç¡®å®šæ€§è¾“å‡º
# é€‚ç”¨åœºæ™¯: æ•°æ®æå–ã€åˆ†ç±»ã€ç»“æ„åŒ–è¾“å‡ºã€ä»£ç ç”Ÿæˆ
deterministic_model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0
)

# temperature = 0.3-0.5: å¹³è¡¡
# é€‚ç”¨åœºæ™¯: å®¢æœå¯¹è¯ã€é—®ç­”ç³»ç»Ÿã€æŠ€æœ¯æ–‡æ¡£
balanced_model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.4
)

# temperature = 0.7-1.0: é«˜åˆ›é€ æ€§
# é€‚ç”¨åœºæ™¯: åˆ›æ„å†™ä½œã€å¤´è„‘é£æš´ã€æ•…äº‹ç”Ÿæˆ
creative_model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.9
)
```

### Max Tokens

```python
# çŸ­å›ç­” (èŠ‚çœæˆæœ¬)
short_model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    max_tokens=100
)

# æ ‡å‡†å“åº”
standard_model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024
)

# é•¿å†…å®¹ç”Ÿæˆ
long_model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    max_tokens=4096
)
```

### è¶…æ—¶å’Œé‡è¯•é…ç½®

```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    timeout=120,  # 120 ç§’è¶…æ—¶
    max_retries=5,  # æœ€å¤šé‡è¯• 5 æ¬¡
    default_request_timeout=60
)

# ä½¿ç”¨ RunnableConfig è‡ªå®šä¹‰
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    max_concurrency=5,
    recursion_limit=10
)

response = model.invoke(messages, config=config)
```

---

## å·¥å…·è°ƒç”¨ (Tool Calling)

å·¥å…·è°ƒç”¨æ˜¯ Models çš„æ ¸å¿ƒèƒ½åŠ›ä¹‹ä¸€ï¼Œå…è®¸æ¨¡å‹å†³å®šä½•æ—¶è°ƒç”¨å¤–éƒ¨å‡½æ•°æ¥è·å–ä¿¡æ¯æˆ–æ‰§è¡Œæ“ä½œã€‚

### åŸºæœ¬å·¥å…·è°ƒç”¨

```python
# å¼•å…¥æ¨¡å‹å’Œå·¥å…·
from langchain_anthropic import ChatAnthropic
# from langchain.tools import tool
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage

# å®šä¹‰å·¥å…·
@tool
def get_weather(location: str) -> str:
    """è·å–æŒ‡å®šä½ç½®çš„å¤©æ°”ä¿¡æ¯ã€‚
    
    Args:
        location: åŸå¸‚åç§°ï¼Œä¾‹å¦‚ 'åŒ—äº¬' æˆ– 'ä¸Šæµ·'
    """
    # å®é™…åº”ç”¨ä¸­ä¼šè°ƒç”¨å¤©æ°” API
    return f"{location}çš„å¤©æ°”æ˜¯æ™´æœ—ï¼Œæ¸©åº¦ 22Â°C"

@tool
def calculate(expression: str) -> float:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ã€‚
    
    Args:
        expression: è¦è®¡ç®—çš„æ•°å­¦è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ '2 + 2'
    """
    return eval(expression)

# åˆå§‹åŒ–æ¨¡å‹å¹¶ç»‘å®šå·¥å…·
model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
model_with_tools = model.bind_tools([get_weather, calculate])

# è°ƒç”¨æ¨¡å‹
response = model_with_tools.invoke([
    HumanMessage(content="åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·?")
])

# æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
if response.tool_calls:
    print("æ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·:")
    for tool_call in response.tool_calls:
        print(f"  - å·¥å…·: {tool_call['name']}")
        print(f"  - å‚æ•°: {tool_call['args']}")
```

### å®Œæ•´å·¥å…·è°ƒç”¨æµç¨‹

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, ToolMessage
from langchain_core.tools import tool

@tool
def search_database(query: str) -> str:
    """åœ¨æ•°æ®åº“ä¸­æœç´¢ä¿¡æ¯ã€‚"""
    return f"æ‰¾åˆ° 5 æ¡å…³äº '{query}' çš„è®°å½•"

# åˆå§‹åŒ–
model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
model_with_tools = model.bind_tools([search_database])

# 1. ç”¨æˆ·é—®é¢˜
messages = [HumanMessage(content="æŸ¥æ‰¾æ‰€æœ‰å…³äº Python çš„è®°å½•")]

# 2. æ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·
response = model_with_tools.invoke(messages)

# 3. æ‰§è¡Œå·¥å…·è°ƒç”¨
if response.tool_calls:
    for tool_call in response.tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        
        # æ‰§è¡Œå·¥å…·
        tool_result = search_database.invoke(tool_args)
        
        # 4. å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
        # tool_call_id æ˜¯å·¥å…·è°ƒç”¨çš„å”¯ä¸€æ ‡è¯†ç¬¦,å·¥å…·è¿”å›çš„æ¯ä¸ª ToolMessage éƒ½åŒ…å«ä¸€ä¸ª tool_call_id ï¼Œè¯¥ tool_call_id ä¸åŸå§‹å·¥å…·è°ƒç”¨ç›¸åŒ¹é…
        messages.append(response)
        messages.append(ToolMessage(
            content=tool_result,
            tool_call_id=tool_call["id"]
        ))

# 5. æ¨¡å‹ä½¿ç”¨å·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”
final_response = model.invoke(messages)
print(final_response.content)
```

### å¼ºåˆ¶å·¥å…·è°ƒç”¨

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import tool

@tool
def format_response(data: dict) -> str:
    """æ ¼å¼åŒ–å“åº”æ•°æ®ã€‚"""
    return str(data)

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# å¼ºåˆ¶ä½¿ç”¨ç‰¹å®šå·¥å…·
model_forced = model.bind_tools(
    [format_response],
    tool_choice="format_response"  # å¼ºåˆ¶ä½¿ç”¨è¿™ä¸ªå·¥å…·
)

# å¼ºåˆ¶ä½¿ç”¨ä»»æ„å·¥å…·
model_any = model.bind_tools(
    [format_response],
    tool_choice="any"  # å¿…é¡»ä½¿ç”¨æŸä¸ªå·¥å…·
)

# è‡ªåŠ¨å†³å®š (é»˜è®¤)
model_auto = model.bind_tools(
    [format_response],
    tool_choice="auto"
)
```

### å¹¶è¡Œå·¥å…·è°ƒç”¨

æŸäº›æ¨¡å‹æ”¯æŒåœ¨å•æ¬¡å“åº”ä¸­è°ƒç”¨å¤šä¸ªå·¥å…·ã€‚

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage

@tool
def get_weather(location: str) -> str:
    """è·å–å¤©æ°”ä¿¡æ¯ã€‚"""
    return f"{location}: æ™´æœ—ï¼Œ22Â°C"

@tool
def get_time(timezone: str) -> str:
    """è·å–æ—¶é—´ä¿¡æ¯ã€‚"""
    return f"{timezone}: 14:30"

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
model_with_tools = model.bind_tools([get_weather, get_time])

# æ¨¡å‹å¯èƒ½åŒæ—¶è°ƒç”¨å¤šä¸ªå·¥å…·
response = model_with_tools.invoke([
    HumanMessage(content="å‘Šè¯‰æˆ‘åŒ—äº¬çš„å¤©æ°”å’Œå½“å‰æ—¶é—´")
])

# å¤„ç†å¤šä¸ªå·¥å…·è°ƒç”¨
for tool_call in response.tool_calls:
    print(f"è°ƒç”¨å·¥å…·: {tool_call['name']} with {tool_call['args']}")
```

**ç¦ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨**

æŸäº›æ¨¡å‹ï¼ˆåŒ…æ‹¬ OpenAI å’Œ Anthropicï¼‰å…è®¸ç¦ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨åŠŸèƒ½ï¼š

```python
# ç¦ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼Œå¼ºåˆ¶æ¨¡å‹ä¸€æ¬¡åªè°ƒç”¨ä¸€ä¸ªå·¥å…·
model_sequential = model.bind_tools(
    [get_weather, get_time],
    parallel_tool_calls=False
)
```

### æµå¼å·¥å…·è°ƒç”¨ (Streaming Tool Calls)

åœ¨æµå¼å“åº”ä¸­ï¼Œå·¥å…·è°ƒç”¨é€šè¿‡ `ToolCallChunk` é€æ­¥æ„å»ºã€‚è¿™å…è®¸ä½ åœ¨å·¥å…·è°ƒç”¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®æ—¶æŸ¥çœ‹è¿›åº¦ï¼Œè€Œä¸æ˜¯ç­‰å¾…å®Œæ•´å“åº”ã€‚

#### åŸºæœ¬æµå¼å·¥å…·è°ƒç”¨

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import tool

@tool
def get_weather(location: str) -> str:
    """è·å–æŒ‡å®šä½ç½®çš„å¤©æ°”ä¿¡æ¯ã€‚"""
    return f"{location}çš„å¤©æ°”æ˜¯æ™´æœ—ï¼Œæ¸©åº¦ 22Â°C"

@tool
def get_time(timezone: str) -> str:
    """è·å–æŒ‡å®šæ—¶åŒºçš„æ—¶é—´ã€‚"""
    return f"{timezone}çš„æ—¶é—´æ˜¯ 14:30"

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
model_with_tools = model.bind_tools([get_weather, get_time])

# æµå¼è°ƒç”¨ - å·¥å…·è°ƒç”¨ç‰‡æ®µä¼šé€æ­¥åˆ°è¾¾
for chunk in model_with_tools.stream(
    "åŒ—äº¬å’Œä¸œäº¬çš„å¤©æ°”æ€ä¹ˆæ ·?"
):
    # å·¥å…·è°ƒç”¨å—é€æ­¥åˆ°è¾¾
    for tool_chunk in chunk.tool_call_chunks:
        if name := tool_chunk.get("name"):
            print(f"å·¥å…·: {name}")
        if id_ := tool_chunk.get("id"):
            print(f"ID: {id_}")
        if args := tool_chunk.get("args"):
            print(f"å‚æ•°: {args}")

# è¾“å‡ºç¤ºä¾‹:
# å·¥å…·: get_weather
# ID: call_SvMlU1TVIZugrFLckFE2ceRE
# å‚æ•°: {"lo
# å‚æ•°: catio
# å‚æ•°: n": "åŒ—
# å‚æ•°: äº¬"}
# å·¥å…·: get_weather
# ID: call_QMZdy6qInx13oWKE7KhuhOLR
# å‚æ•°: {"lo
# å‚æ•°: catio
# å‚æ•°: n": "ä¸œ
# å‚æ•°: äº¬"}
```

#### ç´¯ç§¯å—ä»¥æ„å»ºå®Œæ•´å·¥å…·è°ƒç”¨

æµå¼å“åº”ä¸­çš„å·¥å…·è°ƒç”¨ç‰‡æ®µå¯ä»¥ç´¯ç§¯èµ·æ¥ï¼Œä»¥ä¾¿è·å–å®Œæ•´çš„å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼š

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import tool

@tool
def search_database(query: str) -> str:
    """åœ¨æ•°æ®åº“ä¸­æœç´¢ä¿¡æ¯ã€‚"""
    return f"æ‰¾åˆ° 5 æ¡å…³äº '{query}' çš„è®°å½•"

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
model_with_tools = model.bind_tools([search_database])

# ç´¯ç§¯å—ä»¥æ„å»ºå®Œæ•´çš„å·¥å…·è°ƒç”¨
gathered = None
for chunk in model_with_tools.stream("æœç´¢å…³äº Python çš„ä¿¡æ¯"):
    gathered = chunk if gathered is None else gathered + chunk
    print(gathered.tool_calls)

# è¾“å‡ºé€æ­¥æ„å»ºçš„å®Œæ•´å·¥å…·è°ƒç”¨:
# []
# []
# [{'name': 'search_database', 'args': {}, 'id': 'call_xxx'}]
# [{'name': 'search_database', 'args': {'qu': ''}, 'id': 'call_xxx'}]
# [{'name': 'search_database', 'args': {'query': 'Py'}, 'id': 'call_xxx'}]
# [{'name': 'search_database', 'args': {'query': 'Python'}, 'id': 'call_xxx'}]
```

#### æµå¼å·¥å…·è°ƒç”¨çš„å®é™…åº”ç”¨

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, ToolMessage

@tool
def analyze_data(data_type: str, metric: str) -> dict:
    """åˆ†æç‰¹å®šç±»å‹çš„æ•°æ®æŒ‡æ ‡ã€‚"""
    return {
        "data_type": data_type,
        "metric": metric,
        "result": f"åˆ†æå®Œæˆ: {data_type} çš„ {metric}"
    }

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
model_with_tools = model.bind_tools([analyze_data])

# å®æ—¶æ˜¾ç¤ºå·¥å…·è°ƒç”¨çš„æ„å»ºè¿‡ç¨‹
messages = [HumanMessage(content="åˆ†æé”€å”®æ•°æ®çš„å¢é•¿ç‡")]
gathered_chunk = None

print("ğŸ”„ å¼€å§‹æµå¼å·¥å…·è°ƒç”¨...")
for chunk in model_with_tools.stream(messages):
    # ç´¯ç§¯å—
    gathered_chunk = chunk if gathered_chunk is None else gathered_chunk + chunk

    # å®æ—¶æ˜¾ç¤ºè¿›åº¦
    if chunk.tool_call_chunks:
        for tool_chunk in chunk.tool_call_chunks:
            if args := tool_chunk.get("args"):
                print(f"ğŸ“¡ æ¥æ”¶å‚æ•°ç‰‡æ®µ: {args}")

# æ˜¾ç¤ºå®Œæ•´çš„å·¥å…·è°ƒç”¨
if gathered_chunk and gathered_chunk.tool_calls:
    print("\nâœ… å®Œæ•´å·¥å…·è°ƒç”¨:")
    for tool_call in gathered_chunk.tool_calls:
        print(f"  å·¥å…·åç§°: {tool_call['name']}")
        print(f"  å®Œæ•´å‚æ•°: {tool_call['args']}")
        print(f"  è°ƒç”¨ ID: {tool_call['id']}")
```

#### æµå¼å¤šä¸ªå¹¶è¡Œå·¥å…·è°ƒç”¨

å½“æ¨¡å‹å†³å®šå¹¶è¡Œè°ƒç”¨å¤šä¸ªå·¥å…·æ—¶ï¼Œæµå¼å“åº”ä¼šåŒ…å«å¤šä¸ªå·¥å…·çš„ç‰‡æ®µï¼š

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import tool

@tool
def get_stock_price(symbol: str) -> str:
    """è·å–è‚¡ç¥¨ä»·æ ¼ã€‚"""
    return f"{symbol} å½“å‰ä»·æ ¼: $150.00"

@tool
def get_company_info(symbol: str) -> str:
    """è·å–å…¬å¸ä¿¡æ¯ã€‚"""
    return f"{symbol} å…¬å¸ä¿¡æ¯: ç§‘æŠ€å…¬å¸"

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
model_with_tools = model.bind_tools([get_stock_price, get_company_info])

# è·Ÿè¸ªå¤šä¸ªå·¥å…·è°ƒç”¨çš„æ„å»ºè¿‡ç¨‹
tool_calls_progress = {}

for chunk in model_with_tools.stream(
    "å‘Šè¯‰æˆ‘ AAPL çš„è‚¡ä»·å’Œå…¬å¸ä¿¡æ¯"
):
    for tool_chunk in chunk.tool_call_chunks:
        # ä½¿ç”¨ index è·Ÿè¸ªä¸åŒçš„å·¥å…·è°ƒç”¨
        index = tool_chunk.get("index", 0)

        if index not in tool_calls_progress:
            tool_calls_progress[index] = {
                "name": "",
                "args": "",
                "id": ""
            }

        # ç´¯ç§¯æ¯ä¸ªå·¥å…·è°ƒç”¨çš„ä¿¡æ¯
        if name := tool_chunk.get("name"):
            tool_calls_progress[index]["name"] = name
        if id_ := tool_chunk.get("id"):
            tool_calls_progress[index]["id"] = id_
        if args := tool_chunk.get("args"):
            tool_calls_progress[index]["args"] += args

        print(f"ğŸ”§ å·¥å…· #{index}: {tool_calls_progress[index]}")
```

#### ToolCallChunk æ•°æ®ç»“æ„

æµå¼å·¥å…·è°ƒç”¨ä¸­çš„ `ToolCallChunk` åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```python
# ToolCallChunk ç»“æ„
{
    "type": "tool_call_chunk",      # å§‹ç»ˆä¸º "tool_call_chunk"
    "name": "tool_name",            # è¢«è°ƒç”¨çš„å·¥å…·åç§°ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
    "args": '{"partial": "json"}',  # éƒ¨åˆ†å·¥å…·å‚æ•°ï¼ˆå¯èƒ½æ˜¯ä¸å®Œæ•´çš„ JSONï¼‰
    "id": "call_xxx",               # å·¥å…·è°ƒç”¨æ ‡è¯†ç¬¦
    "index": 0                      # æ­¤å—åœ¨æµä¸­çš„ä½ç½®
}
```

#### ä½¿ç”¨ astream_events è¿›è¡Œé«˜çº§æµå¼å¤„ç†

å¯¹äºæ›´å¤æ‚çš„æµå¼åœºæ™¯ï¼Œå¯ä»¥ä½¿ç”¨ `astream_events()` æ¥è·å–è¯­ä¹‰äº‹ä»¶ï¼š

```python
import asyncio
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import tool

@tool
def complex_calculation(formula: str) -> float:
    """æ‰§è¡Œå¤æ‚è®¡ç®—ã€‚"""
    return eval(formula)

async def stream_tool_calls_with_events():
    model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
    model_with_tools = model.bind_tools([complex_calculation])

    async for event in model_with_tools.astream_events(
        "è®¡ç®— (25 * 4) + (100 / 2)",
        version="v1"
    ):
        if event["event"] == "on_chat_model_start":
            print(f"ğŸš€ æ¨¡å‹å¼€å§‹: {event['data']['input']}")

        elif event["event"] == "on_chat_model_stream":
            chunk = event['data']['chunk']
            if hasattr(chunk, 'tool_call_chunks') and chunk.tool_call_chunks:
                for tool_chunk in chunk.tool_call_chunks:
                    print(f"ğŸ”¨ å·¥å…·å—: {tool_chunk}")

        elif event["event"] == "on_chat_model_end":
            output = event['data']['output']
            if hasattr(output, 'tool_calls') and output.tool_calls:
                print(f"âœ… å®Œæ•´å·¥å…·è°ƒç”¨: {output.tool_calls}")

# è¿è¡Œå¼‚æ­¥å‡½æ•°
# asyncio.run(stream_tool_calls_with_events())
```

#### æµå¼å·¥å…·è°ƒç”¨çš„æœ€ä½³å®è·µ

1. **è¿›åº¦æŒ‡ç¤ºå™¨**: ä½¿ç”¨æµå¼å·¥å…·è°ƒç”¨ä¸ºç”¨æˆ·æä¾›å®æ—¶åé¦ˆ
2. **é”™è¯¯å¤„ç†**: ç›‘æ§ä¸å®Œæ•´çš„ JSON å‚æ•°ï¼Œå¤„ç†è§£æé”™è¯¯
3. **ç´¯ç§¯ç­–ç•¥**: å†³å®šä½•æ—¶ç´¯ç§¯å—ä»¥è·å¾—å®Œæ•´ä¿¡æ¯
4. **æ€§èƒ½ä¼˜åŒ–**: å¯¹äºå¤§å‹å‚æ•°ï¼Œæµå¼å¤„ç†å¯ä»¥æé«˜å“åº”é€Ÿåº¦
5. **ç”¨æˆ·ä½“éªŒ**: åœ¨ UI ä¸­æ˜¾ç¤º"æ­£åœ¨è°ƒç”¨å·¥å…·..."çš„åŠ è½½çŠ¶æ€

```python
# å®Œæ•´ç¤ºä¾‹ï¼šå¸¦è¿›åº¦æŒ‡ç¤ºçš„æµå¼å·¥å…·è°ƒç”¨
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import tool
import json

@tool
def fetch_large_dataset(category: str, filters: dict) -> str:
    """è·å–å¤§å‹æ•°æ®é›†ã€‚"""
    return f"è·å– {category} æ•°æ®ï¼Œåº”ç”¨è¿‡æ»¤å™¨: {filters}"

def stream_with_progress():
    model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
    model_with_tools = model.bind_tools([fetch_large_dataset])

    gathered = None
    current_tool_name = None

    for chunk in model_with_tools.stream(
        "è·å–2024å¹´ç¬¬ä¸€å­£åº¦çš„é”€å”®æ•°æ®ï¼Œè¿‡æ»¤æ¡ä»¶ï¼šåœ°åŒºä¸ºåä¸œï¼Œé‡‘é¢å¤§äº10000"
    ):
        gathered = chunk if gathered is None else gathered + chunk

        # æ£€æµ‹å·¥å…·åç§°
        if chunk.tool_call_chunks:
            for tool_chunk in chunk.tool_call_chunks:
                if name := tool_chunk.get("name"):
                    current_tool_name = name
                    print(f"\nğŸ”§ å‡†å¤‡è°ƒç”¨å·¥å…·: {name}")

                if args := tool_chunk.get("args"):
                    print(".", end="", flush=True)  # è¿›åº¦ç‚¹

    # æ‰§è¡Œå·¥å…·è°ƒç”¨
    if gathered and gathered.tool_calls:
        print("\n\nğŸ“‹ æ‰§è¡Œå·¥å…·è°ƒç”¨:")
        for tool_call in gathered.tool_calls:
            print(f"  âœ“ {tool_call['name']}")
            print(f"  âœ“ å‚æ•°: {json.dumps(tool_call['args'], ensure_ascii=False, indent=2)}")

# stream_with_progress()
```

---

## ç»“æ„åŒ–è¾“å‡º

`with_structured_output` å…è®¸æ¨¡å‹æŒ‰ç…§é¢„å®šä¹‰çš„ Pydantic schema ç”Ÿæˆç»“æ„åŒ–æ•°æ®ã€‚

### åŸºæœ¬ç»“æ„åŒ–è¾“å‡º

```python
# TODO ValidationéªŒè¯ ï¼šPydantic æ¨¡å‹æä¾›è‡ªåŠ¨éªŒè¯ï¼Œè€Œ TypedDict å’Œ JSON Schema åˆ™éœ€è¦æ‰‹åŠ¨éªŒè¯ã€‚

from langchain_anthropic import ChatAnthropic
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage

# å®šä¹‰è¾“å‡ºç»“æ„
class Person(BaseModel):
    """ä¸€ä¸ªäººçš„ä¿¡æ¯ã€‚"""
    name: str = Field(description="äººçš„å§“å")
    age: int = Field(description="äººçš„å¹´é¾„")
    email: str = Field(description="ç”µå­é‚®ä»¶åœ°å€")
    occupation: str = Field(description="èŒä¸š")

# é…ç½®æ¨¡å‹ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
structured_model = model.with_structured_output(Person) # ç»“æ„åŒ–è¾“å‡º

# è°ƒç”¨æ¨¡å‹
response = structured_model.invoke([
    HumanMessage(content="å¼ ä¼Ÿæ˜¯ä¸€ä½ 35 å²çš„è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œé‚®ç®±æ˜¯ zhang@example.com")
])

# response æ˜¯ Person å®ä¾‹
print(f"å§“å: {response.name}")
print(f"å¹´é¾„: {response.age}")
print(f"é‚®ç®±: {response.email}")
print(f"èŒä¸š: {response.occupation}")
```

### å¤æ‚åµŒå¥—ç»“æ„

```python
from pydantic import BaseModel, Field
from typing import List, Optional

class Address(BaseModel):
    """åœ°å€ä¿¡æ¯ã€‚"""
    street: str = Field(description="è¡—é“åœ°å€")
    city: str = Field(description="åŸå¸‚")
    country: str = Field(description="å›½å®¶")
    postal_code: Optional[str] = Field(description="é‚®æ”¿ç¼–ç ")

class Company(BaseModel):
    """å…¬å¸ä¿¡æ¯ã€‚"""
    name: str = Field(description="å…¬å¸åç§°")
    industry: str = Field(description="è¡Œä¸š")
    employees: int = Field(description="å‘˜å·¥æ•°é‡")

class Employee(BaseModel):
    """å‘˜å·¥å®Œæ•´ä¿¡æ¯ã€‚"""
    name: str = Field(description="å§“å")
    age: int = Field(description="å¹´é¾„")
    position: str = Field(description="èŒä½")
    address: Address = Field(description="ä½å€")
    company: Company = Field(description="æ‰€åœ¨å…¬å¸")
    skills: List[str] = Field(description="æŠ€èƒ½åˆ—è¡¨")

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
structured_model = model.with_structured_output(Employee)

response = structured_model.invoke([
    HumanMessage(content="""
    ææ˜ï¼Œ32å²ï¼Œåœ¨åŒ—äº¬æµ·æ·€åŒºä¸­å…³æ‘å¤§è¡—1å·çš„é˜¿é‡Œå·´å·´å·¥ä½œï¼Œ
    æ‹…ä»»é«˜çº§è½¯ä»¶å·¥ç¨‹å¸ˆã€‚å…¬å¸æœ‰5000åå‘˜å·¥ï¼Œä¸»è¦ä»äº‹ç”µå­å•†åŠ¡ã€‚
    ä»–æ“…é•¿Pythonã€æœºå™¨å­¦ä¹ å’Œäº‘è®¡ç®—ã€‚é‚®ç¼–100080ã€‚
    """)
])

print(f"å‘˜å·¥: {response.name}, {response.position}")
print(f"å…¬å¸: {response.company.name}, {response.company.employees}äºº")
print(f"åœ°å€: {response.address.city}, {response.address.street}")
print(f"æŠ€èƒ½: {', '.join(response.skills)}")
```

### åˆ—è¡¨ç±»å‹è¾“å‡º

```python
from pydantic import BaseModel, Field
from typing import List

class Product(BaseModel):
    """äº§å“ä¿¡æ¯ã€‚"""
    name: str = Field(description="äº§å“åç§°")
    price: float = Field(description="ä»·æ ¼")
    category: str = Field(description="ç±»åˆ«")

class ProductList(BaseModel):
    """äº§å“åˆ—è¡¨ã€‚"""
    products: List[Product] = Field(description="äº§å“åˆ—è¡¨")

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
structured_model = model.with_structured_output(ProductList)

response = structured_model.invoke([
    HumanMessage(content="""
    æˆ‘ä»¬æœ‰ä»¥ä¸‹äº§å“:
    1. iPhone 15 Pro - 8999å…ƒ - ç”µå­äº§å“
    2. MacBook Air - 9499å…ƒ - ç”µå­äº§å“  
    3. AirPods Pro - 1999å…ƒ - é…ä»¶
    """)
])

for product in response.products:
    print(f"{product.name}: Â¥{product.price} ({product.category})")
```

### ä½¿ç”¨ Pydantic éªŒè¯å™¨ï¼ˆValidation éªŒè¯å™¨ï¼‰

```python
# TODO Pydantic æ¨¡å‹æä¾›è‡ªåŠ¨éªŒè¯ï¼Œè€Œ TypedDict å’Œ JSON Schema åˆ™éœ€è¦æ‰‹åŠ¨éªŒè¯ã€‚
from pydantic import BaseModel, Field, validator

class OrderInfo(BaseModel):
    """è®¢å•ä¿¡æ¯ã€‚"""
    order_id: str = Field(description="è®¢å•å·")
    amount: float = Field(description="é‡‘é¢", gt=0)
    status: str = Field(description="çŠ¶æ€")
    
    @validator('status')
    def validate_status(cls, v):
        """éªŒè¯çŠ¶æ€å¿…é¡»æ˜¯å…è®¸çš„å€¼ä¹‹ä¸€ã€‚"""
        allowed = ['pending', 'paid', 'shipped', 'delivered', 'cancelled']
        if v.lower() not in allowed:
            raise ValueError(f'çŠ¶æ€å¿…é¡»æ˜¯: {allowed}ä¹‹ä¸€')
        return v.lower()
    
    @validator('order_id')
    def validate_order_id(cls, v):
        """éªŒè¯è®¢å•å·æ ¼å¼ã€‚"""
        if not v.startswith('ORD-'):
            raise ValueError('è®¢å•å·å¿…é¡»ä»¥ ORD- å¼€å¤´')
        return v

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
structured_model = model.with_structured_output(OrderInfo)

response = structured_model.invoke([
    HumanMessage(content="è®¢å• ORD-12345ï¼Œé‡‘é¢ 299.99ï¼ŒçŠ¶æ€å·²æ”¯ä»˜")
])
```

---

## æµå¼å¤„ç†

æµå¼å¤„ç†å…è®¸å®æ—¶æ¥æ”¶æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

### Token æµå¼å¤„ç†

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    streaming=True
)

# ä½¿ç”¨ stream() æ–¹æ³•
for chunk in model.stream([HumanMessage(content="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—")]):
    print(chunk.content, end="", flush=True)
```

### å¼‚æ­¥æµå¼å¤„ç†

```python
import asyncio
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

async def async_stream_example():
    model = ChatAnthropic(
        model="claude-3-5-sonnet-20241022",
        streaming=True
    )
    
    async for chunk in model.astream([
        HumanMessage(content="è§£é‡Šé‡å­çº ç¼ ")
    ]):
        print(chunk.content, end="", flush=True)

# è¿è¡Œ
asyncio.run(async_stream_example())
```

### äº‹ä»¶æµ (astream_events)

æ›´ç»†ç²’åº¦çš„æµå¼æ§åˆ¶ï¼Œå¯ä»¥ç›‘å¬æ¨¡å‹ã€å·¥å…·ç­‰çš„æ‰€æœ‰äº‹ä»¶ã€‚

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
import asyncio

@tool
def search(query: str) -> str:
    """æœç´¢ä¿¡æ¯ã€‚"""
    return f"å…³äº {query} çš„æœç´¢ç»“æœ"

async def stream_events_example():
    model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
    model_with_tools = model.bind_tools([search])
    
    async for event in model_with_tools.astream_events(
        [HumanMessage(content="æœç´¢ LangChain ä¿¡æ¯")],
        version="v2"
    ):
        kind = event["event"]
        
        if kind == "on_chat_model_start":
            print("æ¨¡å‹å¼€å§‹å¤„ç†...")
        
        elif kind == "on_chat_model_stream":
            content = event["data"]["chunk"].content
            if content:
                print(content, end="", flush=True)
        
        elif kind == "on_tool_start":
            print(f"\nå·¥å…·è°ƒç”¨å¼€å§‹: {event['name']}")
        
        elif kind == "on_tool_end":
            print(f"\nå·¥å…·è°ƒç”¨ç»“æŸ: {event['data']['output']}")

asyncio.run(stream_events_example())
```

### æµå¼å¤„ç†ä¸­çš„ Token ç»Ÿè®¡

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

total_tokens = 0
content = ""

for chunk in model.stream([HumanMessage(content="ä»‹ç»äººå·¥æ™ºèƒ½")]):
    content += chunk.content
    
    # æŸäº› chunk åŒ…å« usage ä¿¡æ¯
    if hasattr(chunk, 'usage_metadata'):
        total_tokens = chunk.usage_metadata.get('total_tokens', 0)

print(f"\n\næ€» tokens: {total_tokens}")
```

---

## å¤šæ¨¡æ€æ”¯æŒ

ç°ä»£æ¨¡å‹æ”¯æŒå¤„ç†æ–‡æœ¬ã€å›¾åƒç­‰å¤šç§è¾“å…¥ç±»å‹ã€‚

### å›¾åƒè¾“å…¥

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage
import base64

# è¯»å–å›¾åƒ
with open("image.jpg", "rb") as f:
    image_data = base64.b64encode(f.read()).decode()

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# å‘é€å›¾åƒç»™æ¨¡å‹
response = model.invoke([
    HumanMessage(content=[
        {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆ?"},
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
        }
    ])
])

print(response.content)
```

### å¤„ç†å¤šå¼ å›¾åƒ

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage
import base64

def encode_image(image_path: str) -> str:
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode()

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# åŒæ—¶å¤„ç†å¤šå¼ å›¾ç‰‡
response = model.invoke([
    HumanMessage(content=[
        {"type": "text", "text": "æ¯”è¾ƒè¿™ä¸¤å¼ å›¾ç‰‡çš„å·®å¼‚"},
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{encode_image('image1.jpg')}"}
        },
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{encode_image('image2.jpg')}"}
        }
    ])
])

print(response.content)
```

### ä½¿ç”¨å›¾åƒ URL

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# ç›´æ¥ä½¿ç”¨å›¾åƒ URL
response = model.invoke([
    HumanMessage(content=[
        {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡"},
        {
            "type": "image_url",
            "image_url": {"url": "https://example.com/image.jpg"}
        }
    ])
])

print(response.content)
```

---

## Token ä½¿ç”¨å’Œå…ƒæ•°æ®

### è·å– Token ä½¿ç”¨ä¿¡æ¯

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

response = model.invoke([
    HumanMessage(content="è§£é‡Šæœºå™¨å­¦ä¹ ")
])

# è·å– token ä½¿ç”¨ä¿¡æ¯
if hasattr(response, 'usage_metadata'):
    usage = response.usage_metadata
    print(f"è¾“å…¥ tokens: {usage.get('input_tokens', 0)}")
    print(f"è¾“å‡º tokens: {usage.get('output_tokens', 0)}")
    print(f"æ€» tokens: {usage.get('total_tokens', 0)}")
```

### è®¡ç®—æˆæœ¬

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

# Claude 3.5 Sonnet ä»·æ ¼ (ç¤ºä¾‹)
INPUT_PRICE_PER_1M = 3.0  # $3 per 1M input tokens
OUTPUT_PRICE_PER_1M = 15.0  # $15 per 1M output tokens

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

response = model.invoke([
    HumanMessage(content="å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ")
])

if hasattr(response, 'usage_metadata'):
    usage = response.usage_metadata
    input_tokens = usage.get('input_tokens', 0)
    output_tokens = usage.get('output_tokens', 0)
    
    # è®¡ç®—æˆæœ¬
    input_cost = (input_tokens / 1_000_000) * INPUT_PRICE_PER_1M
    output_cost = (output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M
    total_cost = input_cost + output_cost
    
    print(f"è¾“å…¥: {input_tokens} tokens (${input_cost:.6f})")
    print(f"è¾“å‡º: {output_tokens} tokens (${output_cost:.6f})")
    print(f"æ€»æˆæœ¬: ${total_cost:.6f}")
```

### å“åº”å…ƒæ•°æ®

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

response = model.invoke([
    HumanMessage(content="ä½ å¥½")
])

# è·å–å…ƒæ•°æ®
print("å“åº”å…ƒæ•°æ®:")
print(f"  æ¨¡å‹: {response.response_metadata.get('model')}")
print(f"  Stop reason: {response.response_metadata.get('stop_reason')}")
print(f"  æ¶ˆæ¯ ID: {response.id}")
```

---

## é”™è¯¯å¤„ç†å’Œé‡è¯•

### åŸºæœ¬é”™è¯¯å¤„ç†

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage
import anthropic

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

try:
    response = model.invoke([
        HumanMessage(content="ä½ å¥½")
    ])
    print(response.content)
    
except anthropic.APIError as e:
    print(f"API é”™è¯¯: {e}")
    
except anthropic.RateLimitError as e:
    print(f"é€Ÿç‡é™åˆ¶: {e}")
    
except anthropic.APIConnectionError as e:
    print(f"è¿æ¥é”™è¯¯: {e}")
    
except Exception as e:
    print(f"æœªçŸ¥é”™è¯¯: {e}")
```

### è‡ªåŠ¨é‡è¯•é…ç½®

```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    max_retries=5,  # æœ€å¤šé‡è¯• 5 æ¬¡
    timeout=120,  # è¶…æ—¶æ—¶é—´
)

# LangChain ä¼šè‡ªåŠ¨å¤„ç†é‡è¯•
response = model.invoke([HumanMessage(content="ä½ å¥½")])
```

### ä½¿ç”¨ tenacity è¿›è¡Œé«˜çº§é‡è¯•

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
import anthropic

@retry(
    stop=stop_after_attempt(3),  # æœ€å¤šé‡è¯• 3 æ¬¡
    wait=wait_exponential(multiplier=1, min=4, max=60),  # æŒ‡æ•°é€€é¿
    retry=retry_if_exception_type((
        anthropic.RateLimitError,
        anthropic.APIConnectionError
    ))
)
def call_model_with_retry(messages):
    model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
    return model.invoke(messages)

# ä½¿ç”¨
try:
    response = call_model_with_retry([
        HumanMessage(content="ä½ å¥½")
    ])
    print(response.content)
except Exception as e:
    print(f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†: {e}")
```

---

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹

```python
# ç®€å•ä»»åŠ¡ â†’ å¿«é€Ÿæ¨¡å‹
simple_model = ChatAnthropic(model="claude-3-haiku-20240307")

# å¤æ‚æ¨ç† â†’ å¼ºå¤§æ¨¡å‹
complex_model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# å…³é”®ä»»åŠ¡ â†’ æœ€å¼ºæ¨¡å‹
critical_model = ChatAnthropic(model="claude-3-opus-20240229")
```

### 2. ä¼˜åŒ– Temperature

```python
# äº‹å®æ€§ä»»åŠ¡: temperature = 0
factual_model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0
)

# åˆ›é€ æ€§ä»»åŠ¡: temperature = 0.7-1.0
creative_model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.9
)
```

### 3. æ¸…æ™°çš„ç³»ç»Ÿæç¤ºè¯

```python
from langchain_core.messages import SystemMessage, HumanMessage

system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æˆ·æœåŠ¡åŠ©æ‰‹ã€‚

ä½ çš„èŒè´£:
- ç¤¼è²Œã€ä¸“ä¸šåœ°å›ç­”å®¢æˆ·é—®é¢˜
- å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯šå®è¯´æ˜å¹¶å¯»æ±‚å¸®åŠ©
- ä½¿ç”¨ç®€æ´ã€æ¸…æ™°çš„è¯­è¨€

ä½ çš„é™åˆ¶:
- ä¸è¦æä¾›åŒ»ç–—æˆ–æ³•å¾‹å»ºè®®
- ä¸è¦åˆ†äº«å®¢æˆ·çš„ä¸ªäººä¿¡æ¯
- ä¸è¦åšå‡ºå…¬å¸æ— æ³•å…‘ç°çš„æ‰¿è¯º
"""

model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
response = model.invoke([
    SystemMessage(content=system_prompt),
    HumanMessage(content="æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™åˆ°?")
])
```

### 4. å·¥å…·æè¿°æ¸…æ™°å‡†ç¡®

```python
from langchain_core.tools import tool

@tool
def get_customer_info(customer_id: str) -> dict:
    """è·å–å®¢æˆ·è¯¦ç»†ä¿¡æ¯ã€‚
    
    ä½¿ç”¨æ­¤å·¥å…·æŸ¥æ‰¾å®¢æˆ·çš„è´¦æˆ·ä¿¡æ¯ã€è®¢å•å†å²å’Œåå¥½è®¾ç½®ã€‚
    
    Args:
        customer_id: å®¢æˆ·çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œæ ¼å¼: CUST-XXXXX
        
    Returns:
        åŒ…å«å®¢æˆ·ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…æ‹¬å§“åã€é‚®ç®±ã€ä¼šå‘˜ç­‰çº§ç­‰
    """
    return {"name": "å¼ ä¼Ÿ", "email": "zhang@example.com"}
```

### 5. ç›‘æ§å’Œæ—¥å¿—

```python
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def monitored_model_call(messages):
    model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
    start_time = time.time()
    
    try:
        response = model.invoke(messages)
        duration = time.time() - start_time
        logger.info(f"æ¨¡å‹è°ƒç”¨æˆåŠŸ - è€—æ—¶: {duration:.2f}s")
        
        if hasattr(response, 'usage_metadata'):
            usage = response.usage_metadata
            logger.info(f"Tokenä½¿ç”¨ - è¾“å…¥: {usage.get('input_tokens')}, è¾“å‡º: {usage.get('output_tokens')}")
        
        return response
    except Exception as e:
        logger.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        raise
```

---

## é«˜çº§ç”¨æ³•

### 1. é“¾å¼è°ƒç”¨ (Chains)

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{role}"),
    ("human", "{input}")
])

# åˆ›å»ºé“¾
model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
chain = prompt | model | StrOutputParser()

# è°ƒç”¨é“¾
response = chain.invoke({
    "role": "è¯—äºº",
    "input": "å†™ä¸€é¦–å…³äºæœˆäº®çš„è¯—"
})

print(response)
```

### 2. Fallback æœºåˆ¶

```python
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

# ä¸»æ¨¡å‹
primary_model = ChatAnthropic(model="claude-3-5-sonnet-20241022")

# å¤‡ç”¨æ¨¡å‹
fallback_model = ChatOpenAI(model="gpt-4o")

# åˆ›å»ºå¸¦ fallback çš„æ¨¡å‹
model_with_fallback = primary_model.with_fallbacks([fallback_model])

# å¦‚æœä¸»æ¨¡å‹å¤±è´¥ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
response = model_with_fallback.invoke([HumanMessage(content="ä½ å¥½")])
```

### 3. æ‰¹å¤„ç†ä¼˜åŒ–

```python
import asyncio
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

async def batch_process():
    model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
    
    # å‡†å¤‡å¤šä¸ªè¯·æ±‚
    questions = [
        "ä»€ä¹ˆæ˜¯AI?",
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ?",
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ?"
    ]
    
    # å¹¶è¡Œå¤„ç†
    tasks = [
        model.ainvoke([HumanMessage(content=q)])
        for q in questions
    ]
    
    responses = await asyncio.gather(*tasks)
    return responses

# è¿è¡Œ
results = asyncio.run(batch_process())
```

---

## ğŸ“Š æ¨¡å‹é€‰æ‹©å¯¹æ¯”è¡¨

| æ¨¡å‹ | æä¾›å•† | ä¸Šä¸‹æ–‡çª—å£ | æœ€ä½³ç”¨é€” | ç›¸å¯¹æˆæœ¬ |
|------|--------|-----------|----------|---------|
| Claude 3.5 Sonnet | Anthropic | 200K | Agentã€å·¥å…·è°ƒç”¨ã€å¤æ‚æ¨ç† | ä¸­ç­‰ |
| Claude 3 Opus | Anthropic | 200K | æœ€å¤æ‚ä»»åŠ¡ã€æœ€é«˜è´¨é‡è¾“å‡º | é«˜ |
| Claude 3 Haiku | Anthropic | 200K | å¿«é€Ÿå“åº”ã€ç®€å•ä»»åŠ¡ | ä½ |
| GPT-4o | OpenAI | 128K | å¤šæ¨¡æ€ã€é€šç”¨ä»»åŠ¡ | ä¸­ç­‰ |
| GPT-4 Turbo | OpenAI | 128K | å¤æ‚æ¨ç†ã€é•¿ä¸Šä¸‹æ–‡ | é«˜ |
| GPT-3.5 Turbo | OpenAI | 16K | ç®€å•ä»»åŠ¡ã€ç»æµå‹ | ä½ |
| Gemini 2.0 Flash | Google | 1M+ | è¶…é•¿ä¸Šä¸‹æ–‡ã€å¤šæ¨¡æ€ | ä½ |
| Gemini 1.5 Pro | Google | 2M | æœ€é•¿ä¸Šä¸‹æ–‡ã€å¤æ‚åˆ†æ | ä¸­ç­‰ |

---

## ğŸ¯ å…³é”®æ¦‚å¿µæ€»ç»“

1. **Models æ˜¯ Agent çš„æ¨ç†å¼•æ“**: è´Ÿè´£ç†è§£ã€å†³ç­–å’Œç”Ÿæˆ
2. **ä¼˜å…ˆä½¿ç”¨ Chat Models**: æ›´é€‚åˆç°ä»£å¯¹è¯åº”ç”¨
3. **init_chat_model ç»Ÿä¸€åˆå§‹åŒ–**: è·¨æä¾›å•†çš„ä¸€è‡´æ¥å£
4. **å·¥å…·è°ƒç”¨æ˜¯æ ¸å¿ƒèƒ½åŠ›**: é€šè¿‡ bind_tools å®ç°
5. **ç»“æ„åŒ–è¾“å‡ºæé«˜å¯é æ€§**: ä½¿ç”¨ with_structured_output
6. **æµå¼å¤„ç†æ”¹å–„ä½“éªŒ**: stream() å’Œ astream_events()
7. **åˆç†é…ç½®å‚æ•°**: temperatureã€max_tokens ç­‰
8. **ç›‘æ§ token ä½¿ç”¨**: ä¼˜åŒ–æˆæœ¬å’Œæ€§èƒ½
9. **é”™è¯¯å¤„ç†å’Œé‡è¯•**: æé«˜ç³»ç»Ÿç¨³å®šæ€§
10. **æ ¹æ®ä»»åŠ¡é€‰æ‹©æ¨¡å‹**: å¹³è¡¡æ€§èƒ½ã€æˆæœ¬å’Œé€Ÿåº¦

---

## â“ å¸¸è§é—®é¢˜

**Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Ÿ**  
A: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦ã€å“åº”æ—¶é—´è¦æ±‚å’Œæˆæœ¬é¢„ç®—é€‰æ‹©ã€‚ç®€å•ä»»åŠ¡ç”¨ Haiku/GPT-3.5ï¼Œå¤æ‚æ¨ç†ç”¨ Sonnet/GPT-4oï¼Œå…³é”®ä»»åŠ¡ç”¨ Opusã€‚

**Q: å·¥å…·è°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ**  
A: ç¡®ä¿å·¥å…·æè¿°æ¸…æ™°ã€å‚æ•°ç±»å‹æ­£ç¡®ï¼Œä½¿ç”¨ Pydantic éªŒè¯ï¼Œå¹¶å¤„ç† ToolExceptionã€‚

**Q: å¦‚ä½•é™ä½ API æˆæœ¬ï¼Ÿ**  
A: ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ã€ä¼˜åŒ–æç¤ºè¯é•¿åº¦ã€åˆ©ç”¨ç¼“å­˜ã€å‡å°‘ max_tokensã€æ‰¹å¤„ç†è¯·æ±‚ã€‚

**Q: æµå¼å¤„ç†å’Œæ™®é€šè°ƒç”¨æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**  
A: æµå¼å¤„ç†å®æ—¶è¿”å› tokensï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œä½†ç¨å¾®å¢åŠ å¤æ‚åº¦ã€‚

**Q: ç»“æ„åŒ–è¾“å‡ºæ€»æ˜¯å¯é å—ï¼Ÿ**  
A: å¤§å¤šæ•°æƒ…å†µä¸‹å¯é ï¼Œä½†åº”æ·»åŠ  Pydantic éªŒè¯å’Œé”™è¯¯å¤„ç†ä½œä¸ºä¿éšœã€‚

---

## ğŸ”— ç›¸å…³èµ„æº

- **å®˜æ–¹æ–‡æ¡£**: https://docs.langchain.com/oss/python/langchain/models
- **Anthropic æ–‡æ¡£**: https://docs.anthropic.com/
- **OpenAI æ–‡æ¡£**: https://platform.openai.com/docs
- **é…å¥—æ–‡æ¡£**:
  - [LangChain Agents è¯¦ç»†æ€»ç»“](./LangChain_Agents_è¯¦ç»†æ€»ç»“.md)
  - [LangChain Tools è¯¦ç»†æŒ‡å—](./LangChain_Tools_è¯¦ç»†æŒ‡å—.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ  
**åŸºäº**: LangChain v0.3+, Python 3.9+

æœ¬æ–‡æ¡£æ¶µç›–äº† LangChain Models çš„æ ¸å¿ƒæ¦‚å¿µã€ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µï¼ŒåŒ…å« 100+ å®ç”¨ä»£ç ç¤ºä¾‹ã€‚
