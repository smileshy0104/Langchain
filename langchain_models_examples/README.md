# LangChain Models å®Œæ•´ç¤ºä¾‹é›† (GLM æ¨¡å‹ç‰ˆæœ¬)

æœ¬é¡¹ç›®åŒ…å«åŸºäº LangChain Models å®˜æ–¹æ–‡æ¡£çš„å®Œæ•´ç¤ºä¾‹ä»£ç ï¼Œä½¿ç”¨æ™ºè°± AI çš„ GLM æ¨¡å‹å®ç°ã€‚

## ğŸ“‹ ç›®å½•

- [01_model_initialization.py](01_model_initialization.py) - æ¨¡å‹åˆå§‹åŒ–å’Œå‚æ•°é…ç½®
- [02_tool_calling.py](02_tool_calling.py) - å·¥å…·è°ƒç”¨ç¤ºä¾‹
- [03_structured_output.py](03_structured_output.py) - ç»“æ„åŒ–è¾“å‡ºç¤ºä¾‹
- [04_streaming_and_advanced.py](04_streaming_and_advanced.py) - æµå¼å¤„ç†å’Œé«˜çº§ç”¨æ³•

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install langchain langchain-community langchain-core zhipuai pydantic
```

### 2. è®¾ç½® API Key

```bash
export ZHIPUAI_API_KEY="your-api-key-here"
```

### 3. è¿è¡Œç¤ºä¾‹

```bash
# æ¨¡å‹åˆå§‹åŒ–ç¤ºä¾‹
python 01_model_initialization.py

# å·¥å…·è°ƒç”¨ç¤ºä¾‹
python 02_tool_calling.py

# ç»“æ„åŒ–è¾“å‡ºç¤ºä¾‹
python 03_structured_output.py

# æµå¼å¤„ç†å’Œé«˜çº§ç”¨æ³•
python 04_streaming_and_advanced.py
```

## ğŸ“š ç¤ºä¾‹è¯´æ˜

### 01_model_initialization.py - æ¨¡å‹åˆå§‹åŒ–

**åŒ…å«å†…å®¹:**
- Chat Models åŸºæœ¬ä½¿ç”¨
- æ¨¡å‹å‚æ•°é…ç½® (temperature, max_tokens ç­‰)
- Temperature ä½¿ç”¨æŒ‡å— (0-1)
- Max Tokens é…ç½®
- ç³»ç»Ÿæç¤ºè¯ä½¿ç”¨
- å¯¹è¯å†å²ç®¡ç†
- ä¸åŒæ¨¡å‹é€‰æ‹©
- é”™è¯¯å¤„ç†
- å“åº”å…ƒæ•°æ®è·å–
- æµå¼å¤„ç†é¢„è§ˆ

**æ ¸å¿ƒå‚æ•°:**
```python
model = ChatZhipuAI(
    model="glm-4-plus",
    temperature=0.7,    # æ§åˆ¶éšæœºæ€§ 0-1
    max_tokens=1024,    # æœ€å¤§è¾“å‡ºé•¿åº¦
    top_p=0.9,          # æ ¸é‡‡æ ·
    timeout=60,         # è¶…æ—¶æ—¶é—´
    max_retries=3       # æœ€å¤§é‡è¯•æ¬¡æ•°
)
```

---

### 02_tool_calling.py - å·¥å…·è°ƒç”¨

**åŒ…å«å†…å®¹:**
- åŸºæœ¬å·¥å…·è°ƒç”¨
- å®Œæ•´å·¥å…·è°ƒç”¨æµç¨‹
- å¹¶è¡Œå·¥å…·è°ƒç”¨
- é¡ºåºå·¥å…·è°ƒç”¨ (ç¦ç”¨å¹¶è¡Œ)
- å¼ºåˆ¶å·¥å…·è°ƒç”¨
- å·¥å…·è°ƒç”¨å†³ç­–æ¨¡å¼

**å·¥å…·å®šä¹‰:**
```python
@tool
def get_weather(location: str) -> str:
    """è·å–æŒ‡å®šä½ç½®çš„å¤©æ°”ä¿¡æ¯ã€‚

    Args:
        location: åŸå¸‚åç§°ï¼Œä¾‹å¦‚ 'åŒ—äº¬' æˆ– 'ä¸Šæµ·'
    """
    return f"{location}çš„å¤©æ°”æ˜¯æ™´æœ—ï¼Œæ¸©åº¦ 22Â°C"
```

**ä½¿ç”¨å·¥å…·:**
```python
model = ChatZhipuAI(model="glm-4-plus")
model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke([
    HumanMessage(content="åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·?")
])
```

---

### 03_structured_output.py - ç»“æ„åŒ–è¾“å‡º

**åŒ…å«å†…å®¹:**
- åŸºæœ¬ç»“æ„åŒ–è¾“å‡º
- å¤æ‚åµŒå¥—ç»“æ„
- åˆ—è¡¨ç±»å‹è¾“å‡º
- Pydantic éªŒè¯å™¨
- æ•°æ®æå–ç¤ºä¾‹
- æƒ…æ„Ÿåˆ†æç¤ºä¾‹
- äº‹ä»¶æå–ç¤ºä¾‹

**å®šä¹‰ç»“æ„:**
```python
class Person(BaseModel):
    """ä¸€ä¸ªäººçš„ä¿¡æ¯ã€‚"""
    name: str = Field(description="äººçš„å§“å")
    age: int = Field(description="äººçš„å¹´é¾„")
    email: str = Field(description="ç”µå­é‚®ä»¶åœ°å€")
    occupation: str = Field(description="èŒä¸š")
```

**ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º:**
```python
model = ChatZhipuAI(model="glm-4-plus")
structured_model = model.with_structured_output(Person)

response = structured_model.invoke([
    HumanMessage(content="å¼ ä¼Ÿæ˜¯ä¸€ä½ 35 å²çš„è½¯ä»¶å·¥ç¨‹å¸ˆ")
])

# response æ˜¯ Person å®ä¾‹
print(response.name, response.age)
```

---

### 04_streaming_and_advanced.py - æµå¼å¤„ç†å’Œé«˜çº§ç”¨æ³•

**åŒ…å«å†…å®¹:**
- Token æµå¼å¤„ç†
- å¼‚æ­¥æµå¼å¤„ç†
- æµå¼å·¥å…·è°ƒç”¨
- é“¾å¼è°ƒç”¨ (Chains)
- æ‰¹å¤„ç†ä¼˜åŒ–
- Fallback æœºåˆ¶
- é‡è¯•é…ç½®
- Token ä½¿ç”¨ç»Ÿè®¡
- ç›‘æ§å’Œæ—¥å¿—

**æµå¼å¤„ç†:**
```python
model = ChatZhipuAI(model="glm-4-plus", streaming=True)

for chunk in model.stream([HumanMessage(content="å†™ä¸€é¦–è¯—")]):
    print(chunk.content, end="", flush=True)
```

**é“¾å¼è°ƒç”¨:**
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{role}"),
    ("human", "{input}")
])

chain = prompt | model | StrOutputParser()

response = chain.invoke({
    "role": "è¯—äºº",
    "input": "å†™ä¸€é¦–è¯—"
})
```

## ğŸ”‘ GLM æ¨¡å‹è¯´æ˜

### å¯ç”¨æ¨¡å‹

- **glm-4-plus** - æ¨èï¼Œæ€§èƒ½å¼ºå¤§ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡
- **glm-4-flash** - å¿«é€Ÿå“åº”ï¼Œæˆæœ¬ä½ï¼Œé€‚åˆç®€å•ä»»åŠ¡
- **glm-4** - æ ‡å‡†ç‰ˆæœ¬

### æ¨¡å‹é€‰æ‹©å»ºè®®

| åœºæ™¯ | æ¨èæ¨¡å‹ | Temperature |
|------|---------|------------|
| æ•°æ®æå–ã€åˆ†ç±» | glm-4-plus | 0 |
| å®¢æœå¯¹è¯ã€é—®ç­” | glm-4-plus | 0.3-0.5 |
| åˆ›æ„å†™ä½œ | glm-4-plus | 0.7-0.9 |
| ç®€å•ä»»åŠ¡ | glm-4-flash | 0.5 |

## ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ

### 1. Chat Models vs LLMs

**Chat Models** (æ¨è):
- æ¥å—æ¶ˆæ¯åˆ—è¡¨
- æ”¯æŒç³»ç»Ÿæ¶ˆæ¯ã€ç”¨æˆ·æ¶ˆæ¯ã€AI æ¶ˆæ¯
- åŸç”Ÿæ”¯æŒå·¥å…·è°ƒç”¨
- æ›´å¥½çš„å¯¹è¯ç®¡ç†

**LLMs**:
- æ¥å—å­—ç¬¦ä¸²è¾“å…¥
- è¿”å›å­—ç¬¦ä¸²è¾“å‡º
- é€‚åˆå•è½®æ–‡æœ¬ç”Ÿæˆ

### 2. Temperature æ§åˆ¶

```python
# temperature = 0: ç¡®å®šæ€§è¾“å‡º
# é€‚ç”¨: æ•°æ®æå–ã€åˆ†ç±»ã€ä»£ç ç”Ÿæˆ
model = ChatZhipuAI(temperature=0)

# temperature = 0.5: å¹³è¡¡æ¨¡å¼
# é€‚ç”¨: å®¢æœå¯¹è¯ã€æŠ€æœ¯æ–‡æ¡£
model = ChatZhipuAI(temperature=0.5)

# temperature = 0.9: é«˜åˆ›é€ æ€§
# é€‚ç”¨: åˆ›æ„å†™ä½œã€å¤´è„‘é£æš´
model = ChatZhipuAI(temperature=0.9)
```

### 3. å·¥å…·è°ƒç”¨æµç¨‹

```
ç”¨æˆ·è¾“å…¥ â†’ Modelæ¨ç† â†’ å†³å®šè°ƒç”¨å·¥å…· â†’ æ‰§è¡Œå·¥å…· â†’
è¿”å›ç»“æœ â†’ Modelç»¼åˆ â†’ æœ€ç»ˆå“åº”
```

### 4. ç»“æ„åŒ–è¾“å‡ºä¼˜åŠ¿

- âœ… ä¿è¯è¾“å‡ºæ ¼å¼ä¸€è‡´
- âœ… è‡ªåŠ¨ç±»å‹éªŒè¯
- âœ… æ˜“äºè§£æå’Œå¤„ç†
- âœ… å‡å°‘é”™è¯¯å¤„ç†ä»£ç 

### 5. æµå¼å¤„ç†å¥½å¤„

- âš¡ å®æ—¶å“åº”ï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒ
- ğŸ“Š å¯ä»¥æ˜¾ç¤ºè¿›åº¦
- ğŸ”„ æ”¯æŒå¤§é‡æ–‡æœ¬ç”Ÿæˆ

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æ•°æ®æå–

```python
class ContactInfo(BaseModel):
    name: str
    phone: str
    email: str

model = ChatZhipuAI(model="glm-4-plus", temperature=0)
structured_model = model.with_structured_output(ContactInfo)

response = structured_model.invoke([
    HumanMessage(content="å¼ ä¸‰ï¼Œç”µè¯13812345678ï¼Œé‚®ç®±zhang@example.com")
])
```

### åœºæ™¯ 2: æ™ºèƒ½å®¢æœ

```python
@tool
def search_kb(query: str) -> str:
    """æœç´¢çŸ¥è¯†åº“"""
    return "æ‰¾åˆ°ç›¸å…³æ–‡æ¡£..."

model = ChatZhipuAI(model="glm-4-plus", temperature=0.5)
model_with_tools = model.bind_tools([search_kb])

response = model_with_tools.invoke([
    SystemMessage(content="ä½ æ˜¯å®¢æœåŠ©æ‰‹"),
    HumanMessage(content="å¦‚ä½•é€€æ¬¾?")
])
```

### åœºæ™¯ 3: å†…å®¹ç”Ÿæˆ

```python
model = ChatZhipuAI(
    model="glm-4-plus",
    temperature=0.8,
    streaming=True
)

for chunk in model.stream([
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªåˆ›æ„ä½œå®¶"),
    HumanMessage(content="å†™ä¸€ä¸ªç§‘å¹»æ•…äº‹")
]):
    print(chunk.content, end="", flush=True)
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **API Key å®‰å…¨**
   - ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨
   - ä¸è¦ç¡¬ç¼–ç åœ¨ä»£ç ä¸­
   - ä¸è¦æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶

2. **æˆæœ¬æ§åˆ¶**
   - ä½¿ç”¨ `max_tokens` é™åˆ¶è¾“å‡º
   - é€‰æ‹©åˆé€‚çš„ temperature
   - è€ƒè™‘ä½¿ç”¨ glm-4-flash é™ä½æˆæœ¬

3. **é”™è¯¯å¤„ç†**
   - ä½¿ç”¨ `max_retries` é…ç½®é‡è¯•
   - æ•è·å¼‚å¸¸å¹¶å¤„ç†
   - æ·»åŠ è¶…æ—¶è®¾ç½®

4. **æ€§èƒ½ä¼˜åŒ–**
   - ä½¿ç”¨æµå¼å¤„ç†æ”¹å–„ä½“éªŒ
   - æ‰¹å¤„ç†å¤šä¸ªè¯·æ±‚
   - ä½¿ç”¨å¼‚æ­¥è°ƒç”¨æå‡å¹¶å‘

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æç¤º "API Key æœªè®¾ç½®"

**è§£å†³æ–¹æ¡ˆ:**
```bash
export ZHIPUAI_API_KEY="your-api-key-here"
```

### Q2: å·¥å…·æ²¡æœ‰è¢«è°ƒç”¨

**åŸå› :**
- å·¥å…·æè¿°ä¸å¤Ÿæ¸…æ™°
- ç³»ç»Ÿæç¤ºè¯æ²¡æœ‰æåˆ°å·¥å…·

**è§£å†³æ–¹æ¡ˆ:**
- æ”¹è¿›å·¥å…·çš„æ–‡æ¡£å­—ç¬¦ä¸²
- åœ¨ç³»ç»Ÿæç¤ºè¯ä¸­è¯´æ˜å¯ç”¨å·¥å…·

### Q3: ç»“æ„åŒ–è¾“å‡ºæ ¼å¼é”™è¯¯

**åŸå› :**
- Schema å®šä¹‰ä¸å¤Ÿæ¸…æ™°
- Field æè¿°ä¸è¯¦ç»†

**è§£å†³æ–¹æ¡ˆ:**
- ä½¿ç”¨æ›´è¯¦ç»†çš„ Field æè¿°
- æ·»åŠ ç¤ºä¾‹å’Œçº¦æŸ
- ä½¿ç”¨ Pydantic éªŒè¯å™¨

### Q4: æµå¼å¤„ç†ä¸­æ–­

**åŸå› :**
- ç½‘ç»œé—®é¢˜
- è¶…æ—¶è®¾ç½®è¿‡çŸ­

**è§£å†³æ–¹æ¡ˆ:**
- å¢åŠ  timeout å‚æ•°
- æ·»åŠ é‡è¯•é€»è¾‘
- å¤„ç†å¼‚å¸¸

## ğŸ“– å‚è€ƒèµ„æº

- [LangChain å®˜æ–¹æ–‡æ¡£](https://docs.langchain.com/oss/python/langchain/models)
- [æ™ºè°± AI æ–‡æ¡£](https://open.bigmodel.cn/dev/api)
- [Pydantic æ–‡æ¡£](https://docs.pydantic.dev/)
- [åŸå§‹æ€»ç»“æ–‡æ¡£](../langchain-docs/LangChain_Models_è¯¦ç»†æŒ‡å—.md)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request!

## ğŸ“„ è®¸å¯è¯

MIT License

---

**ä½œè€…**: åŸºäº LangChain å®˜æ–¹æ–‡æ¡£æ”¹ç¼–
**ç‰ˆæœ¬**: 1.0
**æ›´æ–°æ—¥æœŸ**: 2025-01-23
