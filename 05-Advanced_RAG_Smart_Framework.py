#!/usr/bin/env python3
"""
LangChain v1.0 - RAG æ™ºèƒ½æ¡†æ¶
=====================================

åŸºäºæ™ºèƒ½å¯¹è¯åŠ©æ‰‹æ¡ˆä¾‹ï¼Œä½¿ç”¨ LangChain v1.0 æ„å»ºçš„å…¨æ–° RAG æ™ºèƒ½æ¡†æ¶

æ ¸å¿ƒç‰¹æ€§ï¼š
- ğŸ“š å¤šæºæ–‡æ¡£åŠ è½½ï¼ˆç½‘é¡µã€PDFã€æ–‡æœ¬ï¼‰
- ğŸ” æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
- ğŸ¤– æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
- ğŸ”§ åŠ¨æ€å·¥å…·é›†æˆ
- ğŸ’¾ ä¼šè¯è®°å¿†ç®¡ç†
- ğŸ¯ å¯é…ç½®æ£€ç´¢å‚æ•°
- ğŸ“Š æ€§èƒ½ç›‘æ§
- ğŸŒ å¤šè¯­è¨€æ”¯æŒ

åŸºäº GLM-4.6 æ¨¡å‹
"""

from __future__ import annotations

import os
import json
import time
from dataclasses import dataclass, field
from typing import (
    Any,
    Dict,
    List,
    Optional,
    Union,
    Callable,
    Literal,
)

import dotenv
from langchain_community.chat_models import ChatZhipuAI
from langchain_community.document_loaders import (
    WebBaseLoader,
    TextLoader,
    PyPDFLoader,
)
from langchain_community.retrievers import TFIDFRetriever
from langchain_community.vectorstores import FAISS, Chroma
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
)
from langchain_core.documents import Document
from langchain_core.tools import tool
from langchain_core.output_parsers import StrOutputParser
from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware, ModelRequest
from langchain.agents.middleware.types import ModelResponse

# åŠ è½½ç¯å¢ƒå˜é‡
dotenv.load_dotenv(dotenv_path="../.env")


# ========== é…ç½®ç±» ==========

@dataclass
class RAGConfig:
    """RAG æ¡†æ¶é…ç½®"""
    # æ¨¡å‹é…ç½®
    model_name: str = "glm-4.6"
    temperature: float = 0.3
    max_tokens: int = 2000

    # æ£€ç´¢é…ç½®
    chunk_size: int = 1000
    chunk_overlap: int = 200
    k: int = 5  # æ£€ç´¢æ–‡æ¡£æ•°é‡

    # å‘é‡æ•°æ®åº“
    vector_store_type: Literal["faiss", "chroma"] = "faiss"
    embedding_model: str = "text-embedding-3-large"

    # æ£€ç´¢ç­–ç•¥
    search_type: Literal["similarity", "mmr", "hybrid"] = "similarity"
    fetch_k: int = 20
    lambda_mult: float = 0.5

    # è®°å¿†é…ç½®
    max_history: int = 10
    session_timeout: int = 3600  # ç§’


@dataclass
class QueryContext:
    """æŸ¥è¯¢ä¸Šä¸‹æ–‡"""
    session_id: str
    user_id: str
    query: str
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class RetrievalMetrics:
    """æ£€ç´¢æŒ‡æ ‡"""
    query_time: float = 0.0
    docs_retrieved: int = 0
    relevance_score: float = 0.0
    context_used: bool = False


# ========== æ–‡æ¡£å¤„ç†å™¨ ==========

class DocumentProcessor:
    """æ–‡æ¡£åŠ è½½å’Œé¢„å¤„ç†"""

    @staticmethod
    def load_web_docs(urls: List[str]) -> List[Document]:
        """åŠ è½½ç½‘é¡µæ–‡æ¡£"""
        docs = []
        for url in urls:
            try:
                loader = WebBaseLoader(url)
                docs.extend(loader.load())
                print(f"âœ… åŠ è½½ç½‘é¡µ: {url}")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {url}: {e}")
        return docs

    @staticmethod
    def load_text_docs(file_paths: List[str]) -> List[Document]:
        """åŠ è½½æ–‡æœ¬æ–‡æ¡£"""
        docs = []
        for path in file_paths:
            try:
                loader = TextLoader(path, encoding="utf-8")
                docs.extend(loader.load())
                print(f"âœ… åŠ è½½æ–‡æœ¬: {path}")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {path}: {e}")
        return docs

    @staticmethod
    def load_pdf_docs(file_paths: List[str]) -> List[Document]:
        """åŠ è½½ PDF æ–‡æ¡£"""
        docs = []
        for path in file_paths:
            try:
                loader = PyPDFLoader(path)
                docs.extend(loader.load())
                print(f"âœ… åŠ è½½ PDF: {path}")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {path}: {e}")
        return docs

    @staticmethod
    def split_documents(
        docs: List[Document],
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
    ) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " "],
        )
        return splitter.split_documents(docs)

    @staticmethod
    def create_sample_docs() -> List[Document]:
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ï¼ˆå…³äº AIï¼‰"""
        sample_texts = [
            """äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
            AI çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿæ„ŸçŸ¥ã€ç†è§£ã€å­¦ä¹ å’Œå†³ç­–ã€‚

            ä¸»è¦åº”ç”¨é¢†åŸŸåŒ…æ‹¬ï¼š
            1. è‡ªç„¶è¯­è¨€å¤„ç† - ä½¿è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€
            2. è®¡ç®—æœºè§†è§‰ - ä½¿è®¡ç®—æœºèƒ½å¤Ÿè§£é‡Šå’Œç†è§£è§†è§‰ä¿¡æ¯
            3. æœºå™¨å­¦ä¹  - è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ è€Œä¸éœ€è¦æ˜ç¡®ç¼–ç¨‹
            4. æœºå™¨äººæŠ€æœ¯ - å°†æ™ºèƒ½é›†æˆåˆ°ç‰©ç†æœºå™¨äººä¸­

            AI çš„å‘å±•ç»å†äº†å¤šä¸ªé˜¶æ®µï¼šç¬¦å· AIã€è¿æ¥ä¸»ä¹‰ã€æ·±åº¦å­¦ä¹ ç­‰ã€‚
            å½“å‰çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTã€ChatGLMï¼‰ä»£è¡¨äº† AI å‘å±•çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚""",

            """æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨ç»Ÿè®¡æŠ€æœ¯è®©è®¡ç®—æœºä»æ•°æ®ä¸­"å­¦ä¹ "ï¼Œ
            è€Œä¸éœ€è¦æ˜ç¡®ç¼–ç¨‹ã€‚æœºå™¨å­¦ä¹ åˆ†ä¸ºä¸‰ç§ä¸»è¦ç±»å‹ï¼š

            1. ç›‘ç£å­¦ä¹  - ä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹
            2. æ— ç›‘ç£å­¦ä¹  - å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼
            3. å¼ºåŒ–å­¦ä¹  - é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä¼˜ç­–ç•¥

            æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œã€‚
            å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚""",

            """è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦çš„äº¤å‰é¢†åŸŸï¼Œ
            è‡´åŠ›äºè®©è®¡ç®—æœºç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚

            NLP çš„ä¸»è¦ä»»åŠ¡åŒ…æ‹¬ï¼š
            - æ–‡æœ¬åˆ†ç±»ï¼šè‡ªåŠ¨å°†æ–‡æœ¬åˆ†é…åˆ°é¢„å®šä¹‰çš„ç±»åˆ«
            - æƒ…æ„Ÿåˆ†æï¼šè¯†åˆ«æ–‡æœ¬ä¸­çš„æƒ…æ„Ÿå€¾å‘
            - æœºå™¨ç¿»è¯‘ï¼šå°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€
            - é—®ç­”ç³»ç»Ÿï¼šç†è§£é—®é¢˜å¹¶æä¾›å‡†ç¡®çš„ç­”æ¡ˆ
            - æ–‡æœ¬æ‘˜è¦ï¼šç”Ÿæˆæ–‡æ¡£çš„ç®€æ´æ‘˜è¦

            ç°ä»£ NLP å¹¿æ³›ä½¿ç”¨ Transformer æ¶æ„å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œ
            è¿™äº›æ¨¡å‹åœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“ä¸Šè®­ç»ƒï¼Œç„¶åé’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚""",
        ]

        docs = []
        for i, text in enumerate(sample_texts):
            docs.append(
                Document(
                    page_content=text,
                    metadata={
                        "source": f"sample_doc_{i}",
                        "title": f"AI çŸ¥è¯†æ–‡æ¡£ {i+1}",
                    },
                )
            )
        return docs


# ========== å‘é‡å­˜å‚¨ç®¡ç† ==========

class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.vector_store = None
        self.retriever = None
        self.hybrid_retriever = None

    def create_vector_store(
        self,
        documents: List[Document],
        embeddings: Optional[Any] = None,
    ) -> Any:
        """åˆ›å»ºå‘é‡å­˜å‚¨"""
        # ä½¿ç”¨ OpenAI åµŒå…¥ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        if embeddings is None:
            embeddings = OpenAIEmbeddings(
                model="text-embedding-3-large",
                openai_api_key=os.getenv("OPENAI_API_KEY"),
                openai_api_base=os.getenv("OPENAI_BASE_URL"),
            )

        # åˆ›å»ºå‘é‡å­˜å‚¨
        if self.config.vector_store_type == "faiss":
            self.vector_store = FAISS.from_documents(documents, embeddings)
        else:
            self.vector_store = Chroma.from_documents(documents, embeddings)

        print(f"âœ… å‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ: {self.config.vector_store_type}")
        return self.vector_store

    def create_retrievers(self) -> None:
        """åˆ›å»ºæ£€ç´¢å™¨"""
        if not self.vector_store:
            raise ValueError("è¯·å…ˆåˆ›å»ºå‘é‡å­˜å‚¨")

        # 1. åŸºç¡€å‘é‡æ£€ç´¢
        if self.config.search_type == "similarity":
            self.retriever = self.vector_store.as_retriever(
                search_kwargs={"k": self.config.k}
            )
        elif self.config.search_type == "mmr":
            self.retriever = self.vector_store.as_retriever(
                search_type="mmr",
                search_kwargs={
                    "k": self.config.k,
                    "fetch_k": self.config.fetch_k,
                    "lambda_mult": self.config.lambda_mult,
                }
            )
        else:  # hybrid
            # åˆ›å»ºæ··åˆæ£€ç´¢
            vector_retriever = self.vector_store.as_retriever(
                search_kwargs={"k": self.config.k}
            )
            # TFIDF æ£€ç´¢å™¨
            self.retriever = vector_retriever

        # 2. åˆ›å»ºæ··åˆæ£€ç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
        self.hybrid_retriever = self.vector_store.as_retriever(
            search_kwargs={"k": self.config.k * 2}
        )

        print(f"âœ… æ£€ç´¢å™¨åˆ›å»ºå®Œæˆ: {self.config.search_type}")

    def get_relevant_docs(
        self,
        query: str,
        use_hybrid: bool = False,
    ) -> List[Document]:
        """è·å–ç›¸å…³æ–‡æ¡£"""
        start_time = time.time()
        retriever = self.hybrid_retriever if use_hybrid else self.retriever
        docs = retriever.invoke(query)
        query_time = time.time() - start_time

        metrics = RetrievalMetrics(
            query_time=query_time,
            docs_retrieved=len(docs),
            relevance_score=1.0,
            context_used=use_hybrid,
        )

        print(f"ğŸ“Š æ£€ç´¢æŒ‡æ ‡:")
        print(f"   - æŸ¥è¯¢æ—¶é—´: {query_time:.3f}s")
        print(f"   - æ£€ç´¢æ–‡æ¡£æ•°: {len(docs)}")
        print(f"   - ç­–ç•¥: {'æ··åˆæ£€ç´¢' if use_hybrid else self.config.search_type}")

        return docs, metrics


# ========== å·¥å…· ==========

@tool
def rag_search(query: str) -> str:
    """RAG æ£€ç´¢å·¥å…· - æœç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³ä¿¡æ¯

    Args:
        query: æŸ¥è¯¢å…³é”®è¯æˆ–é—®é¢˜
    """
    return f"[RAGæœç´¢] æ­£åœ¨æœç´¢: {query}"


@tool
def web_search(query: str) -> str:
    """ç½‘ç»œæœç´¢å·¥å…· - æœç´¢æœ€æ–°ä¿¡æ¯

    Args:
        query: æœç´¢å…³é”®è¯
    """
    return f"[ç½‘ç»œæœç´¢] æ­£åœ¨æœç´¢: {query}"


@tool
def hybrid_search(query: str, use_rag: bool = True, use_web: bool = True) -> str:
    """æ··åˆæœç´¢å·¥å…· - ç»“åˆ RAG å’Œç½‘ç»œæœç´¢

    Args:
        query: æŸ¥è¯¢é—®é¢˜
        use_rag: æ˜¯å¦ä½¿ç”¨ RAG æœç´¢
        use_web: æ˜¯å¦ä½¿ç”¨ç½‘ç»œæœç´¢
    """
    strategies = []
    if use_rag:
        strategies.append("RAG")
    if use_web:
        strategies.append("ç½‘ç»œ")
    return f"[æ··åˆæœç´¢] ä½¿ç”¨ {', '.join(strategies)} æœç´¢: {query}"


@tool
def query_analyzer(query: str) -> str:
    """æŸ¥è¯¢åˆ†æå·¥å…· - åˆ†ææŸ¥è¯¢æ„å›¾å’Œå¤æ‚åº¦

    Args:
        query: å¾…åˆ†æçš„æŸ¥è¯¢
    """
    analysis = {
        "length": len(query),
        "keywords": query.split()[:5],
        "intent": "question" if "?" in query or "ä»€ä¹ˆ" in query else "command",
        "domain": "AI/ML" if any(kw in query.lower() for kw in ["ai", "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "nlp"]) else "general",
    }
    return f"[æŸ¥è¯¢åˆ†æ] {json.dumps(analysis, ensure_ascii=False, indent=2)}"


# ========== ä¸­é—´ä»¶ ==========

class RAGMetricsMiddleware(AgentMiddleware):
    """RAG æŒ‡æ ‡ç›‘æ§ä¸­é—´ä»¶"""

    def __init__(self):
        self.metrics = {
            "total_queries": 0,
            "rag_used": 0,
            "web_used": 0,
            "avg_response_time": 0.0,
        }
        self.query_history = []

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        start_time = time.time()

        # æ·»åŠ ç³»ç»Ÿæç¤º
        system_msg = SystemMessage(
            content="""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„ RAG åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æŒ‡å—å›ç­”é—®é¢˜ï¼š

1. ä¼˜å…ˆä½¿ç”¨ RAG æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œå¯ä»¥ä½¿ç”¨ç½‘ç»œæœç´¢è¡¥å……
3. å›ç­”è¦å‡†ç¡®ã€æ¸…æ™°ã€æœ‰æ¡ç†
4. å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·å¦‚å®è¯´æ˜
5. å¼•ç”¨æ¥æºæ—¶ï¼Œä½¿ç”¨ [æ–‡æ¡£ç¼–å·] çš„æ ¼å¼

ä½¿ç”¨æ ¼å¼ï¼š
- RAG ç­”æ¡ˆï¼š[åŸºäºæ£€ç´¢æ–‡æ¡£çš„å›ç­”]
- è¡¥å……ä¿¡æ¯ï¼š[åŸºäºç½‘ç»œæœç´¢çš„å›ç­”]ï¼ˆå¦‚æœ‰ï¼‰
"""
        )

        # æ’å…¥ç³»ç»Ÿæ¶ˆæ¯
        request.messages.insert(0, system_msg)

        # æ‰§è¡Œè¯·æ±‚
        response = handler(request)

        # è®°å½•æŒ‡æ ‡
        query_time = time.time() - start_time
        self.metrics["total_queries"] += 1
        self.metrics["avg_response_time"] = (
            (self.metrics["avg_response_time"] * (self.metrics["total_queries"] - 1) + query_time)
            / self.metrics["total_queries"]
        )

        return response

    def get_metrics(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æŒ‡æ ‡"""
        return self.metrics.copy()


# ========== æ ¸å¿ƒ RAG æ¡†æ¶ ==========

class RAGSmartFramework:
    """RAG æ™ºèƒ½æ¡†æ¶ - æ ¸å¿ƒç±»"""

    def __init__(self, config: Optional[RAGConfig] = None):
        self.config = config or RAGConfig()
        self.docs = []
        self.vector_manager = VectorStoreManager(self.config)
        self.metrics_middleware = RAGMetricsMiddleware()
        self.tools = [
            rag_search,
            web_search,
            hybrid_search,
            query_analyzer,
        ]

        # åˆ›å»ºæ¨¡å‹
        self.llm = ChatZhipuAI(
            model=self.config.model_name,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            api_key=os.getenv("ZHIPUAI_API_KEY"),
        )

        # åˆ›å»º Agent
        self._create_agent()

        print("ğŸš€ RAG æ™ºèƒ½æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")

    def _create_agent(self):
        """åˆ›å»º Agent"""
        try:
            self.agent = create_agent(
                model=self.llm,
                tools=self.tools,
                middleware=[self.metrics_middleware],
            )
            print("âœ… Agent åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ Agent åˆ›å»ºå¤±è´¥: {e}")
            raise

    def load_knowledge_base(
        self,
        web_urls: Optional[List[str]] = None,
        text_files: Optional[List[str]] = None,
        pdf_files: Optional[List[str]] = None,
        use_sample_docs: bool = True,
    ):
        """åŠ è½½çŸ¥è¯†åº“"""
        docs = []

        # åŠ è½½å„ç§æ–‡æ¡£
        if web_urls:
            docs.extend(DocumentProcessor.load_web_docs(web_urls))
        if text_files:
            docs.extend(DocumentProcessor.load_text_docs(text_files))
        if pdf_files:
            docs.extend(DocumentProcessor.load_pdf_docs(pdf_files))
        if use_sample_docs and not docs:
            docs = DocumentProcessor.create_sample_docs()
            print("ğŸ“š ä½¿ç”¨ç¤ºä¾‹ AI æ–‡æ¡£")

        if not docs:
            raise ValueError("æ²¡æœ‰åŠ è½½ä»»ä½•æ–‡æ¡£")

        # åˆ†å‰²æ–‡æ¡£
        self.docs = DocumentProcessor.split_documents(
            docs,
            self.config.chunk_size,
            self.config.chunk_overlap,
        )
        print(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆ: {len(self.docs)} ä¸ªæ–‡æ¡£å—")

        # åˆ›å»ºå‘é‡å­˜å‚¨å’Œæ£€ç´¢å™¨
        self.vector_manager.create_vector_store(self.docs)
        self.vector_manager.create_retrievers()

    def query(
        self,
        question: str,
        session_id: str = "default",
        use_hybrid: bool = True,
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        context = QueryContext(
            session_id=session_id,
            user_id="user",
            query=question,
        )

        # è·å–ç›¸å…³æ–‡æ¡£
        docs, metrics = self.vector_manager.get_relevant_docs(
            question,
            use_hybrid=use_hybrid,
        )

        # æ„å»ºä¸Šä¸‹æ–‡
        context_str = "\n\n".join([
            f"[æ–‡æ¡£ {i+1}] {doc.page_content}"
            for i, doc in enumerate(docs)
        ])

        # å¢å¼ºæç¤º
        enhanced_question = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†åº“ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

{context_str}

é—®é¢˜ï¼š{question}

è¯·æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¿¡æ¯å›ç­”ï¼Œå¹¶æ³¨æ˜ä½¿ç”¨çš„æ–‡æ¡£ç¼–å·ã€‚"""

        # æ‰§è¡ŒæŸ¥è¯¢
        start_time = time.time()
        try:
            result = self.agent.invoke({
                "messages": [HumanMessage(content=enhanced_question)]
            })
            response_time = time.time() - start_time

            # æ›´æ–°æŒ‡æ ‡
            if use_hybrid:
                self.metrics_middleware.metrics["rag_used"] += 1

            return {
                "question": question,
                "answer": result.get("output", str(result)),
                "context_docs": [doc.metadata for doc in docs],
                "metrics": {
                    **metrics.__dict__,
                    "response_time": response_time,
                    "total_time": metrics.query_time + response_time,
                },
                "rag_metrics": self.metrics_middleware.get_metrics(),
            }
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "question": question,
                "answer": f"æŸ¥è¯¢å‡ºé”™: {str(e)}",
                "error": str(e),
            }

    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æŸ¥è¯¢"""
        results = []
        for q in questions:
            result = self.query(q)
            results.append(result)
        return results

    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        rag_metrics = self.metrics_middleware.get_metrics()
        return {
            "RAG æ¡†æ¶ç»Ÿè®¡": {
                "æ€»æŸ¥è¯¢æ•°": rag_metrics["total_queries"],
                "RAG ä½¿ç”¨æ¬¡æ•°": rag_metrics["rag_used"],
                "å¹³å‡å“åº”æ—¶é—´": f"{rag_metrics['avg_response_time']:.3f}s",
            },
            "æ£€ç´¢é…ç½®": {
                "æ¨¡å‹": self.config.model_name,
                "æ£€ç´¢ç­–ç•¥": self.config.search_type,
                "æ–‡æ¡£å—å¤§å°": self.config.chunk_size,
                "æ£€ç´¢æ•°é‡": self.config.k,
            },
        }


# ========== æ¼”ç¤ºå‡½æ•° ==========

def demo_basic_rag():
    """æ¼”ç¤ºåŸºæœ¬ RAG åŠŸèƒ½"""
    print("=" * 70)
    print("ğŸ“š åŸºæœ¬ RAG æ¼”ç¤º")
    print("=" * 70)

    config = RAGConfig(
        chunk_size=500,
        chunk_overlap=100,
        k=3,
    )

    framework = RAGSmartFramework(config)
    framework.load_knowledge_base(use_sample_docs=True)

    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
        "è‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸»è¦ä»»åŠ¡æ˜¯ä»€ä¹ˆï¼Ÿ",
    ]

    print(f"\nğŸ” æ‰§è¡Œ {len(questions)} ä¸ªæŸ¥è¯¢...\n")

    for i, q in enumerate(questions, 1):
        print(f"ã€æŸ¥è¯¢ {i}ã€‘{q}")
        result = framework.query(q)
        print(f"\nç­”æ¡ˆï¼š\n{result['answer'][:200]}...")
        print(f"\nğŸ“Š æŒ‡æ ‡: æ£€ç´¢{result['metrics']['docs_retrieved']}ä¸ªæ–‡æ¡£, "
              f"è€—æ—¶{result['metrics']['total_time']:.3f}s")
        print("-" * 70)


def demo_hybrid_search():
    """æ¼”ç¤ºæ··åˆæœç´¢"""
    print("\n" + "=" * 70)
    print("ğŸ”„ æ··åˆæœç´¢æ¼”ç¤º")
    print("=" * 70)

    config = RAGConfig(
        search_type="mmr",
        k=5,
    )

    framework = RAGSmartFramework(config)
    framework.load_knowledge_base(use_sample_docs=True)

    # æµ‹è¯•ä¸åŒç­–ç•¥
    question = "AI çš„ä¸»è¦åº”ç”¨é¢†åŸŸæœ‰å“ªäº›ï¼Ÿ"

    print(f"\nğŸ“ é—®é¢˜ï¼š{question}\n")

    # 1. çº¯å‘é‡æ£€ç´¢
    print("ã€ç­–ç•¥ 1ã€‘å‘é‡æ£€ç´¢")
    result1 = framework.query(question, use_hybrid=False)
    print(f"ç­”æ¡ˆï¼š{result1['answer'][:150]}...")

    # 2. æ··åˆæ£€ç´¢
    print("\nã€ç­–ç•¥ 2ã€‘æ··åˆæ£€ç´¢")
    result2 = framework.query(question, use_hybrid=True)
    print(f"ç­”æ¡ˆï¼š{result2['answer'][:150]}...")

    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"   å‘é‡æ£€ç´¢: {result1['metrics']['query_time']:.3f}s")
    print(f"   æ··åˆæ£€ç´¢: {result2['metrics']['query_time']:.3f}s")


def demo_multi_source():
    """æ¼”ç¤ºå¤šæºæ•°æ®åŠ è½½"""
    print("\n" + "=" * 70)
    print("ğŸ“¡ å¤šæºæ•°æ®åŠ è½½æ¼”ç¤º")
    print("=" * 70)

    config = RAGConfig(k=5)
    framework = RAGSmartFramework(config)

    # åŠ è½½ç¤ºä¾‹æ•°æ®ï¼ˆå¯ä»¥æ‰©å±•åˆ°ç½‘é¡µã€PDFç­‰ï¼‰
    framework.load_knowledge_base(use_sample_docs=True)

    print(f"\nâœ… çŸ¥è¯†åº“å·²åŠ è½½: {len(framework.docs)} ä¸ªæ–‡æ¡£å—")


def demo_performance_monitoring():
    """æ¼”ç¤ºæ€§èƒ½ç›‘æ§"""
    print("\n" + "=" * 70)
    print("ğŸ“Š æ€§èƒ½ç›‘æ§æ¼”ç¤º")
    print("=" * 70)

    config = RAGConfig(k=3)
    framework = RAGSmartFramework(config)
    framework.load_knowledge_base(use_sample_docs=True)

    # æ‰§è¡Œå¤šä¸ªæŸ¥è¯¢
    questions = [
        "ä»€ä¹ˆæ˜¯ç›‘ç£å­¦ä¹ ï¼Ÿ",
        "æ— ç›‘ç£å­¦ä¹ çš„ä½œç”¨ï¼Ÿ",
        "å¼ºåŒ–å­¦ä¹ çš„åŸç†ï¼Ÿ",
    ]

    print("\nğŸ”„ æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢...")
    for q in questions:
        framework.query(q)

    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    report = framework.get_performance_report()
    print("\nğŸ“ˆ æ€§èƒ½æŠ¥å‘Š:")
    for category, metrics in report.items():
        print(f"\n{category}:")
        for key, value in metrics.items():
            print(f"  â€¢ {key}: {value}")


def explain_rag_architecture():
    """è§£é‡Š RAG æ¶æ„"""
    print("\n" + "=" * 70)
    print("ğŸ—ï¸ RAG æ™ºèƒ½æ¡†æ¶æ¶æ„è¯¦è§£")
    print("=" * 70)

    print("""
ğŸ”§ æ ¸å¿ƒç»„ä»¶ï¼š

1. DocumentProcessorï¼ˆæ–‡æ¡£å¤„ç†å™¨ï¼‰
   - åŠ è½½ï¼šç½‘é¡µã€æ–‡æœ¬ã€PDF
   - é¢„å¤„ç†ï¼šæ¸…æ´—ã€åˆ†å—ã€é‡å 
   - ç¤ºä¾‹æ•°æ®ï¼šAI çŸ¥è¯†åº“

2. VectorStoreManagerï¼ˆå‘é‡å­˜å‚¨ç®¡ç†ï¼‰
   - æ”¯æŒï¼šFAISSã€Chroma
   - ç­–ç•¥ï¼šSimilarityã€MMRã€Hybrid
   - æ£€ç´¢ï¼šå‘é‡æ£€ç´¢ã€å…³é”®è¯æ£€ç´¢ã€æ··åˆæ£€ç´¢

3. RAGSmartFrameworkï¼ˆæ ¸å¿ƒæ¡†æ¶ï¼‰
   - é…ç½®ï¼šRAGConfig
   - æ¨¡å‹ï¼šChatZhipuAI (GLM-4.6)
   - å·¥å…·ï¼šRAGã€ç½‘ç»œã€æ··åˆã€æŸ¥è¯¢åˆ†æ
   - ä¸­é—´ä»¶ï¼šæŒ‡æ ‡ç›‘æ§

4. AgentExecutorï¼ˆæ™ºèƒ½ä»£ç†ï¼‰
   - è‡ªåŠ¨é€‰æ‹©å·¥å…·
   - ä¸Šä¸‹æ–‡ç†è§£
   - ç­”æ¡ˆç”Ÿæˆ

ğŸš€ å·¥ä½œæµç¨‹ï¼š
æ•°æ®åŠ è½½ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ â†’ å·¥å…·é€‰æ‹© â†’ ç­”æ¡ˆç”Ÿæˆ â†’ æŒ‡æ ‡ç›‘æ§

ğŸ’¡ åˆ›æ–°ç‰¹æ€§ï¼š
âœ… æ”¯æŒå¤šæºæ•°æ®ï¼ˆç½‘é¡µã€PDFã€æ–‡æœ¬ï¼‰
âœ… æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
âœ… å¯é…ç½®å‚æ•°ï¼ˆchunk_sizeã€kå€¼ã€ç­–ç•¥ï¼‰
âœ… æ€§èƒ½ç›‘æ§ï¼ˆæŸ¥è¯¢æ—¶é—´ã€æ–‡æ¡£æ•°ã€æˆåŠŸç‡ï¼‰
âœ… æ™ºèƒ½å·¥å…·é€‰æ‹©ï¼ˆRAG vs ç½‘ç»œæœç´¢ï¼‰
âœ… ä¸­é—´ä»¶æ”¯æŒï¼ˆæ‰©å±•æ€§ï¼‰
""")


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("ğŸš€ LangChain v1.0 - RAG æ™ºèƒ½æ¡†æ¶æ¼”ç¤º")
    print("=" * 80)
    print("""
âœ¨ æ¼”ç¤ºå†…å®¹ï¼š
1. ğŸ“š åŸºæœ¬ RAG åŠŸèƒ½ - å‘é‡æ£€ç´¢å’Œé—®ç­”
2. ğŸ”„ æ··åˆæœç´¢ - ç»“åˆå‘é‡å’Œå…³é”®è¯
3. ğŸ“¡ å¤šæºæ•°æ® - æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼
4. ğŸ“Š æ€§èƒ½ç›‘æ§ - å®æ—¶æŒ‡æ ‡è¿½è¸ª

åŸºäº GLM-4.6 æ¨¡å‹
    """)

    try:
        # 1. æ¶æ„è¯´æ˜
        explain_rag_architecture()

        # 2. åŸºæœ¬æ¼”ç¤º
        demo_basic_rag()

        # 3. æ··åˆæœç´¢
        demo_hybrid_search()

        # 4. å¤šæºæ•°æ®
        demo_multi_source()

        # 5. æ€§èƒ½ç›‘æ§
        demo_performance_monitoring()

        print("\n" + "=" * 70)
        print("ğŸ‰ æ‰€æœ‰ RAG æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 70)
        print("""
ğŸ’¡ ä¸‹ä¸€æ­¥ï¼š
1. å°è¯•åŠ è½½è‡ªå·±çš„æ–‡æ¡£ï¼ˆç½‘é¡µã€PDFã€æ–‡æœ¬ï¼‰
2. è°ƒæ•´æ£€ç´¢å‚æ•°ä¼˜åŒ–æ€§èƒ½
3. æ‰©å±•å·¥å…·é›†æ”¯æŒæ›´å¤šåŠŸèƒ½
4. åœ¨å®é™…é¡¹ç›®ä¸­éƒ¨ç½²ä½¿ç”¨
        """)

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åºã€‚")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
