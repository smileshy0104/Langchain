#!/usr/bin/env python3
"""
GLM-4.6 + LangChain PromptTemplate è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå„ç§æç¤ºè¯æ¨¡æ¿çš„ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µ
"""

import os
import dotenv
from typing import List, Dict, Any
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.prompts import (
    ChatPromptTemplate,
    PromptTemplate,
    FewShotPromptTemplate,
    FewShotChatMessagePromptTemplate,
    MessagesPlaceholder
)
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser

# åŠ è½½ç¯å¢ƒå˜é‡ - ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½.envæ–‡ä»¶
dotenv.load_dotenv(dotenv_path="../.env")

# æ£€æŸ¥APIå¯†é’¥
api_key = os.getenv("ZHIPUAI_API_KEY")
if not api_key or api_key == "your-zhipu-api-key-here":
    print("âŒ é”™è¯¯ï¼šè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ZHIPUAI_API_KEY")
    exit(1)

# åˆå§‹åŒ–GLMæ¨¡å‹
def get_glm_model(temperature: float = 0.7):
    """è·å–GLMæ¨¡å‹å®ä¾‹"""
    return ChatZhipuAI(
        model="glm-4.6",
        temperature=temperature,
        api_key=api_key
    )

def basic_template_example():
    """åŸºç¡€æ¨¡æ¿ç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ“ åŸºç¡€æ¨¡æ¿ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # 1. ç®€å•å­—ç¬¦ä¸²æ¨¡æ¿
    print("\n1ï¸âƒ£ ç®€å•å­—ç¬¦ä¸²æ¨¡æ¿")
    prompt_template = PromptTemplate(
        input_variables=["topic", "style"],
        template="è¯·å†™ä¸€ç¯‡å…³äº{topic}çš„{style}é£æ ¼çš„æ–‡ç« ã€‚"
    )

    # æ ¼å¼åŒ–æç¤ºè¯
    prompt = prompt_template.format(topic="äººå·¥æ™ºèƒ½", style="ç§‘æŠ€")
    print(f"ğŸ“‹ ç”Ÿæˆçš„æç¤ºè¯: {prompt}")

    # è°ƒç”¨æ¨¡å‹
    response = model.invoke(prompt)
    print(f"ğŸ¤– æ¨¡å‹å›å¤: {response.content[:200]}...\n")

    # 2. èŠå¤©æ¨¡æ¿
    print("2ï¸âƒ£ èŠå¤©æ¨¡æ¿")
    chat_prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ª{role}ã€‚è¯·ç”¨{tone}çš„è¯­è°ƒå›ç­”ï¼š{question}"
    )

    formatted_prompt = chat_prompt.format_messages(
        role="ä¸“ä¸šç¨‹åºå‘˜",
        tone="å‹å¥½è€å¿ƒ",
        question="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"
    )

    response = model.invoke(formatted_prompt)
    print(f"ğŸ¤– æ¨¡å‹å›å¤: {response.content[:200]}...\n")

def multi_variable_template():
    """å¤šå˜é‡æ¨¡æ¿ç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ”§ å¤šå˜é‡æ¨¡æ¿ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # å¤æ‚çš„å¤šå˜é‡æ¨¡æ¿
    complex_template = """
ä»»åŠ¡æè¿°ï¼š{task}
ç›®æ ‡å—ä¼—ï¼š{audience}
å†…å®¹ç±»å‹ï¼š{content_type}
å­—æ•°è¦æ±‚ï¼š{word_count}
ç‰¹æ®Šè¦æ±‚ï¼š{requirements}

è¯·æ ¹æ®ä»¥ä¸Šè¦æ±‚ç”Ÿæˆå†…å®¹ã€‚
    """.strip()

    prompt_template = PromptTemplate(
        input_variables=["task", "audience", "content_type", "word_count", "requirements"],
        template=complex_template
    )

    # å¡«å……å˜é‡
    prompt = prompt_template.format(
        task="å†™ä¸€ç¯‡æŠ€æœ¯åšå®¢",
        audience="ç¼–ç¨‹åˆå­¦è€…",
        content_type="æ•™ç¨‹",
        word_count="800-1000å­—",
        requirements="åŒ…å«ä»£ç ç¤ºä¾‹ï¼Œè¯­è¨€é€šä¿—æ˜“æ‡‚ï¼Œç»“æ„æ¸…æ™°"
    )

    print(f"ğŸ“‹ ç”Ÿæˆçš„æç¤ºè¯:\n{prompt}\n")

    response = model.invoke(prompt)
    print(f"ğŸ¤– æ¨¡å‹å›å¤: {response.content[:300]}...\n")

def chat_message_template():
    """èŠå¤©æ¶ˆæ¯æ¨¡æ¿ç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ’¬ èŠå¤©æ¶ˆæ¯æ¨¡æ¿ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # 1. å¤šè§’è‰²èŠå¤©æ¨¡æ¿
    print("1ï¸âƒ£ å¤šè§’è‰²èŠå¤©æ¨¡æ¿")
    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œä¸“é—¨{specialty}ã€‚"),
        ("human", "ä½ å¥½ï¼æˆ‘éœ€è¦{help_type}ã€‚"),
        ("ai", "æ‚¨å¥½ï¼æˆ‘æ˜¯ä¸“ä¸šçš„{role}ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚è¯·é—®æ‚¨å…·ä½“éœ€è¦ä»€ä¹ˆ{help_type}ï¼Ÿ"),
        ("human", "{specific_question}")
    ])

    messages = chat_prompt.format_messages(
        role="æ•°æ®ç§‘å­¦å®¶",
        specialty="æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ ",
        help_type="æ•°æ®åˆ†æå»ºè®®",
        specific_question="æˆ‘æœ‰ä¸€ä¸ªé”€å”®æ•°æ®é›†ï¼Œåº”è¯¥å¦‚ä½•å¼€å§‹åˆ†æï¼Ÿ"
    )

    print("ğŸ“‹ ç”Ÿæˆçš„å¯¹è¯æ¨¡æ¿:")
    for i, msg in enumerate(messages, 1):
        print(f"  {i}. [{msg.__class__.__name__}]: {msg.content[:100]}...")

    response = model.invoke(messages)
    print(f"\nğŸ¤– æ¨¡å‹å›å¤: {response.content[:300]}...\n")

    # 2. ä½¿ç”¨ MessagesPlaceholder
    print("2ï¸âƒ£ åŠ¨æ€æ¶ˆæ¯å ä½ç¬¦")
    dynamic_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}ã€‚"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{current_question}")
    ])

    # æ¨¡æ‹Ÿå¯¹è¯å†å²
    history = [
        HumanMessage(content="ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"),
        AIMessage(content="Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½è€Œé—»åã€‚"),
        HumanMessage(content="Pythoné€‚åˆåšä»€ä¹ˆï¼Ÿ")
    ]

    messages = dynamic_prompt.format_messages(
        role="Pythonä¸“å®¶",
        history=history,
        current_question="è¯·æ¨èä¸€äº›Pythonå­¦ä¹ çš„èµ„æºã€‚"
    )

    response = model.invoke(messages)
    print(f"ğŸ¤– æ¨¡å‹å›å¤: {response.content[:300]}...\n")

def few_shot_template():
    """å°‘æ ·æœ¬å­¦ä¹ æ¨¡æ¿ç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ¯ å°‘æ ·æœ¬å­¦ä¹ æ¨¡æ¿ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # 1. æ–‡æœ¬æ ¼å¼å°‘æ ·æœ¬
    print("1ï¸âƒ£ æ–‡æœ¬æ ¼å¼å°‘æ ·æœ¬å­¦ä¹ ")
    examples = [
        {
            "question": "è‹¹æœæ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "è‹¹æœæ˜¯ä¸€ç§æ°´æœï¼Œå¯Œå«ç»´ç”Ÿç´ ï¼Œå£æ„Ÿæ¸…è„†ã€‚"
        },
        {
            "question": "é¦™è•‰æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "é¦™è•‰æ˜¯ä¸€ç§çƒ­å¸¦æ°´æœï¼Œå¯Œå«é’¾å…ƒç´ ï¼Œå£æ„Ÿè½¯ç³¯ã€‚"
        },
        {
            "question": "æ©™å­æ˜¯ä»€ä¹ˆï¼Ÿ",
            "answer": "æ©™å­æ˜¯ä¸€ç§æŸ‘æ©˜ç±»æ°´æœï¼Œå¯Œå«ç»´ç”Ÿç´ Cï¼Œå£æ„Ÿé…¸ç”œã€‚"
        }
    ]

    example_prompt = PromptTemplate(
        input_variables=["question", "answer"],
        template="é—®é¢˜ï¼š{question}\nå›ç­”ï¼š{answer}"
    )

    few_shot_prompt = FewShotPromptTemplate(
        examples=examples,
        example_prompt=example_prompt,
        prefix="è¯·æ ¹æ®ä»¥ä¸‹ç¤ºä¾‹å›ç­”é—®é¢˜ï¼š",
        suffix="é—®é¢˜ï¼š{input}\nå›ç­”ï¼š",
        input_variables=["input"]
    )

    prompt = few_shot_prompt.format(input="è‰è“æ˜¯ä»€ä¹ˆï¼Ÿ")
    print(f"ğŸ“‹ ç”Ÿæˆçš„å°‘æ ·æœ¬æç¤ºè¯:\n{prompt}")

    response = model.invoke(prompt)
    print(f"ğŸ¤– æ¨¡å‹å›å¤: {response.content}\n")

    # 2. èŠå¤©æ ¼å¼å°‘æ ·æœ¬
    print("2ï¸âƒ£ èŠå¤©æ ¼å¼å°‘æ ·æœ¬å­¦ä¹ ")
    chat_examples = [
        {
            "input": "è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’",
            "output": "é€’å½’æ˜¯ä¸€ç§ç¼–ç¨‹æŠ€æœ¯ï¼Œå‡½æ•°ç›´æ¥æˆ–é—´æ¥è°ƒç”¨è‡ªèº«æ¥è§£å†³é—®é¢˜ã€‚"
        },
        {
            "input": "è§£é‡Šä»€ä¹ˆæ˜¯é¢å‘å¯¹è±¡",
            "output": "é¢å‘å¯¹è±¡æ˜¯ä¸€ç§ç¼–ç¨‹èŒƒå¼ï¼Œé€šè¿‡å¯¹è±¡å’Œç±»æ¥ç»„ç»‡ä»£ç ã€‚"
        }
    ]

    chat_example_prompt = ChatPromptTemplate.from_messages([
        ("human", "{input}"),
        ("ai", "{output}")
    ])

    few_shot_chat_prompt = FewShotChatMessagePromptTemplate(
        examples=chat_examples,
        example_prompt=chat_example_prompt
    )

    final_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹è€å¸ˆï¼Œè¯·ç”¨ç®€æ´çš„è¯­è¨€è§£é‡Šç¼–ç¨‹æ¦‚å¿µã€‚"),
        few_shot_chat_prompt,
        ("human", "{input}")
    ])

    messages = final_prompt.format_messages(input="è§£é‡Šä»€ä¹ˆæ˜¯API")
    response = model.invoke(messages)
    print(f"ğŸ¤– æ¨¡å‹å›å¤: {response.content}\n")

def output_parser_template():
    """è¾“å‡ºè§£æå™¨æ¨¡æ¿ç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ” è¾“å‡ºè§£æå™¨æ¨¡æ¿ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # 1. åˆ—è¡¨è¾“å‡ºè§£æ
    print("1ï¸âƒ£ åˆ—è¡¨è¾“å‡ºè§£æ")
    list_parser = CommaSeparatedListOutputParser()

    list_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åŠ©æ‰‹ï¼Œè¯·æä¾›ç›¸å…³çš„å…³é”®è¯åˆ—è¡¨ã€‚"),
        ("human", "è¯·åˆ—å‡º5ä¸ªä¸{topic}ç›¸å…³çš„å…³é”®è¯ã€‚"),
        ("system", "{format_instructions}")
    ])

    chain = list_prompt | model | list_parser

    result = chain.invoke({
        "topic": "æœºå™¨å­¦ä¹ ",
        "format_instructions": list_parser.get_format_instructions()
    })

    print(f"ğŸ“‹ è§£æç»“æœ: {result}")
    print(f"ğŸ“Š ç»“æœç±»å‹: {type(result)}\n")

    # 2. å­—ç¬¦ä¸²è¾“å‡ºè§£æ
    print("2ï¸âƒ£ å­—ç¬¦ä¸²è¾“å‡ºè§£æ")
    string_parser = StrOutputParser()

    string_prompt = ChatPromptTemplate.from_template(
        "è¯·å†™ä¸€ä¸ªå…³äº{subject}çš„{length}ï¼Œè¦æ±‚{style}ã€‚"
    )

    chain = string_prompt | model | string_parser

    result = chain.invoke({
        "subject": "ç¼–ç¨‹",
        "length": "å››è¡Œè¯—",
        "style": "å¯Œæœ‰è¯—æ„"
    })

    print(f"ğŸ“‹ è§£æç»“æœ: {result}")
    print(f"ğŸ“Š ç»“æœç±»å‹: {type(result)}\n")

def conditional_template():
    """æ¡ä»¶æ¨¡æ¿ç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ”„ æ¡ä»¶æ¨¡æ¿ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    def create_conditional_prompt(user_level: str, topic: str) -> ChatPromptTemplate:
        """æ ¹æ®ç”¨æˆ·æ°´å¹³åˆ›å»ºä¸åŒçš„æç¤ºè¯æ¨¡æ¿"""

        if user_level == "beginner":
            system_msg = "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åˆå­¦è€…å¯¼å¸ˆï¼Œè¯·ç”¨æœ€ç®€å•æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šæ¦‚å¿µï¼Œé¿å…ä¸“ä¸šæœ¯è¯­ã€‚"
        elif user_level == "intermediate":
            system_msg = "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹è¿›é˜¶å¯¼å¸ˆï¼Œå¯ä»¥é€‚å½“ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œä½†éœ€è¦è§£é‡Šæ¸…æ¥šã€‚"
        elif user_level == "advanced":
            system_msg = "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹ä¸“å®¶ï¼Œå¯ä»¥è¿›è¡Œæ·±å…¥çš„æŠ€æœ¯è®¨è®ºï¼Œå‡è®¾ç”¨æˆ·æœ‰æ‰å®çš„åŸºç¡€ã€‚"
        else:
            system_msg = "ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹å¯¼å¸ˆï¼Œè¯·æ ¹æ®é—®é¢˜çš„å¤æ‚ç¨‹åº¦è°ƒæ•´è§£é‡Šæ·±åº¦ã€‚"

        return ChatPromptTemplate.from_messages([
            ("system", system_msg),
            ("human", f"è¯·è§£é‡Š{topic}")
        ])

    # æµ‹è¯•ä¸åŒæ°´å¹³
    levels = ["beginner", "intermediate", "advanced"]
    topic = "ä»€ä¹ˆæ˜¯å¼‚æ­¥ç¼–ç¨‹ï¼Ÿ"

    for level in levels:
        print(f"ğŸ“š {level.upper()} æ°´å¹³:")
        prompt = create_conditional_prompt(level, topic)
        messages = prompt.format_messages()

        response = model.invoke(messages)
        print(f"ğŸ¤– å›ç­”: {response.content[:200]}...\n")

def template_pipeline():
    """æ¨¡æ¿ç®¡é“ç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ”— æ¨¡æ¿ç®¡é“ç¤ºä¾‹")
    print("=" * 60)

    model = get_glm_model()

    # åˆ›å»ºä¸€ä¸ªå¤„ç†ç®¡é“
    def create_content_pipeline(topic: str, content_type: str, audience: str):
        """åˆ›å»ºå†…å®¹ç”Ÿæˆç®¡é“"""

        # ç¬¬ä¸€æ­¥ï¼šå†…å®¹è§„åˆ’
        planning_prompt = ChatPromptTemplate.from_template(
            """
            ä»»åŠ¡ï¼šä¸º{audience}è§„åˆ’ä¸€ç¯‡å…³äº{topic}çš„{content_type}

            è¯·æä¾›ï¼š
            1. ä¸»è¦è¦ç‚¹ï¼ˆ3-5ä¸ªï¼‰
            2. å†…å®¹ç»“æ„
            3. æ³¨æ„äº‹é¡¹

            è¯·ç”¨ç®€æ´çš„è¦ç‚¹å½¢å¼å›ç­”ã€‚
            """
        )

        # ç¬¬äºŒæ­¥ï¼šå†…å®¹ç”Ÿæˆ
        generation_prompt = ChatPromptTemplate.from_template(
            """
            åŸºäºä»¥ä¸‹è§„åˆ’ï¼Œä¸º{audience}å†™ä¸€ç¯‡å…³äº{topic}çš„{content_type}ï¼š

            è§„åˆ’è¦ç‚¹ï¼š
            {outline}

            è¦æ±‚ï¼š
            - è¯­è¨€é€‚åˆç›®æ ‡å—ä¼—
            - ç»“æ„æ¸…æ™°
            - å†…å®¹å‡†ç¡®
            - é•¿åº¦é€‚ä¸­
            """
        )

        # ç¬¬ä¸‰æ­¥ï¼šå†…å®¹ä¼˜åŒ–
        optimization_prompt = ChatPromptTemplate.from_template(
            """
            è¯·ä¼˜åŒ–ä»¥ä¸‹å†…å®¹ï¼Œä½¿å…¶æ›´åŠ å®Œå–„ï¼š

            åŸå§‹å†…å®¹ï¼š
            {content}

            ä¼˜åŒ–è¦æ±‚ï¼š
            - æ£€æŸ¥å¹¶ä¿®æ­£é”™è¯¯
            - æ”¹å–„è¯­è¨€è¡¨è¾¾
            - ç¡®ä¿é€»è¾‘æ¸…æ™°
            - é€‚å½“å¢åŠ å®ä¾‹
            """
        )

        return planning_prompt, generation_prompt, optimization_prompt

    # æ‰§è¡Œç®¡é“
    topic = "æœºå™¨å­¦ä¹ åŸºç¡€"
    content_type = "å…¥é—¨æ•™ç¨‹"
    audience = "ç¼–ç¨‹åˆå­¦è€…"

    planning_prompt, generation_prompt, optimization_prompt = create_content_pipeline(
        topic, content_type, audience
    )

    print("ğŸ”§ æ­¥éª¤1: å†…å®¹è§„åˆ’")
    planning_chain = planning_prompt | model | StrOutputParser()
    outline = planning_chain.invoke({
        "topic": topic,
        "content_type": content_type,
        "audience": audience
    })
    print(f"ğŸ“‹ è§„åˆ’ç»“æœ:\n{outline}\n")

    print("âœï¸ æ­¥éª¤2: å†…å®¹ç”Ÿæˆ")
    generation_chain = generation_prompt | model | StrOutputParser()
    content = generation_chain.invoke({
        "topic": topic,
        "content_type": content_type,
        "audience": audience,
        "outline": outline
    })
    print(f"ğŸ“ ç”Ÿæˆå†…å®¹:\n{content[:400]}...\n")

    print("âš¡ æ­¥éª¤3: å†…å®¹ä¼˜åŒ–")
    optimization_chain = optimization_prompt | model | StrOutputParser()
    optimized_content = optimization_chain.invoke({"content": content})
    print(f"ğŸŒŸ ä¼˜åŒ–å†…å®¹:\n{optimized_content[:400]}...\n")

def best_practices():
    """æœ€ä½³å®è·µç¤ºä¾‹"""
    print("=" * 60)
    print("ğŸ’¡ PromptTemplate æœ€ä½³å®è·µ")
    print("=" * 60)

    print("""
1. ğŸ¯ æ˜ç¡®æ€§åŸåˆ™
   - æ¸…æ™°å®šä¹‰è¾“å…¥å˜é‡
   - ä½¿ç”¨æè¿°æ€§çš„å˜é‡å
   - æä¾›å…·ä½“çš„æ ¼å¼è¦æ±‚

2. ğŸ”§ æ¨¡å—åŒ–è®¾è®¡
   - å°†å¤æ‚æç¤ºè¯åˆ†è§£ä¸ºå¤šä¸ªç®€å•æ¨¡æ¿
   - ä½¿ç”¨ç»„åˆè€Œéç»§æ‰¿
   - ä¿æŒæ¨¡æ¿çš„å¯é‡ç”¨æ€§

3. ğŸ›¡ï¸ å®‰å…¨æ€§è€ƒè™‘
   - éªŒè¯è¾“å…¥å‚æ•°
   - è®¾ç½®åˆç†çš„é»˜è®¤å€¼
   - é¿å…æç¤ºè¯æ³¨å…¥æ”»å‡»

4. ğŸ“Š æ€§èƒ½ä¼˜åŒ–
   - ç¼“å­˜å¸¸ç”¨çš„æ¨¡æ¿
   - é¢„ç¼–è¯‘å¤æ‚çš„æ¨¡æ¿
   - ä½¿ç”¨æ‰¹é‡å¤„ç†

5. ğŸ§ª æµ‹è¯•ç­–ç•¥
   - ä¸ºæ¨¡æ¿ç¼–å†™å•å…ƒæµ‹è¯•
   - æµ‹è¯•è¾¹ç•Œæƒ…å†µ
   - éªŒè¯è¾“å‡ºæ ¼å¼
    """)

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ GLM-4.6 + LangChain PromptTemplate è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)

    try:
        # è¿è¡Œå„ç§ç¤ºä¾‹
        basic_template_example()
        multi_variable_template()
        chat_message_template()
        few_shot_template()
        output_parser_template()
        conditional_template()
        template_pipeline()
        best_practices()

        print("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ“š æ›´å¤šä¿¡æ¯è¯·å‚è€ƒï¼š")
        print("- LangChainå®˜æ–¹æ–‡æ¡£: https://python.langchain.com/")
        print("- æç¤ºè¯å·¥ç¨‹æŒ‡å—: https://www.promptingguide.ai/")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{e}")

if __name__ == "__main__":
    main()