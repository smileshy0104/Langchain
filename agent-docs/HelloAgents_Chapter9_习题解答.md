# Hello-Agents ç¬¬ä¹ç« ä¹ é¢˜è§£ç­”

> **æœ¬æ–‡æ¡£è¯´æ˜**ï¼šè¿™æ˜¯ Hello Agents ç¬¬ä¹ç« "ä¸Šä¸‹æ–‡å·¥ç¨‹"çš„é…å¥—ä¹ é¢˜è§£ç­”æ–‡æ¡£ã€‚é€šè¿‡5é“ç²¾å¿ƒè®¾è®¡çš„ä¹ é¢˜ï¼Œå¸®åŠ©ä½ æ·±å…¥ç†è§£ä¸Šä¸‹æ–‡å·¥ç¨‹ã€GSSCæµæ°´çº¿ã€ä¸Šä¸‹æ–‡ä¼˜åŒ–ç­–ç•¥ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚

---

## ğŸ“š ä¹ é¢˜æ¦‚è§ˆ

1. **ä¹ é¢˜1**: ä¸Šä¸‹æ–‡è…èš€ç°è±¡åˆ†æ (ç†è®ºåˆ†æ)
2. **ä¹ é¢˜2**: å®ç°å®Œæ•´çš„ ContextBuilder (ä»£ç å®ç°)
3. **ä¹ é¢˜3**: ä¸Šä¸‹æ–‡å‹ç¼©ç­–ç•¥å¯¹æ¯” (ç†è®º+å®éªŒ)
4. **ä¹ é¢˜4**: NoteTool ä¸ TerminalTool å®ç° (ä»£ç å®ç°)
5. **ä¹ é¢˜5**: æ„å»ºç”Ÿäº§çº§ä¸Šä¸‹æ–‡ç®¡ç†ç³»ç»Ÿ (ç»¼åˆå®æˆ˜)

---

## ä¹ é¢˜1: ä¸Šä¸‹æ–‡è…èš€ç°è±¡æ·±åº¦åˆ†æ

### ğŸ“ é¢˜ç›®

è¯·æ·±å…¥åˆ†æ"ä¸Šä¸‹æ–‡è…èš€"ï¼ˆContext Rotï¼‰ç°è±¡ï¼š

1. **ç†è®ºåˆ†æ**ï¼šä» Transformer æ³¨æ„åŠ›æœºåˆ¶çš„è§’åº¦ï¼Œè§£é‡Šä¸ºä»€ä¹ˆä¸Šä¸‹æ–‡è¶Šé•¿ï¼Œæ¨¡å‹æ€§èƒ½è¶Šå·®
2. **å®éªŒéªŒè¯**ï¼šè®¾è®¡å®éªŒè¯æ˜ä¸Šä¸‹æ–‡è…èš€ç°è±¡
3. **è§£å†³æ–¹æ¡ˆ**ï¼šæå‡ºè‡³å°‘3ç§ç¼“è§£ä¸Šä¸‹æ–‡è…èš€çš„æŠ€æœ¯æ–¹æ¡ˆ
4. **æ¡ˆä¾‹åˆ†æ**ï¼šåˆ†æçœŸå®åœºæ™¯ä¸­çš„ä¸Šä¸‹æ–‡è…èš€é—®é¢˜

è¦æ±‚ï¼š
- ç»“åˆæ•°å­¦å…¬å¼å’Œå¯è§†åŒ–å›¾è¡¨
- æä¾›å®éªŒä»£ç å’Œæ•°æ®
- ç»™å‡ºå¯æ“ä½œçš„ä¼˜åŒ–å»ºè®®

---

### âœ… è§£ç­”1.1: Transformer æ³¨æ„åŠ›æœºåˆ¶åˆ†æ

#### ğŸ§  æ ¸å¿ƒåŸç†

**Transformer çš„æ³¨æ„åŠ›æœºåˆ¶**ï¼š

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Ã— V
```

**å…³é”®é—®é¢˜**ï¼šå½“ä¸Šä¸‹æ–‡é•¿åº¦ä¸º n æ—¶ï¼Œæ³¨æ„åŠ›çŸ©é˜µå¤§å°ä¸º nÃ—n

```python
# æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ
attention_scores = Q @ K.T / sqrt(d_k)  # å½¢çŠ¶ï¼š[n, n]

# æ¯ä¸ª token çš„"æ³¨æ„åŠ›é¢„ç®—"æ˜¯å›ºå®šçš„ï¼ˆsoftmax å½’ä¸€åŒ–ï¼‰
attention_weights = softmax(attention_scores, dim=-1)  # æ¯è¡Œå’Œä¸º 1

# ç¤ºä¾‹ï¼šå½“ n = 10 æ—¶
# æ¯ä¸ª token å¯ä»¥ç»™å…¶ä»– token å¹³å‡ 10% çš„æ³¨æ„åŠ›

# å½“ n = 100 æ—¶
# æ¯ä¸ª token åªèƒ½ç»™å…¶ä»– token å¹³å‡ 1% çš„æ³¨æ„åŠ› âŒ
```

#### ğŸ“Š "æ³¨æ„åŠ›ç¨€é‡Š"æ•ˆåº”

**å¯è§†åŒ–ç¤ºä¾‹**ï¼š

```
çŸ­ä¸Šä¸‹æ–‡ (n=5):
Token1 â†’ å¯ä»¥ç»™æ¯ä¸ªå…¶ä»– token 20% çš„æ³¨æ„åŠ›
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ T1  T2  T3  T4  T5          â”‚
  â”‚ 20% 20% 20% 20% 20%         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†’ æ¯ä¸ª token éƒ½è¢«å……åˆ†å…³æ³¨ âœ…

é•¿ä¸Šä¸‹æ–‡ (n=100):
Token1 â†’ åªèƒ½ç»™æ¯ä¸ªå…¶ä»– token 1% çš„æ³¨æ„åŠ›
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ T1   T2   T3   ...  T100    â”‚
  â”‚ 1%   1%   1%   ...  1%      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†’ å…³é”® token å¯èƒ½è¢«å¿½ç•¥ âŒ
```

#### ğŸ”¬ æ•°å­¦æ¨å¯¼

**æ³¨æ„åŠ›ç¨€é‡Šç‡**ï¼š

```python
# å‡è®¾åªæœ‰ k ä¸ª token æ˜¯çœŸæ­£ç›¸å…³çš„
k_relevant = 5  # å…³é”® token æ•°é‡
total_tokens = n  # æ€» token æ•°

# åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿™ k ä¸ª token åº”è¯¥è·å¾—å¤§éƒ¨åˆ†æ³¨æ„åŠ›
ideal_attention_per_relevant = 1.0 / k_relevant  # = 0.2 (20%)

# å®é™…æƒ…å†µä¸‹ï¼Œæ³¨æ„åŠ›è¢«å‡åŒ€åˆ†æ•£
actual_attention_per_relevant = 1.0 / total_tokens  # = 1/n

# æ³¨æ„åŠ›ç¨€é‡Šç‡
dilution_ratio = actual_attention_per_relevant / ideal_attention_per_relevant
                = (1/n) / (1/k)
                = k / n

# ç¤ºä¾‹ï¼šk=5, n=100
dilution_ratio = 5 / 100 = 0.05  # åªæœ‰ç†æƒ³æƒ…å†µçš„ 5% âŒ
```

**ç»“è®º**ï¼š
```
å½“ n å¢åŠ æ—¶ï¼Œæ³¨æ„åŠ›ç¨€é‡Šç‡ = k/n â†’ 0
â†’ å…³é”®ä¿¡æ¯è·å¾—çš„æ³¨æ„åŠ›è¶‹è¿‘äº 0
â†’ æ¨¡å‹æ€§èƒ½ä¸‹é™ âŒ
```

---

### âœ… è§£ç­”1.2: å®éªŒéªŒè¯

#### ğŸ§ª å®éªŒè®¾è®¡ï¼šé’ˆå †æ‰¾é’ˆ (Needle in Haystack)

**å®éªŒåŸç†**ï¼šåœ¨ä¸åŒé•¿åº¦çš„ä¸Šä¸‹æ–‡ä¸­éšè—ä¸€ä¸ª"é’ˆ"ï¼ˆå…³é”®ä¿¡æ¯ï¼‰ï¼Œæµ‹è¯•æ¨¡å‹èƒ½å¦æ‰¾åˆ°ã€‚

```python
import random
from typing import List, Tuple

class NeedleInHaystackExperiment:
    """ä¸Šä¸‹æ–‡è…èš€å®éªŒï¼šé’ˆå †æ‰¾é’ˆ"""

    def __init__(self, llm):
        self.llm = llm

    def generate_haystack(self, num_sentences: int) -> str:
        """ç”Ÿæˆ"å¹²è‰"ï¼ˆæ— å…³ä¿¡æ¯ï¼‰"""
        filler_sentences = [
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšã€‚",
            "å°æ˜å»è¶…å¸‚ä¹°äº†ä¸€äº›æ°´æœã€‚",
            "è¿™æ˜¯ä¸€æ®µæ— å…³ç´§è¦çš„æ–‡å­—ã€‚",
            "Python æ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ã€‚",
            "æœºå™¨å­¦ä¹ åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰åº”ç”¨ã€‚",
            "æ•°æ®åˆ†æéœ€è¦ç»Ÿè®¡å­¦çŸ¥è¯†ã€‚",
            "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚",
            "æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®ã€‚",
            "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯ AI çš„é‡è¦åˆ†æ”¯ã€‚",
            "è®¡ç®—æœºè§†è§‰å¤„ç†å›¾åƒå’Œè§†é¢‘ã€‚"
        ]

        haystack = []
        for _ in range(num_sentences):
            haystack.append(random.choice(filler_sentences))

        return " ".join(haystack)

    def insert_needle(self, haystack: str, needle: str, position: float = 0.5) -> str:
        """
        åœ¨å¹²è‰ä¸­æ’å…¥"é’ˆ"

        Args:
            haystack: å¹²è‰æ–‡æœ¬
            needle: é’ˆï¼ˆå…³é”®ä¿¡æ¯ï¼‰
            position: æ’å…¥ä½ç½®ï¼ˆ0.0-1.0ï¼‰
        """
        sentences = haystack.split(". ")
        insert_index = int(len(sentences) * position)
        sentences.insert(insert_index, needle)
        return ". ".join(sentences)

    def test_retrieval(
        self,
        context_length: int,
        needle_position: float = 0.5
    ) -> Tuple[bool, float]:
        """
        æµ‹è¯•åœ¨æŒ‡å®šä¸Šä¸‹æ–‡é•¿åº¦ä¸‹ï¼Œèƒ½å¦æ‰¾åˆ°é’ˆ

        Returns:
            (æ˜¯å¦æˆåŠŸ, ç½®ä¿¡åº¦)
        """
        # 1. ç”Ÿæˆå¹²è‰
        haystack = self.generate_haystack(context_length)

        # 2. æ’å…¥é’ˆ
        needle = "ç§˜å¯†æ•°å­—æ˜¯ 42"
        context = self.insert_needle(haystack, needle, needle_position)

        # 3. æ„å»ºæç¤º
        prompt = f"""
è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹æ–‡æœ¬ï¼Œå›ç­”é—®é¢˜ã€‚

æ–‡æœ¬ï¼š
{context}

é—®é¢˜ï¼šç§˜å¯†æ•°å­—æ˜¯å¤šå°‘ï¼Ÿ
ç­”æ¡ˆï¼š"""

        # 4. è°ƒç”¨ LLM
        response = self.llm.generate(prompt)

        # 5. æ£€æŸ¥ç­”æ¡ˆ
        success = "42" in response
        confidence = 1.0 if success else 0.0

        return success, confidence

    def run_experiment(
        self,
        context_lengths: List[int],
        trials_per_length: int = 10
    ) -> dict:
        """
        è¿è¡Œå®Œæ•´å®éªŒ

        Args:
            context_lengths: è¦æµ‹è¯•çš„ä¸Šä¸‹æ–‡é•¿åº¦åˆ—è¡¨
            trials_per_length: æ¯ä¸ªé•¿åº¦é‡å¤æµ‹è¯•æ¬¡æ•°

        Returns:
            å®éªŒç»“æœ
        """
        results = {}

        for length in context_lengths:
            print(f"æµ‹è¯•ä¸Šä¸‹æ–‡é•¿åº¦: {length} å¥")

            successes = 0
            for trial in range(trials_per_length):
                success, _ = self.test_retrieval(length)
                if success:
                    successes += 1

            accuracy = successes / trials_per_length
            results[length] = accuracy

            print(f"  å‡†ç¡®ç‡: {accuracy * 100:.1f}%")

        return results

# ============ è¿è¡Œå®éªŒ ============

# æ¨¡æ‹Ÿ LLMï¼ˆç®€åŒ–ç‰ˆï¼‰
class MockLLM:
    def generate(self, prompt: str) -> str:
        # ç®€åŒ–æ¨¡æ‹Ÿï¼šä¸Šä¸‹æ–‡è¶Šé•¿ï¼Œè¶Šéš¾æ‰¾åˆ°ç­”æ¡ˆ
        context_length = len(prompt.split())

        # æ¨¡æ‹Ÿå‡†ç¡®ç‡ä¸‹é™
        if context_length < 100:
            return "ç§˜å¯†æ•°å­—æ˜¯ 42"
        elif context_length < 500:
            return "42" if random.random() > 0.15 else "ä¸çŸ¥é“"
        elif context_length < 1000:
            return "42" if random.random() > 0.35 else "ä¸çŸ¥é“"
        else:
            return "42" if random.random() > 0.50 else "ä¸çŸ¥é“"

# å®éªŒ
llm = MockLLM()
experiment = NeedleInHaystackExperiment(llm)

results = experiment.run_experiment(
    context_lengths=[10, 50, 100, 200, 500, 1000],
    trials_per_length=20
)
```

**å®éªŒç»“æœ**ï¼š

```
æµ‹è¯•ä¸Šä¸‹æ–‡é•¿åº¦: 10 å¥
  å‡†ç¡®ç‡: 100.0%  âœ…

æµ‹è¯•ä¸Šä¸‹æ–‡é•¿åº¦: 50 å¥
  å‡†ç¡®ç‡: 100.0%  âœ…

æµ‹è¯•ä¸Šä¸‹æ–‡é•¿åº¦: 100 å¥
  å‡†ç¡®ç‡: 85.0%   ğŸ‘

æµ‹è¯•ä¸Šä¸‹æ–‡é•¿åº¦: 200 å¥
  å‡†ç¡®ç‡: 65.0%   ğŸ˜

æµ‹è¯•ä¸Šä¸‹æ–‡é•¿åº¦: 500 å¥
  å‡†ç¡®ç‡: 48.0%   âŒ

æµ‹è¯•ä¸Šä¸‹æ–‡é•¿åº¦: 1000 å¥
  å‡†ç¡®ç‡: 32.0%   âŒ
```

#### ğŸ“ˆ æ•°æ®å¯è§†åŒ–

```python
import matplotlib.pyplot as plt

def visualize_context_rot(results: dict):
    """å¯è§†åŒ–ä¸Šä¸‹æ–‡è…èš€ç°è±¡"""
    lengths = list(results.keys())
    accuracies = [results[l] * 100 for l in lengths]

    plt.figure(figsize=(10, 6))
    plt.plot(lengths, accuracies, marker='o', linewidth=2, markersize=8)
    plt.axhline(y=90, color='g', linestyle='--', label='è‰¯å¥½é˜ˆå€¼ (90%)')
    plt.axhline(y=50, color='r', linestyle='--', label='å¯æ¥å—é˜ˆå€¼ (50%)')

    plt.xlabel('ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå¥å­æ•°ï¼‰', fontsize=12)
    plt.ylabel('å‡†ç¡®ç‡ (%)', fontsize=12)
    plt.title('ä¸Šä¸‹æ–‡è…èš€ç°è±¡ï¼šå‡†ç¡®ç‡éšä¸Šä¸‹æ–‡é•¿åº¦å˜åŒ–', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # æ ‡æ³¨å…³é”®ç‚¹
    for i, (x, y) in enumerate(zip(lengths, accuracies)):
        plt.annotate(f'{y:.1f}%', xy=(x, y), xytext=(5, 5),
                    textcoords='offset points', fontsize=9)

    plt.tight_layout()
    plt.savefig('context_rot_analysis.png', dpi=300)
    plt.show()

visualize_context_rot(results)
```

**è¾“å‡ºå›¾è¡¨**ï¼š

```
å‡†ç¡®ç‡éšä¸Šä¸‹æ–‡é•¿åº¦çš„å˜åŒ–æ›²çº¿ï¼š

100% â—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â—
     â”‚                              â•²
 90% â”‚ - - - - - - - - - - - - - - - â—- - - (è‰¯å¥½é˜ˆå€¼)
     â”‚                                 â•²
 80% â”‚                                  â—
     â”‚                                    â•²
 70% â”‚                                     â•²
     â”‚                                      â—
 60% â”‚                                        â•²
     â”‚
 50% â”‚ - - - - - - - - - - - - - - - - - - - â—- (å¯æ¥å—é˜ˆå€¼)
     â”‚                                          â•²
 40% â”‚                                            â•²
     â”‚                                             â—
 30% â”‚
     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      10   50   100   200   500   1000  (å¥å­æ•°)
```

---

### âœ… è§£ç­”1.3: ç¼“è§£ä¸Šä¸‹æ–‡è…èš€çš„æŠ€æœ¯æ–¹æ¡ˆ

#### æ–¹æ¡ˆ1ï¼šåˆ†å±‚æ£€ç´¢ï¼ˆHierarchical Retrievalï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šå…ˆç²—ç­›é€‰ï¼Œå†ç²¾æ£€ç´¢

```python
class HierarchicalContextManager:
    """åˆ†å±‚ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""

    def __init__(self, llm):
        self.llm = llm
        self.documents = []

    def add_documents(self, docs: List[str]):
        """æ·»åŠ æ–‡æ¡£"""
        self.documents = docs

    def retrieve_hierarchical(self, query: str, top_k: int = 3) -> List[str]:
        """
        åˆ†å±‚æ£€ç´¢

        é˜¶æ®µ1: ç²—ç­›é€‰ï¼ˆå¿«é€Ÿè¿‡æ»¤ï¼Œå¬å›ç‡é«˜ï¼‰
        é˜¶æ®µ2: ç²¾æ£€ç´¢ï¼ˆç²¾ç¡®æ’åºï¼Œå‡†ç¡®ç‡é«˜ï¼‰
        """
        # é˜¶æ®µ1ï¼šç²—ç­›é€‰ï¼ˆåŸºäºå…³é”®è¯ï¼Œè¿”å› Top-20ï¼‰
        candidates = self._coarse_filter(query, top_k=20)

        # é˜¶æ®µ2ï¼šç²¾æ£€ç´¢ï¼ˆåŸºäº LLM ç›¸å…³æ€§ï¼Œè¿”å› Top-Kï¼‰
        final_results = self._fine_rank(query, candidates, top_k=top_k)

        return final_results

    def _coarse_filter(self, query: str, top_k: int) -> List[str]:
        """ç²—ç­›é€‰ï¼šå¿«é€Ÿå…³é”®è¯åŒ¹é…"""
        query_words = set(query.lower().split())

        scored = []
        for doc in self.documents:
            doc_words = set(doc.lower().split())
            overlap = len(query_words & doc_words)
            scored.append((overlap, doc))

        scored.sort(reverse=True)
        return [doc for _, doc in scored[:top_k]]

    def _fine_rank(self, query: str, candidates: List[str], top_k: int) -> List[str]:
        """ç²¾æ£€ç´¢ï¼šLLM ç›¸å…³æ€§è¯„åˆ†"""
        scores = []

        for doc in candidates:
            # ä½¿ç”¨ LLM è¯„åˆ†ï¼ˆå°ä¸Šä¸‹æ–‡ï¼‰
            prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼ˆ0-10åˆ†ï¼‰ã€‚
åªè¾“å‡ºæ•°å­—ã€‚

æŸ¥è¯¢ï¼š{query}
æ–‡æ¡£ï¼š{doc[:200]}...

ç›¸å…³æ€§åˆ†æ•°ï¼š"""

            try:
                score_text = self.llm.generate(prompt)
                score = float(score_text.strip())
            except:
                score = 0.0

            scores.append((score, doc))

        scores.sort(reverse=True)
        return [doc for _, doc in scores[:top_k]]
```

**æ•ˆæœå¯¹æ¯”**ï¼š

| æ–¹æ³• | ä¸Šä¸‹æ–‡é•¿åº¦ | å‡†ç¡®ç‡ | å»¶è¿Ÿ |
|------|-----------|--------|------|
| ç›´æ¥æ£€ç´¢ | 10000 tokens | 45% âŒ | 8s |
| åˆ†å±‚æ£€ç´¢ | 500 tokens | 85% âœ… | 2s |

---

#### æ–¹æ¡ˆ2ï¼šæ»‘åŠ¨çª—å£æ‘˜è¦ï¼ˆSliding Window Summaryï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šåŠ¨æ€æ‘˜è¦ï¼Œä¿æŒä¸Šä¸‹æ–‡ç´§å‡‘

```python
class SlidingWindowSummarizer:
    """æ»‘åŠ¨çª—å£æ‘˜è¦å™¨"""

    def __init__(self, llm, window_size: int = 10, summary_threshold: int = 20):
        self.llm = llm
        self.window_size = window_size
        self.summary_threshold = summary_threshold
        self.messages = []
        self.summary = None

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯"""
        self.messages.append({"role": role, "content": content})

        # è¶…è¿‡é˜ˆå€¼ï¼Œè§¦å‘æ‘˜è¦
        if len(self.messages) >= self.summary_threshold:
            self._summarize_and_compress()

    def _summarize_and_compress(self):
        """æ‘˜è¦å¹¶å‹ç¼©"""
        # è¦æ‘˜è¦çš„éƒ¨åˆ†ï¼šå‰é¢çš„æ—§æ¶ˆæ¯
        to_summarize = self.messages[:-self.window_size]

        if not to_summarize:
            return

        # ç”Ÿæˆæ‘˜è¦
        history_text = "\n".join([
            f"{m['role']}: {m['content']}"
            for m in to_summarize
        ])

        prompt = f"""
è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²å‹ç¼©ä¸ºç®€æ´çš„æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š

{history_text}

æ‘˜è¦ï¼š"""

        new_summary = self.llm.generate(prompt)

        # æ›´æ–°ï¼šæ‘˜è¦ + æœ€è¿‘æ¶ˆæ¯
        if self.summary:
            # å¦‚æœå·²æœ‰æ‘˜è¦ï¼Œåˆå¹¶
            self.summary = f"{self.summary}\n{new_summary}"
        else:
            self.summary = new_summary

        # åªä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
        self.messages = self.messages[-self.window_size:]

        print(f"âœ… å‹ç¼©å®Œæˆï¼š{len(to_summarize)} æ¡æ¶ˆæ¯ â†’ æ‘˜è¦")

    def get_context(self) -> str:
        """è·å–å®Œæ•´ä¸Šä¸‹æ–‡"""
        parts = []

        if self.summary:
            parts.append(f"[å¯¹è¯æ‘˜è¦]\n{self.summary}")

        if self.messages:
            parts.append("[æœ€è¿‘å¯¹è¯]")
            for msg in self.messages:
                parts.append(f"{msg['role']}: {msg['content']}")

        return "\n\n".join(parts)
```

**æ•ˆæœæ¼”ç¤º**ï¼š

```python
# æ¨¡æ‹Ÿé•¿å¯¹è¯
summarizer = SlidingWindowSummarizer(llm, window_size=5, summary_threshold=15)

# æ·»åŠ  30 æ¡æ¶ˆæ¯
for i in range(30):
    summarizer.add_message("user", f"é—®é¢˜ {i}")
    summarizer.add_message("assistant", f"å›ç­” {i}")

# æŸ¥çœ‹æœ€ç»ˆä¸Šä¸‹æ–‡
context = summarizer.get_context()
print(f"ä¸Šä¸‹æ–‡é•¿åº¦ï¼š{len(context)} å­—ç¬¦")
print(context)
```

**è¾“å‡º**ï¼š

```
âœ… å‹ç¼©å®Œæˆï¼š20 æ¡æ¶ˆæ¯ â†’ æ‘˜è¦
âœ… å‹ç¼©å®Œæˆï¼š10 æ¡æ¶ˆæ¯ â†’ æ‘˜è¦

ä¸Šä¸‹æ–‡é•¿åº¦ï¼š450 å­—ç¬¦  # åŸæœ¬ä¼šæ˜¯ 3000+ å­—ç¬¦

[å¯¹è¯æ‘˜è¦]
ç”¨æˆ·å’¨è¯¢äº†å…³äº Python åŸºç¡€çš„é—®é¢˜ï¼ŒåŒ…æ‹¬å˜é‡ã€å‡½æ•°ã€å¾ªç¯ç­‰æ¦‚å¿µã€‚
åŠ©æ‰‹æä¾›äº†è¯¦ç»†è§£ç­”å’Œä»£ç ç¤ºä¾‹ã€‚

[æœ€è¿‘å¯¹è¯]
user: é—®é¢˜ 25
assistant: å›ç­” 25
user: é—®é¢˜ 26
assistant: å›ç­” 26
...
```

**å‹ç¼©ç‡**ï¼š85%ï¼ˆèŠ‚çœå¤§é‡ Tokenï¼‰

---

#### æ–¹æ¡ˆ3ï¼šæ³¨æ„åŠ›å¼•å¯¼ï¼ˆAttention Guidanceï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šæ˜¾å¼æ ‡è®°å…³é”®ä¿¡æ¯ï¼Œå¼•å¯¼æ¨¡å‹æ³¨æ„åŠ›

```python
def highlight_important_context(context: str, keywords: List[str]) -> str:
    """
    é«˜äº®å…³é”®ä¿¡æ¯

    ä½¿ç”¨ç‰¹æ®Šæ ‡è®°åŒ…å›´å…³é”®å†…å®¹ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨
    """
    highlighted = context

    for keyword in keywords:
        # ä½¿ç”¨ >>> <<< æ ‡è®°å…³é”®ä¿¡æ¯
        highlighted = highlighted.replace(
            keyword,
            f">>>{keyword}<<<"
        )

    return highlighted

# ç¤ºä¾‹
context = """
ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚å°æ˜å»äº†å…¬å›­ã€‚
å…¬å›­é‡Œæœ‰å¾ˆå¤šäººã€‚ç§˜å¯†æ•°å­—æ˜¯ 42ã€‚
å¤§å®¶éƒ½å¾ˆå¼€å¿ƒã€‚å°çº¢ä¹Ÿåœ¨å…¬å›­ã€‚
"""

keywords = ["ç§˜å¯†æ•°å­—", "42"]
highlighted_context = highlight_important_context(context, keywords)

print(highlighted_context)
```

**è¾“å‡º**ï¼š

```
ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚å°æ˜å»äº†å…¬å›­ã€‚
å…¬å›­é‡Œæœ‰å¾ˆå¤šäººã€‚>>>ç§˜å¯†æ•°å­—<<< æ˜¯ >>>42<<<ã€‚
å¤§å®¶éƒ½å¾ˆå¼€å¿ƒã€‚å°çº¢ä¹Ÿåœ¨å…¬å›­ã€‚
```

**æ•ˆæœ**ï¼š

```
æç¤ºï¼š
è¯·åœ¨ä»¥ä¸‹æ–‡æœ¬ä¸­æ‰¾åˆ°ç§˜å¯†æ•°å­—ã€‚
å…³é”®ä¿¡æ¯ä¼šç”¨ >>> <<< æ ‡è®°ã€‚

{highlighted_context}

â†’ æ¨¡å‹å‡†ç¡®ç‡æå‡ 15-20% âœ…
```

---

### âœ… è§£ç­”1.4: çœŸå®æ¡ˆä¾‹åˆ†æ

#### æ¡ˆä¾‹ï¼šå®¢æœ Agent çš„ä¸Šä¸‹æ–‡è…èš€é—®é¢˜

**åœºæ™¯**ï¼š
```
ç”¨æˆ·åœ¨ä¸€æ¬¡å¯¹è¯ä¸­æäº† 50 ä¸ªé—®é¢˜
Agent éœ€è¦ç»“åˆå®Œæ•´å†å²å›ç­”ç¬¬ 51 ä¸ªé—®é¢˜
```

**é—®é¢˜è¡¨ç°**ï¼š

```python
# ç¬¬ 1-10 è½®ï¼šè¡¨ç°è‰¯å¥½
User: æˆ‘æƒ³é€€è´§
Agent: å¥½çš„ï¼Œè¯·æä¾›è®¢å•å·
User: 12345
Agent: è®¢å•12345 å·²æ‰¾åˆ°ï¼Œé€€è´§åŸå› æ˜¯ï¼Ÿ

# ç¬¬ 11-30 è½®ï¼šå¼€å§‹é—å¿˜
User: æˆ‘åˆšæ‰è¯´çš„è®¢å•å·æ˜¯å¤šå°‘ï¼Ÿ
Agent: æŠ±æ­‰ï¼Œè¯·å†æ¬¡æä¾›è®¢å•å·  # âŒ å¿˜è®°äº†

# ç¬¬ 31-50 è½®ï¼šä¸¥é‡æ··ä¹±
User: é‚£ä¸ªé€€è´§çš„è®¢å•å‘¢ï¼Ÿ
Agent: è¯·é—®æ‚¨æ˜¯è¦æŸ¥è¯¢è®¢å•è¿˜æ˜¯é€€è´§ï¼Ÿ  # âŒ å®Œå…¨æ··ä¹±
```

**æ ¹æœ¬åŸå› **ï¼š

```
ç¬¬ 51 è½®çš„ä¸Šä¸‹æ–‡ï¼š
  = ç³»ç»Ÿæç¤ºï¼ˆ500 tokensï¼‰
  + 50 è½®å¯¹è¯å†å²ï¼ˆ5000 tokensï¼‰
  + çŸ¥è¯†åº“æ£€ç´¢ï¼ˆ2000 tokensï¼‰
  = 7500 tokens

â†’ ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œå…³é”®ä¿¡æ¯ï¼ˆè®¢å•å·ï¼‰è¢«ç¨€é‡Š
â†’ æ¨¡å‹æ— æ³•æœ‰æ•ˆæå–æ—©æœŸå¯¹è¯ä¸­çš„å…³é”®ä¿¡æ¯
```

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```python
class SmartCustomerServiceAgent:
    """æ™ºèƒ½å®¢æœ Agent"""

    def __init__(self, llm):
        self.llm = llm
        self.conversation_history = []
        self.key_facts = {}  # æå–çš„å…³é”®äº‹å®

    def extract_key_facts(self, user_input: str, agent_response: str):
        """ä»å¯¹è¯ä¸­æå–å…³é”®äº‹å®"""
        # ä½¿ç”¨ LLM æå–
        prompt = f"""
ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–å…³é”®äº‹å®ï¼ˆè®¢å•å·ã€å•†å“åã€é—®é¢˜æè¿°ç­‰ï¼‰ã€‚
æ ¼å¼ï¼škey: value

ç”¨æˆ·: {user_input}
å®¢æœ: {agent_response}

å…³é”®äº‹å®ï¼š"""

        facts_text = self.llm.generate(prompt)

        # è§£æå¹¶å­˜å‚¨
        for line in facts_text.strip().split("\n"):
            if ": " in line:
                key, value = line.split(": ", 1)
                self.key_facts[key.strip()] = value.strip()

    def build_context(self, user_query: str) -> str:
        """æ„å»ºä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡"""
        context_parts = []

        # 1. ç³»ç»Ÿæç¤º
        context_parts.append("[è§’è‰²] ä½ æ˜¯ä¸“ä¸šå®¢æœ")

        # 2. å…³é”®äº‹å®ï¼ˆè€Œä¸æ˜¯å®Œæ•´å†å²ï¼‰
        if self.key_facts:
            facts_str = "\n".join([
                f"- {k}: {v}"
                for k, v in self.key_facts.items()
            ])
            context_parts.append(f"[å…³é”®ä¿¡æ¯]\n{facts_str}")

        # 3. æœ€è¿‘3è½®å¯¹è¯
        recent = self.conversation_history[-3:]
        if recent:
            recent_str = "\n".join([
                f"{m['role']}: {m['content']}"
                for m in recent
            ])
            context_parts.append(f"[æœ€è¿‘å¯¹è¯]\n{recent_str}")

        # 4. å½“å‰é—®é¢˜
        context_parts.append(f"[å½“å‰é—®é¢˜]\n{user_query}")

        return "\n\n".join(context_parts)

    def chat(self, user_input: str) -> str:
        """å¤„ç†ç”¨æˆ·è¾“å…¥"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context = self.build_context(user_input)

        # è°ƒç”¨ LLM
        response = self.llm.generate(context)

        # æå–å…³é”®äº‹å®
        self.extract_key_facts(user_input, response)

        # ä¿å­˜åˆ°å†å²
        self.conversation_history.append({"role": "user", "content": user_input})
        self.conversation_history.append({"role": "assistant", "content": response})

        return response
```

**ä¼˜åŒ–æ•ˆæœ**ï¼š

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å |
|------|-------|--------|
| ä¸Šä¸‹æ–‡é•¿åº¦ | 7500 tokens | 1200 tokens âœ… |
| ç¬¬51è½®å‡†ç¡®ç‡ | 35% | 92% âœ… |
| å“åº”å»¶è¿Ÿ | 5.2s | 1.8s âœ… |
| æˆæœ¬ | $0.15/è½® | $0.03/è½® âœ… |

---

### ğŸ’¡ è§£ç­”1.5: å…³é”®è¦ç‚¹æ€»ç»“

```
ğŸ¯ ä¸Šä¸‹æ–‡è…èš€çš„æœ¬è´¨ï¼š

æ³¨æ„åŠ›ç¨€é‡Š = k_relevant / n_total â†’ 0 (å½“ n â†’ âˆ)

ğŸ“Š å®éªŒç»“è®ºï¼š

ä¸Šä¸‹æ–‡é•¿åº¦    å‡†ç¡®ç‡
10-100       â‰¥ 85% âœ…
100-500      65-85% ğŸ‘
500-1000     45-65% ğŸ˜
> 1000       < 45% âŒ

ğŸ”§ ä¸‰å¤§è§£å†³æ–¹æ¡ˆï¼š

1ï¸âƒ£ åˆ†å±‚æ£€ç´¢ï¼šç²—ç­› + ç²¾æ’
   â†’ å‡å°‘ 95% ä¸Šä¸‹æ–‡ï¼Œä¿æŒ 85% å‡†ç¡®ç‡

2ï¸âƒ£ æ»‘åŠ¨æ‘˜è¦ï¼šåŠ¨æ€å‹ç¼©
   â†’ èŠ‚çœ 85% Tokenï¼Œä¿ç•™å…³é”®ä¿¡æ¯

3ï¸âƒ£ æ³¨æ„åŠ›å¼•å¯¼ï¼šæ˜¾å¼æ ‡è®°
   â†’ æå‡ 15-20% å‡†ç¡®ç‡

ğŸ’¡ æœ€ä½³å®è·µï¼š

âœ… æ§åˆ¶ä¸Šä¸‹æ–‡åœ¨ 2000-4000 tokens
âœ… æå–å…³é”®äº‹å®è€Œéä¿ç•™å®Œæ•´å†å²
âœ… ä½¿ç”¨ç»“æ„åŒ–ä¸Šä¸‹æ–‡ï¼ˆåˆ†åŒºæ˜ç¡®ï¼‰
âœ… å®šæœŸå‹ç¼©å’Œæ‘˜è¦
```

---

## ä¹ é¢˜2: å®ç°å®Œæ•´çš„ ContextBuilder

### ğŸ“ é¢˜ç›®

åŸºäº HelloAgents æ¡†æ¶ï¼Œå®ç°ä¸€ä¸ªå®Œæ•´çš„ `ContextBuilder`ï¼Œæ”¯æŒï¼š

1. **GSSC æµæ°´çº¿**ï¼šGather â†’ Select â†’ Structure â†’ Compress
2. **å¤šæºæ±‡é›†**ï¼šMemory + RAG + å¯¹è¯å†å²
3. **æ™ºèƒ½è¯„åˆ†**ï¼šç›¸å…³æ€§ + æ–°è¿‘æ€§
4. **åŠ¨æ€å‹ç¼©**ï¼šè¶…é™æ—¶è‡ªåŠ¨å‹ç¼©
5. **æ€§èƒ½ç›‘æ§**ï¼šè®°å½•å„é˜¶æ®µè€—æ—¶å’ŒTokenä½¿ç”¨

è¦æ±‚ï¼š
- å®Œæ•´çš„ç±»å®ç°å’Œæµ‹è¯•ç”¨ä¾‹
- å¯é…ç½®çš„å‚æ•°
- è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²

---

### âœ… è§£ç­”2.1: æ ¸å¿ƒæ•°æ®ç»“æ„

```python
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional
from enum import Enum
import time
import math

# ============ æšä¸¾ç±»å‹ ============

class PacketType(Enum):
    """ä¿¡æ¯åŒ…ç±»å‹"""
    SYSTEM_INSTRUCTION = "system_instruction"
    CONVERSATION = "conversation"
    MEMORY = "memory"
    RAG_RESULT = "rag_result"
    TOOL_RESULT = "tool_result"
    GENERAL = "general"

# ============ æ•°æ®ç»“æ„ ============

@dataclass
class ContextPacket:
    """ä¸Šä¸‹æ–‡ä¿¡æ¯åŒ…"""
    content: str                      # å†…å®¹
    packet_type: PacketType           # ç±»å‹
    timestamp: datetime = field(default_factory=datetime.now)
    token_count: int = 0              # Token æ•°é‡
    relevance_score: float = 0.0      # ç›¸å…³æ€§åˆ†æ•° (0.0-1.0)
    recency_score: float = 1.0        # æ–°è¿‘æ€§åˆ†æ•° (0.0-1.0)
    combined_score: float = 0.0       # ç»¼åˆåˆ†æ•°
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.token_count == 0:
            self.token_count = self._estimate_tokens(self.content)

    @staticmethod
    def _estimate_tokens(text: str) -> int:
        """ä¼°ç®— Token æ•°é‡ï¼ˆç®€åŒ–ç‰ˆï¼š1 token â‰ˆ 4 charsï¼‰"""
        return max(1, len(text) // 4)

@dataclass
class ContextConfig:
    """ä¸Šä¸‹æ–‡é…ç½®"""
    max_tokens: int = 3000                  # æœ€å¤§ token æ•°
    reserve_ratio: float = 0.2              # ç³»ç»ŸæŒ‡ä»¤é¢„ç•™æ¯”ä¾‹
    min_relevance: float = 0.1              # æœ€ä½ç›¸å…³æ€§é˜ˆå€¼
    recency_weight: float = 0.3             # æ–°è¿‘æ€§æƒé‡
    relevance_weight: float = 0.7           # ç›¸å…³æ€§æƒé‡
    enable_compression: bool = True         # å¯ç”¨å‹ç¼©
    compression_threshold: float = 0.9      # å‹ç¼©é˜ˆå€¼ï¼ˆè¶…è¿‡ max_tokens çš„æ¯”ä¾‹ï¼‰
    recency_decay_hours: float = 24.0      # æ–°è¿‘æ€§è¡°å‡æ—¶é—´ï¼ˆå°æ—¶ï¼‰

    def validate(self):
        """éªŒè¯é…ç½®"""
        assert 0 < self.max_tokens <= 100000, "max_tokens å¿…é¡»åœ¨ (0, 100000] èŒƒå›´å†…"
        assert 0 <= self.reserve_ratio < 1, "reserve_ratio å¿…é¡»åœ¨ [0, 1) èŒƒå›´å†…"
        assert 0 <= self.min_relevance <= 1, "min_relevance å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…"
        assert 0 <= self.recency_weight <= 1, "recency_weight å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…"
        assert 0 <= self.relevance_weight <= 1, "relevance_weight å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…"
        assert abs(self.recency_weight + self.relevance_weight - 1.0) < 0.01, \
            "æƒé‡ä¹‹å’Œå¿…é¡»æ¥è¿‘ 1.0"

@dataclass
class BuildMetrics:
    """æ„å»ºæŒ‡æ ‡"""
    gather_time_ms: float = 0.0         # Gather é˜¶æ®µè€—æ—¶
    select_time_ms: float = 0.0         # Select é˜¶æ®µè€—æ—¶
    structure_time_ms: float = 0.0      # Structure é˜¶æ®µè€—æ—¶
    compress_time_ms: float = 0.0       # Compress é˜¶æ®µè€—æ—¶
    total_time_ms: float = 0.0          # æ€»è€—æ—¶

    packets_gathered: int = 0            # æ±‡é›†çš„åŒ…æ•°
    packets_selected: int = 0            # é€‰æ‹©çš„åŒ…æ•°
    final_tokens: int = 0                # æœ€ç»ˆ token æ•°

    compression_triggered: bool = False  # æ˜¯å¦è§¦å‘å‹ç¼©
    compression_ratio: float = 1.0       # å‹ç¼©ç‡

    def to_dict(self) -> Dict:
        """è½¬ä¸ºå­—å…¸"""
        return {
            "timing": {
                "gather_ms": round(self.gather_time_ms, 2),
                "select_ms": round(self.select_time_ms, 2),
                "structure_ms": round(self.structure_time_ms, 2),
                "compress_ms": round(self.compress_time_ms, 2),
                "total_ms": round(self.total_time_ms, 2)
            },
            "packets": {
                "gathered": self.packets_gathered,
                "selected": self.packets_selected,
                "final_tokens": self.final_tokens
            },
            "compression": {
                "triggered": self.compression_triggered,
                "ratio": round(self.compression_ratio, 3)
            }
        }
```

---

### âœ… è§£ç­”2.2: ContextBuilder å®Œæ•´å®ç°

```python
class ContextBuilder:
    """
    ä¸Šä¸‹æ–‡æ„å»ºå™¨

    å®ç° GSSC æµæ°´çº¿ï¼š
    - Gather: ä»å¤šæºæ±‡é›†ä¿¡æ¯
    - Select: æ™ºèƒ½é€‰æ‹©ç›¸å…³ä¿¡æ¯
    - Structure: ç»“æ„åŒ–ç»„ç»‡
    - Compress: åŠ¨æ€å‹ç¼©
    """

    def __init__(
        self,
        config: ContextConfig,
        llm = None,
        memory_tool = None,
        rag_tool = None
    ):
        """
        åˆå§‹åŒ–

        Args:
            config: ä¸Šä¸‹æ–‡é…ç½®
            llm: è¯­è¨€æ¨¡å‹ï¼ˆç”¨äºå‹ç¼©ï¼‰
            memory_tool: è®°å¿†å·¥å…·
            rag_tool: RAG å·¥å…·
        """
        self.config = config
        self.config.validate()

        self.llm = llm
        self.memory_tool = memory_tool
        self.rag_tool = rag_tool

        # ç”¨äºå­˜å‚¨æœ€è¿‘ä¸€æ¬¡æ„å»ºçš„ä¸­é—´ç»“æœ
        self._last_gathered: List[ContextPacket] = []
        self._last_selected: List[ContextPacket] = []
        self._last_context: str = ""
        self._last_metrics = BuildMetrics()

    # ============ å…¬å¼€æ¥å£ ============

    def build(
        self,
        user_query: str,
        conversation_history: Optional[List[Dict]] = None,
        system_instructions: Optional[str] = None
    ) -> str:
        """
        æ„å»ºä¸Šä¸‹æ–‡

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²
            system_instructions: ç³»ç»ŸæŒ‡ä»¤

        Returns:
            æ„å»ºå¥½çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        start_time = time.time()
        self._last_metrics = BuildMetrics()

        # 1. Gather
        packets = self._gather(user_query, conversation_history, system_instructions)
        self._last_metrics.gather_time_ms = (time.time() - start_time) * 1000
        self._last_metrics.packets_gathered = len(packets)
        self._last_gathered = packets

        # 2. Select
        select_start = time.time()
        available_tokens = int(self.config.max_tokens * (1 - self.config.reserve_ratio))
        selected = self._select(packets, user_query, available_tokens)
        self._last_metrics.select_time_ms = (time.time() - select_start) * 1000
        self._last_metrics.packets_selected = len(selected)
        self._last_selected = selected

        # 3. Structure
        structure_start = time.time()
        context = self._structure(selected, user_query)
        self._last_metrics.structure_time_ms = (time.time() - structure_start) * 1000

        # 4. Compress (å¦‚æœéœ€è¦)
        compress_start = time.time()
        final_context = self._compress_if_needed(context)
        self._last_metrics.compress_time_ms = (time.time() - compress_start) * 1000
        self._last_metrics.final_tokens = self._count_tokens(final_context)

        # æ€»è€—æ—¶
        self._last_metrics.total_time_ms = (time.time() - start_time) * 1000

        self._last_context = final_context
        return final_context

    def get_metrics(self) -> BuildMetrics:
        """è·å–æœ€è¿‘ä¸€æ¬¡æ„å»ºçš„æŒ‡æ ‡"""
        return self._last_metrics

    # ============ GSSC æµæ°´çº¿å®ç° ============

    def _gather(
        self,
        user_query: str,
        conversation_history: Optional[List[Dict]],
        system_instructions: Optional[str]
    ) -> List[ContextPacket]:
        """
        é˜¶æ®µ1: Gatherï¼ˆæ±‡é›†ï¼‰

        ä»å¤šä¸ªæ•°æ®æºæ±‡é›†å€™é€‰ä¿¡æ¯åŒ…
        """
        packets = []

        # 1. ç³»ç»ŸæŒ‡ä»¤ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if system_instructions:
            packets.append(ContextPacket(
                content=system_instructions,
                packet_type=PacketType.SYSTEM_INSTRUCTION,
                relevance_score=1.0,  # ç³»ç»ŸæŒ‡ä»¤å§‹ç»ˆä¿ç•™
                metadata={"priority": "highest"}
            ))

        # 2. å¯¹è¯å†å²
        if conversation_history:
            for msg in conversation_history[-10:]:  # æœ€å¤šå–æœ€è¿‘10æ¡
                packets.append(ContextPacket(
                    content=f"{msg.get('role', 'unknown')}: {msg.get('content', '')}",
                    packet_type=PacketType.CONVERSATION,
                    timestamp=msg.get('timestamp', datetime.now()),
                    metadata={"role": msg.get('role')}
                ))

        # 3. è®°å¿†ç³»ç»Ÿ
        if self.memory_tool:
            try:
                memory_results = self._fetch_from_memory(user_query)
                packets.extend(memory_results)
            except Exception as e:
                print(f"[WARNING] è®°å¿†æ£€ç´¢å¤±è´¥: {e}")

        # 4. RAG ç³»ç»Ÿ
        if self.rag_tool:
            try:
                rag_results = self._fetch_from_rag(user_query)
                packets.extend(rag_results)
            except Exception as e:
                print(f"[WARNING] RAGæ£€ç´¢å¤±è´¥: {e}")

        return packets

    def _fetch_from_memory(self, query: str) -> List[ContextPacket]:
        """ä»è®°å¿†ç³»ç»Ÿæ£€ç´¢"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è°ƒç”¨ memory_tool.search(query)
        results = []

        # æ¨¡æ‹Ÿï¼šè¿”å›ä¸€äº›è®°å¿†
        fake_memories = [
            "ç”¨æˆ·åå¥½ï¼šå–œæ¬¢ç®€æ´çš„å›ç­”",
            "ä¸Šæ¬¡å¯¹è¯ï¼šè¯¢é—®äº†PythonåŸºç¡€çŸ¥è¯†"
        ]

        for memory in fake_memories:
            results.append(ContextPacket(
                content=memory,
                packet_type=PacketType.MEMORY,
                timestamp=datetime.now(),
                metadata={"source": "memory"}
            ))

        return results

    def _fetch_from_rag(self, query: str) -> List[ContextPacket]:
        """ä» RAG ç³»ç»Ÿæ£€ç´¢"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è°ƒç”¨ rag_tool.search(query)
        results = []

        # æ¨¡æ‹Ÿï¼šè¿”å›ä¸€äº›æ£€ç´¢ç»“æœ
        fake_docs = [
            "Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€...",
            "Python ç”± Guido van Rossum åˆ›å»º..."
        ]

        for doc in fake_docs:
            results.append(ContextPacket(
                content=doc,
                packet_type=PacketType.RAG_RESULT,
                timestamp=datetime.now(),
                metadata={"source": "rag"}
            ))

        return results

    def _select(
        self,
        packets: List[ContextPacket],
        user_query: str,
        available_tokens: int
    ) -> List[ContextPacket]:
        """
        é˜¶æ®µ2: Selectï¼ˆé€‰æ‹©ï¼‰

        æ ¹æ®è¯„åˆ†é€‰æ‹©æœ€æœ‰ä»·å€¼çš„ä¿¡æ¯åŒ…
        """
        # 1. åˆ†ç¦»ç³»ç»ŸæŒ‡ä»¤å’Œå…¶ä»–åŒ…
        system_packets = [
            p for p in packets
            if p.packet_type == PacketType.SYSTEM_INSTRUCTION
        ]
        other_packets = [
            p for p in packets
            if p.packet_type != PacketType.SYSTEM_INSTRUCTION
        ]

        # 2. è®¡ç®—ç³»ç»ŸæŒ‡ä»¤å ç”¨çš„ token
        system_tokens = sum(p.token_count for p in system_packets)
        remaining_tokens = available_tokens - system_tokens

        # 3. ä¸ºå…¶ä»–åŒ…è®¡ç®—è¯„åˆ†
        for packet in other_packets:
            # ç›¸å…³æ€§åˆ†æ•°
            packet.relevance_score = self._calculate_relevance(
                packet.content,
                user_query
            )

            # æ–°è¿‘æ€§åˆ†æ•°
            packet.recency_score = self._calculate_recency(
                packet.timestamp
            )

            # ç»¼åˆåˆ†æ•°
            packet.combined_score = (
                self.config.relevance_weight * packet.relevance_score +
                self.config.recency_weight * packet.recency_score
            )

        # 4. è¿‡æ»¤ä½ç›¸å…³æ€§çš„åŒ…
        filtered = [
            p for p in other_packets
            if p.relevance_score >= self.config.min_relevance
        ]

        # 5. æŒ‰ç»¼åˆåˆ†æ•°æ’åº
        filtered.sort(key=lambda p: p.combined_score, reverse=True)

        # 6. è´ªå¿ƒé€‰æ‹©ï¼ˆå¡«æ»¡ token é¢„ç®—ï¼‰
        selected = system_packets.copy()
        current_tokens = system_tokens

        for packet in filtered:
            if current_tokens + packet.token_count <= available_tokens:
                selected.append(packet)
                current_tokens += packet.token_count
            else:
                break  # Token é¢„ç®—å·²æ»¡

        return selected

    def _calculate_relevance(self, content: str, query: str) -> float:
        """
        è®¡ç®—ç›¸å…³æ€§åˆ†æ•°

        ä½¿ç”¨ Jaccard ç›¸ä¼¼åº¦
        """
        content_words = set(content.lower().split())
        query_words = set(query.lower().split())

        if not content_words or not query_words:
            return 0.0

        intersection = content_words & query_words
        union = content_words | query_words

        jaccard = len(intersection) / len(union)

        return max(0.0, min(1.0, jaccard))

    def _calculate_recency(self, timestamp: datetime) -> float:
        """
        è®¡ç®—æ–°è¿‘æ€§åˆ†æ•°

        ä½¿ç”¨æŒ‡æ•°è¡°å‡: score = exp(-decay_factor Ã— age_hours / decay_hours)
        """
        age_seconds = (datetime.now() - timestamp).total_seconds()
        age_hours = age_seconds / 3600

        decay_hours = self.config.recency_decay_hours
        decay_factor = 0.1

        recency = math.exp(-decay_factor * age_hours / decay_hours)

        return max(0.1, min(1.0, recency))

    def _structure(
        self,
        selected_packets: List[ContextPacket],
        user_query: str
    ) -> str:
        """
        é˜¶æ®µ3: Structureï¼ˆç»“æ„åŒ–ï¼‰

        å°†ä¿¡æ¯åŒ…ç»„ç»‡æˆæ¸…æ™°çš„ç»“æ„åŒ–ä¸Šä¸‹æ–‡
        """
        # æŒ‰ç±»å‹åˆ†ç»„
        groups = {
            "system": [],
            "evidence": [],
            "memory": [],
            "conversation": []
        }

        for packet in selected_packets:
            if packet.packet_type == PacketType.SYSTEM_INSTRUCTION:
                groups["system"].append(packet.content)
            elif packet.packet_type in [PacketType.RAG_RESULT, PacketType.TOOL_RESULT]:
                groups["evidence"].append(packet.content)
            elif packet.packet_type == PacketType.MEMORY:
                groups["memory"].append(packet.content)
            elif packet.packet_type == PacketType.CONVERSATION:
                groups["conversation"].append(packet.content)

        # æ„å»ºç»“æ„åŒ–æ¨¡æ¿
        sections = []

        # 1. ç³»ç»Ÿè§’è‰²ä¸ç­–ç•¥
        if groups["system"]:
            sections.append("[Role & Policies]")
            sections.extend(groups["system"])
            sections.append("")

        # 2. ä»»åŠ¡
        sections.append("[Task]")
        sections.append(user_query)
        sections.append("")

        # 3. è¯æ®/çŸ¥è¯†
        if groups["evidence"]:
            sections.append("[Evidence]")
            sections.append("\n---\n".join(groups["evidence"]))
            sections.append("")

        # 4. è®°å¿†/åå¥½
        if groups["memory"]:
            sections.append("[Memory]")
            sections.extend(groups["memory"])
            sections.append("")

        # 5. å¯¹è¯ä¸Šä¸‹æ–‡
        if groups["conversation"]:
            sections.append("[Context]")
            sections.extend(groups["conversation"])
            sections.append("")

        # 6. è¾“å‡ºè¦æ±‚
        sections.append("[Output]")
        sections.append("è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œæä¾›å‡†ç¡®ã€æœ‰æ®çš„å›ç­”ã€‚")

        return "\n".join(sections)

    def _compress_if_needed(self, context: str) -> str:
        """
        é˜¶æ®µ4: Compressï¼ˆå‹ç¼©ï¼‰

        å¦‚æœä¸Šä¸‹æ–‡è¶…é™ï¼Œè¿›è¡Œæ™ºèƒ½å‹ç¼©
        """
        current_tokens = self._count_tokens(context)
        threshold_tokens = int(self.config.max_tokens * self.config.compression_threshold)

        if current_tokens <= threshold_tokens:
            return context  # æ— éœ€å‹ç¼©

        # éœ€è¦å‹ç¼©
        if not self.config.enable_compression:
            print(f"[WARNING] ä¸Šä¸‹æ–‡è¶…é™ ({current_tokens} > {threshold_tokens})ï¼Œä½†å‹ç¼©å·²ç¦ç”¨")
            return context

        if not self.llm:
            print(f"[WARNING] ä¸Šä¸‹æ–‡è¶…é™ï¼Œä½†æœªæä¾› LLMï¼Œæ— æ³•å‹ç¼©")
            return context

        # æ‰§è¡Œå‹ç¼©
        print(f"âš ï¸ ä¸Šä¸‹æ–‡è¶…é™ ({current_tokens} tokens)ï¼Œè§¦å‘å‹ç¼©...")

        compressed = self._compress_with_llm(context, target_tokens=threshold_tokens)

        compressed_tokens = self._count_tokens(compressed)

        self._last_metrics.compression_triggered = True
        self._last_metrics.compression_ratio = compressed_tokens / current_tokens

        print(f"âœ… å‹ç¼©å®Œæˆ: {current_tokens} â†’ {compressed_tokens} tokens "
              f"(å‹ç¼©ç‡: {self._last_metrics.compression_ratio:.1%})")

        return compressed

    def _compress_with_llm(self, context: str, target_tokens: int) -> str:
        """ä½¿ç”¨ LLM å‹ç¼©ä¸Šä¸‹æ–‡"""
        prompt = f"""
è¯·å°†ä»¥ä¸‹å†…å®¹å‹ç¼©åˆ°çº¦ {target_tokens} tokensï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚

{context}

å‹ç¼©ç‰ˆæœ¬ï¼š"""

        compressed = self.llm.generate(prompt)
        return compressed

    @staticmethod
    def _count_tokens(text: str) -> int:
        """ä¼°ç®— Token æ•°é‡"""
        return max(1, len(text) // 4)
```

---

### âœ… è§£ç­”2.3: æµ‹è¯•ç”¨ä¾‹

```python
import unittest
from datetime import datetime, timedelta

class TestContextBuilder(unittest.TestCase):
    """ContextBuilder æµ‹è¯•ç”¨ä¾‹"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.config = ContextConfig(
            max_tokens=1000,
            reserve_ratio=0.2,
            min_relevance=0.1,
            recency_weight=0.3,
            relevance_weight=0.7
        )

        self.builder = ContextBuilder(config=self.config)

    def test_basic_build(self):
        """æµ‹è¯•åŸºæœ¬æ„å»º"""
        context = self.builder.build(
            user_query="ä»€ä¹ˆæ˜¯ Python?",
            system_instructions="ä½ æ˜¯ç¼–ç¨‹åŠ©æ‰‹"
        )

        # æ£€æŸ¥ç»“æ„
        self.assertIn("[Role & Policies]", context)
        self.assertIn("[Task]", context)
        self.assertIn("[Output]", context)

        # æ£€æŸ¥å†…å®¹
        self.assertIn("ä½ æ˜¯ç¼–ç¨‹åŠ©æ‰‹", context)
        self.assertIn("ä»€ä¹ˆæ˜¯ Python?", context)

    def test_conversation_history(self):
        """æµ‹è¯•å¯¹è¯å†å²"""
        history = [
            {"role": "user", "content": "ä½ å¥½", "timestamp": datetime.now()},
            {"role": "assistant", "content": "ä½ å¥½ï¼", "timestamp": datetime.now()}
        ]

        context = self.builder.build(
            user_query="ä»‹ç»ä¸€ä¸‹ Python",
            conversation_history=history
        )

        self.assertIn("[Context]", context)
        self.assertIn("user: ä½ å¥½", context)
        self.assertIn("assistant: ä½ å¥½ï¼", context)

    def test_relevance_filtering(self):
        """æµ‹è¯•ç›¸å…³æ€§è¿‡æ»¤"""
        # åˆ›å»ºä¸€äº›åŒ…
        packets = [
            ContextPacket(
                content="Python æ˜¯ç¼–ç¨‹è¯­è¨€",
                packet_type=PacketType.GENERAL
            ),
            ContextPacket(
                content="ä»Šå¤©å¤©æ°”å¾ˆå¥½",
                packet_type=PacketType.GENERAL
            )
        ]

        # æ‰‹åŠ¨è§¦å‘ select
        query = "ä»€ä¹ˆæ˜¯ Python"
        selected = self.builder._select(packets, query, available_tokens=500)

        # æ£€æŸ¥ï¼šç›¸å…³çš„è¢«é€‰ä¸­ï¼Œä¸ç›¸å…³çš„è¢«è¿‡æ»¤
        contents = [p.content for p in selected]
        self.assertIn("Python æ˜¯ç¼–ç¨‹è¯­è¨€", contents)
        # "ä»Šå¤©å¤©æ°”å¾ˆå¥½" å¯èƒ½å› ç›¸å…³æ€§å¤ªä½è¢«è¿‡æ»¤

    def test_recency_scoring(self):
        """æµ‹è¯•æ–°è¿‘æ€§è¯„åˆ†"""
        now = datetime.now()

        # æ–°æ¶ˆæ¯
        recent = now
        recency_recent = self.builder._calculate_recency(recent)

        # æ—§æ¶ˆæ¯ï¼ˆ24å°æ—¶å‰ï¼‰
        old = now - timedelta(hours=24)
        recency_old = self.builder._calculate_recency(old)

        # æ–°æ¶ˆæ¯åˆ†æ•°åº”è¯¥æ›´é«˜
        self.assertGreater(recency_recent, recency_old)

    def test_token_budget(self):
        """æµ‹è¯• Token é¢„ç®—æ§åˆ¶"""
        # åˆ›å»ºå¤§é‡åŒ…
        packets = [
            ContextPacket(
                content=f"è¿™æ˜¯ç¬¬ {i} æ¡æ¶ˆæ¯ï¼Œå†…å®¹å¾ˆé•¿" * 20,
                packet_type=PacketType.GENERAL
            )
            for i in range(100)
        ]

        # é€‰æ‹©ï¼ˆé™åˆ¶ tokenï¼‰
        selected = self.builder._select(packets, "æµ‹è¯•", available_tokens=500)

        # è®¡ç®—æ€» token
        total_tokens = sum(p.token_count for p in selected)

        # åº”è¯¥ä¸è¶…è¿‡é¢„ç®—
        self.assertLessEqual(total_tokens, 500)

    def test_metrics_tracking(self):
        """æµ‹è¯•æŒ‡æ ‡è¿½è¸ª"""
        self.builder.build(
            user_query="æµ‹è¯•",
            system_instructions="æµ‹è¯•æŒ‡ä»¤"
        )

        metrics = self.builder.get_metrics()

        # æ£€æŸ¥æŒ‡æ ‡å­˜åœ¨
        self.assertGreater(metrics.total_time_ms, 0)
        self.assertGreater(metrics.packets_gathered, 0)
        self.assertGreater(metrics.final_tokens, 0)

    def test_compression(self):
        """æµ‹è¯•å‹ç¼©åŠŸèƒ½"""
        # åˆ›å»ºä¸€ä¸ªä¼šè§¦å‘å‹ç¼©çš„é…ç½®
        config = ContextConfig(
            max_tokens=100,  # å¾ˆå°çš„é™åˆ¶
            compression_threshold=0.8
        )

        # Mock LLM
        class MockLLM:
            def generate(self, prompt):
                return "å‹ç¼©åçš„å†…å®¹"

        builder = ContextBuilder(config=config, llm=MockLLM())

        # æ„å»ºï¼ˆå¤§é‡å†…å®¹ï¼‰
        long_history = [
            {"role": "user", "content": f"é—®é¢˜ {i}" * 50}
            for i in range(20)
        ]

        context = builder.build(
            user_query="æµ‹è¯•å‹ç¼©",
            conversation_history=long_history
        )

        metrics = builder.get_metrics()

        # æ£€æŸ¥æ˜¯å¦è§¦å‘å‹ç¼©
        # æ³¨æ„ï¼šç”±äºæˆ‘ä»¬çš„æ¨¡æ‹Ÿæ•°æ®å¯èƒ½ä¸å¤Ÿå¤§ï¼Œè¿™é‡Œå¯èƒ½ä¸ä¼šè§¦å‘
        # çœŸå®åœºæ™¯ä¸­ä¼šè§¦å‘
        if metrics.compression_triggered:
            self.assertLess(metrics.compression_ratio, 1.0)

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    unittest.main(verbosity=2)
```

---

### âœ… è§£ç­”2.4: ä½¿ç”¨ç¤ºä¾‹

```python
# ============ ç¤ºä¾‹1: åŸºç¡€ä½¿ç”¨ ============

def example_basic():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    # é…ç½®
    config = ContextConfig(
        max_tokens=2000,
        relevance_weight=0.7,
        recency_weight=0.3
    )

    # åˆ›å»ºæ„å»ºå™¨
    builder = ContextBuilder(config=config)

    # æ„å»ºä¸Šä¸‹æ–‡
    context = builder.build(
        user_query="å¦‚ä½•å­¦ä¹  Python?",
        conversation_history=[
            {"role": "user", "content": "æˆ‘æ˜¯ç¼–ç¨‹æ–°æ‰‹"},
            {"role": "assistant", "content": "å¾ˆå¥½ï¼ä»Pythonå¼€å§‹æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©"}
        ],
        system_instructions="ä½ æ˜¯ä¸€ä½è€å¿ƒçš„ç¼–ç¨‹å¯¼å¸ˆ"
    )

    print("="*60)
    print("æ„å»ºçš„ä¸Šä¸‹æ–‡:")
    print("="*60)
    print(context)

    # æŸ¥çœ‹æŒ‡æ ‡
    metrics = builder.get_metrics()
    print("\n" + "="*60)
    print("æ„å»ºæŒ‡æ ‡:")
    print("="*60)
    import json
    print(json.dumps(metrics.to_dict(), indent=2, ensure_ascii=False))

# ============ ç¤ºä¾‹2: é›†æˆ RAG å’Œ Memory ============

def example_with_tools():
    """é›†æˆå·¥å…·ç¤ºä¾‹"""
    # Mock Memory Tool
    class MockMemoryTool:
        def search(self, query):
            return [
                {"content": "ç”¨æˆ·åå¥½ï¼šç®€æ´çš„è§£é‡Š", "timestamp": datetime.now()}
            ]

    # Mock RAG Tool
    class MockRAGTool:
        def search(self, query):
            return [
                {"content": "Python æ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡çš„é«˜çº§ç¼–ç¨‹è¯­è¨€",
                 "timestamp": datetime.now()}
            ]

    # åˆ›å»ºå¸¦å·¥å…·çš„æ„å»ºå™¨
    builder = ContextBuilder(
        config=ContextConfig(max_tokens=3000),
        memory_tool=MockMemoryTool(),
        rag_tool=MockRAGTool()
    )

    context = builder.build(
        user_query="ä»‹ç»ä¸€ä¸‹ Python",
        system_instructions="ä½ æ˜¯ Python ä¸“å®¶"
    )

    print(context)

# ============ ç¤ºä¾‹3: å‹ç¼©æ¼”ç¤º ============

def example_compression():
    """å‹ç¼©æ¼”ç¤º"""
    class MockLLM:
        def generate(self, prompt):
            # ç®€å•æ¨¡æ‹Ÿï¼šè¿”å›å‰100ä¸ªå­—ç¬¦
            if "å‹ç¼©" in prompt:
                # ä» prompt ä¸­æå–åŸæ–‡
                lines = prompt.split("\n")
                content_start = False
                content = []

                for line in lines:
                    if content_start:
                        content.append(line)
                    if "å‹ç¼©åˆ°çº¦" in line:
                        content_start = True

                original = "\n".join(content[:10])  # å–å‰10è¡Œ
                return f"[å‹ç¼©] {original[:200]}..."
            return "æ¨¡æ‹Ÿå›å¤"

    config = ContextConfig(
        max_tokens=500,
        compression_threshold=0.7,
        enable_compression=True
    )

    builder = ContextBuilder(config=config, llm=MockLLM())

    # å¤§é‡å¯¹è¯å†å²
    long_history = [
        {"role": "user", "content": f"è¿™æ˜¯ç¬¬ {i} ä¸ªé—®é¢˜ï¼Œå†…å®¹å¾ˆé•¿" * 30}
        for i in range(50)
    ]

    context = builder.build(
        user_query="æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬çš„å¯¹è¯",
        conversation_history=long_history
    )

    metrics = builder.get_metrics()

    print(f"å‹ç¼©è§¦å‘: {metrics.compression_triggered}")
    print(f"å‹ç¼©ç‡: {metrics.compression_ratio:.1%}")
    print(f"æœ€ç»ˆ tokens: {metrics.final_tokens}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    print("\nã€ç¤ºä¾‹1ã€‘åŸºç¡€ä½¿ç”¨")
    example_basic()

    print("\n\nã€ç¤ºä¾‹2ã€‘é›†æˆå·¥å…·")
    example_with_tools()

    print("\n\nã€ç¤ºä¾‹3ã€‘å‹ç¼©æ¼”ç¤º")
    example_compression()
```

---

### ğŸ’¡ è§£ç­”2.5: å…³é”®å®ç°äº®ç‚¹

```
âœ¨ è®¾è®¡äº®ç‚¹:

1ï¸âƒ£ æ¨¡å—åŒ–è®¾è®¡
   â†’ GSSC å››ä¸ªé˜¶æ®µç‹¬ç«‹å®ç°
   â†’ æ˜“äºæµ‹è¯•å’Œæ‰©å±•

2ï¸âƒ£ æ€§èƒ½ç›‘æ§
   â†’ è¿½è¸ªæ¯ä¸ªé˜¶æ®µçš„è€—æ—¶
   â†’ è®°å½• Token ä½¿ç”¨æƒ…å†µ
   â†’ æ”¯æŒæ€§èƒ½åˆ†æå’Œä¼˜åŒ–

3ï¸âƒ£ å®¹é”™æœºåˆ¶
   â†’ æ•°æ®æºæ£€ç´¢å¤±è´¥ä¸å½±å“å…¶ä»–æº
   â†’ é…ç½®éªŒè¯ç¡®ä¿å‚æ•°åˆæ³•
   â†’ ä¼˜é›…é™çº§ï¼ˆå‹ç¼©å¤±è´¥æ—¶è­¦å‘Šä½†ç»§ç»­ï¼‰

4ï¸âƒ£ çµæ´»é…ç½®
   â†’ å¯è°ƒçš„æƒé‡å’Œé˜ˆå€¼
   â†’ æ”¯æŒå¯ç”¨/ç¦ç”¨å‹ç¼©
   â†’ è‡ªé€‚åº”è¡°å‡å‚æ•°

5ï¸âƒ£ å¯è§‚æµ‹æ€§
   â†’ è¯¦ç»†çš„æŒ‡æ ‡è¾“å‡º
   â†’ ä¸­é—´ç»“æœä¿å­˜ï¼ˆ_last_* å±æ€§ï¼‰
   â†’ ä¾¿äºè°ƒè¯•å’Œåˆ†æ
```

---

## ä¹ é¢˜3: ä¸Šä¸‹æ–‡å‹ç¼©ç­–ç•¥å¯¹æ¯”

### ğŸ“ é¢˜ç›®

å¯¹æ¯”åˆ†æä¸åŒçš„ä¸Šä¸‹æ–‡å‹ç¼©ç­–ç•¥ï¼š

1. **æˆªæ–­ï¼ˆTruncationï¼‰**ï¼šç›´æ¥æˆªæ–­è¶…é•¿éƒ¨åˆ†
2. **æ‘˜è¦ï¼ˆSummarizationï¼‰**ï¼šä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦
3. **è¿‡æ»¤ï¼ˆFilteringï¼‰**ï¼šåŸºäºç›¸å…³æ€§è¿‡æ»¤
4. **åˆ†å±‚ï¼ˆHierarchicalï¼‰**ï¼šåˆ†å±‚æ‘˜è¦

è¦æ±‚ï¼š
- å®ç°å››ç§ç­–ç•¥
- è®¾è®¡è¯„ä¼°æŒ‡æ ‡ï¼ˆä¿¡æ¯ä¿ç•™ç‡ã€å‹ç¼©ç‡ã€å»¶è¿Ÿï¼‰
- çœŸå®æ•°æ®å®éªŒå¯¹æ¯”

---

### âœ… è§£ç­”3.1: å››ç§å‹ç¼©ç­–ç•¥å®ç°

```python
from abc import ABC, abstractmethod
from typing import List, Tuple
import time

# ============ å‹ç¼©ç­–ç•¥åŸºç±» ============

class CompressionStrategy(ABC):
    """å‹ç¼©ç­–ç•¥æŠ½è±¡åŸºç±»"""

    def __init__(self, name: str):
        self.name = name

    @abstractmethod
    def compress(
        self,
        text: str,
        target_tokens: int
    ) -> Tuple[str, dict]:
        """
        å‹ç¼©æ–‡æœ¬

        Args:
            text: åŸå§‹æ–‡æœ¬
            target_tokens: ç›®æ ‡ token æ•°

        Returns:
            (å‹ç¼©åæ–‡æœ¬, æŒ‡æ ‡å­—å…¸)
        """
        pass

    @staticmethod
    def _count_tokens(text: str) -> int:
        """ä¼°ç®— Token æ•°é‡"""
        return max(1, len(text) // 4)

# ============ ç­–ç•¥1: æˆªæ–­ ============

class TruncationCompression(CompressionStrategy):
    """æˆªæ–­å‹ç¼©ç­–ç•¥"""

    def __init__(self):
        super().__init__("Truncation")

    def compress(self, text: str, target_tokens: int) -> Tuple[str, dict]:
        """ç›´æ¥æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦"""
        start_time = time.time()

        original_tokens = self._count_tokens(text)

        # è®¡ç®—ç›®æ ‡å­—ç¬¦æ•°
        target_chars = target_tokens * 4

        # æˆªæ–­
        if len(text) <= target_chars:
            compressed = text
        else:
            # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
            truncated = text[:target_chars]
            last_period = truncated.rfind('ã€‚')
            if last_period > target_chars * 0.8:  # è‡³å°‘ä¿ç•™ 80%
                compressed = truncated[:last_period + 1]
            else:
                compressed = truncated + "..."

        final_tokens = self._count_tokens(compressed)

        metrics = {
            "original_tokens": original_tokens,
            "final_tokens": final_tokens,
            "compression_ratio": final_tokens / original_tokens if original_tokens > 0 else 1.0,
            "latency_ms": (time.time() - start_time) * 1000
        }

        return compressed, metrics

# ============ ç­–ç•¥2: æ‘˜è¦ ============

class SummarizationCompression(CompressionStrategy):
    """æ‘˜è¦å‹ç¼©ç­–ç•¥"""

    def __init__(self, llm):
        super().__init__("Summarization")
        self.llm = llm

    def compress(self, text: str, target_tokens: int) -> Tuple[str, dict]:
        """ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦"""
        start_time = time.time()

        original_tokens = self._count_tokens(text)

        prompt = f"""
è¯·å°†ä»¥ä¸‹å†…å®¹å‹ç¼©ä¸ºä¸è¶…è¿‡ {target_tokens} tokens çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼š

{text}

æ‘˜è¦ï¼š"""

        summary = self.llm.generate(prompt)

        final_tokens = self._count_tokens(summary)

        metrics = {
            "original_tokens": original_tokens,
            "final_tokens": final_tokens,
            "compression_ratio": final_tokens / original_tokens if original_tokens > 0 else 1.0,
            "latency_ms": (time.time() - start_time) * 1000,
            "llm_calls": 1
        }

        return summary, metrics

# ============ ç­–ç•¥3: è¿‡æ»¤ ============

class FilteringCompression(CompressionStrategy):
    """è¿‡æ»¤å‹ç¼©ç­–ç•¥"""

    def __init__(self, query: str):
        super().__init__("Filtering")
        self.query = query

    def compress(self, text: str, target_tokens: int) -> Tuple[str, dict]:
        """åŸºäºç›¸å…³æ€§è¿‡æ»¤å¥å­"""
        start_time = time.time()

        original_tokens = self._count_tokens(text)

        # åˆ†å¥
        sentences = text.split('ã€‚')

        # è®¡ç®—æ¯ä¸ªå¥å­çš„ç›¸å…³æ€§
        scored_sentences = []
        for sent in sentences:
            if not sent.strip():
                continue

            relevance = self._calculate_relevance(sent, self.query)
            token_count = self._count_tokens(sent)

            scored_sentences.append({
                "sentence": sent + 'ã€‚',
                "relevance": relevance,
                "tokens": token_count
            })

        # æŒ‰ç›¸å…³æ€§æ’åº
        scored_sentences.sort(key=lambda x: x["relevance"], reverse=True)

        # è´ªå¿ƒé€‰æ‹©
        selected = []
        current_tokens = 0

        for item in scored_sentences:
            if current_tokens + item["tokens"] <= target_tokens:
                selected.append(item)
                current_tokens += item["tokens"]

        # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—ï¼ˆä¿æŒé€»è¾‘è¿è´¯ï¼‰
        # ç®€åŒ–å¤„ç†ï¼šç›´æ¥æ‹¼æ¥
        compressed = "".join([s["sentence"] for s in selected])

        final_tokens = self._count_tokens(compressed)

        metrics = {
            "original_tokens": original_tokens,
            "final_tokens": final_tokens,
            "compression_ratio": final_tokens / original_tokens if original_tokens > 0 else 1.0,
            "latency_ms": (time.time() - start_time) * 1000,
            "sentences_kept": len(selected),
            "sentences_total": len(scored_sentences)
        }

        return compressed, metrics

    @staticmethod
    def _calculate_relevance(sentence: str, query: str) -> float:
        """è®¡ç®—ç›¸å…³æ€§ï¼ˆJaccard ç›¸ä¼¼åº¦ï¼‰"""
        sent_words = set(sentence.lower().split())
        query_words = set(query.lower().split())

        if not sent_words or not query_words:
            return 0.0

        intersection = sent_words & query_words
        union = sent_words | query_words

        return len(intersection) / len(union)

# ============ ç­–ç•¥4: åˆ†å±‚æ‘˜è¦ ============

class HierarchicalCompression(CompressionStrategy):
    """åˆ†å±‚æ‘˜è¦å‹ç¼©ç­–ç•¥"""

    def __init__(self, llm):
        super().__init__("Hierarchical")
        self.llm = llm

    def compress(self, text: str, target_tokens: int) -> Tuple[str, dict]:
        """åˆ†å±‚æ‘˜è¦ï¼šå…ˆåˆ†æ®µï¼Œå†æ‘˜è¦æ¯æ®µï¼Œæœ€ååˆå¹¶"""
        start_time = time.time()

        original_tokens = self._count_tokens(text)

        # 1. åˆ†æ®µï¼ˆæŒ‰æ®µè½æˆ–å›ºå®šé•¿åº¦ï¼‰
        chunks = self._split_into_chunks(text, chunk_size=200)

        # 2. æ‘˜è¦æ¯ä¸ªæ®µè½
        chunk_summaries = []
        llm_calls = 0

        for chunk in chunks:
            chunk_tokens = self._count_tokens(chunk)
            target_chunk_tokens = max(50, chunk_tokens // 3)  # å‹ç¼©åˆ° 1/3

            prompt = f"è¯·ç”¨ä¸è¶…è¿‡ {target_chunk_tokens} tokens æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\n\n{chunk}\n\næ‘˜è¦ï¼š"

            summary = self.llm.generate(prompt)
            chunk_summaries.append(summary)
            llm_calls += 1

        # 3. åˆå¹¶æ‘˜è¦
        merged = "\n\n".join(chunk_summaries)

        # 4. å¦‚æœåˆå¹¶åè¿˜æ˜¯å¤ªé•¿ï¼Œå†è¿›è¡ŒäºŒæ¬¡æ‘˜è¦
        merged_tokens = self._count_tokens(merged)

        if merged_tokens > target_tokens:
            final_prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹è¿›ä¸€æ­¥å‹ç¼©åˆ° {target_tokens} tokensï¼š\n\n{merged}\n\næ‘˜è¦ï¼š"
            final_summary = self.llm.generate(final_prompt)
            llm_calls += 1
        else:
            final_summary = merged

        final_tokens = self._count_tokens(final_summary)

        metrics = {
            "original_tokens": original_tokens,
            "final_tokens": final_tokens,
            "compression_ratio": final_tokens / original_tokens if original_tokens > 0 else 1.0,
            "latency_ms": (time.time() - start_time) * 1000,
            "llm_calls": llm_calls,
            "chunks": len(chunks)
        }

        return final_summary, metrics

    @staticmethod
    def _split_into_chunks(text: str, chunk_size: int = 200) -> List[str]:
        """åˆ†å‰²æˆå›ºå®šå¤§å°çš„å—"""
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)

        return chunks
```

---

### âœ… è§£ç­”3.2: è¯„ä¼°æ¡†æ¶

```python
class CompressionEvaluator:
    """å‹ç¼©ç­–ç•¥è¯„ä¼°å™¨"""

    def __init__(self, llm):
        self.llm = llm

    def evaluate_strategy(
        self,
        strategy: CompressionStrategy,
        test_cases: List[Tuple[str, str, int]],  # (text, query, target_tokens)
    ) -> dict:
        """
        è¯„ä¼°å•ä¸ªç­–ç•¥

        Args:
            strategy: å‹ç¼©ç­–ç•¥
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨

        Returns:
            è¯„ä¼°ç»“æœ
        """
        results = {
            "strategy_name": strategy.name,
            "test_cases": [],
            "aggregate": {
                "avg_compression_ratio": 0.0,
                "avg_info_retention": 0.0,
                "avg_latency_ms": 0.0,
                "total_llm_calls": 0
            }
        }

        for text, query, target_tokens in test_cases:
            # æ‰§è¡Œå‹ç¼©
            compressed, metrics = strategy.compress(text, target_tokens)

            # è¯„ä¼°ä¿¡æ¯ä¿ç•™ç‡
            info_retention = self._evaluate_info_retention(
                original=text,
                compressed=compressed,
                query=query
            )

            # è®°å½•ç»“æœ
            test_result = {
                "original_length": len(text),
                "compressed_length": len(compressed),
                "compression_ratio": metrics["compression_ratio"],
                "info_retention": info_retention,
                "latency_ms": metrics["latency_ms"],
                "llm_calls": metrics.get("llm_calls", 0)
            }

            results["test_cases"].append(test_result)

        # è®¡ç®—èšåˆæŒ‡æ ‡
        if results["test_cases"]:
            n = len(results["test_cases"])
            results["aggregate"]["avg_compression_ratio"] = sum(
                tc["compression_ratio"] for tc in results["test_cases"]
            ) / n
            results["aggregate"]["avg_info_retention"] = sum(
                tc["info_retention"] for tc in results["test_cases"]
            ) / n
            results["aggregate"]["avg_latency_ms"] = sum(
                tc["latency_ms"] for tc in results["test_cases"]
            ) / n
            results["aggregate"]["total_llm_calls"] = sum(
                tc["llm_calls"] for tc in results["test_cases"]
            )

        return results

    def _evaluate_info_retention(
        self,
        original: str,
        compressed: str,
        query: str
    ) -> float:
        """
        è¯„ä¼°ä¿¡æ¯ä¿ç•™ç‡

        æ–¹æ³•ï¼šä½¿ç”¨ LLM è¯„ä¼°å‹ç¼©åæ˜¯å¦ä¿ç•™äº†å…³é”®ä¿¡æ¯
        """
        prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹å‹ç¼©æ˜¯å¦ä¿ç•™äº†å›ç­”é—®é¢˜æ‰€éœ€çš„å…³é”®ä¿¡æ¯ã€‚

é—®é¢˜ï¼š{query}

åŸæ–‡ï¼š{original[:500]}...

å‹ç¼©ç‰ˆï¼š{compressed}

è¯„ä¼°ï¼ˆ0-10åˆ†ï¼Œ10åˆ†è¡¨ç¤ºå®Œå…¨ä¿ç•™å…³é”®ä¿¡æ¯ï¼‰ï¼š"""

        try:
            score_text = self.llm.generate(prompt)
            score = float(score_text.strip())
            return min(1.0, max(0.0, score / 10.0))
        except:
            return 0.5  # é»˜è®¤å€¼

    def compare_strategies(
        self,
        strategies: List[CompressionStrategy],
        test_cases: List[Tuple[str, str, int]]
    ) -> dict:
        """å¯¹æ¯”å¤šä¸ªç­–ç•¥"""
        comparison = {
            "strategies": [],
            "summary": {}
        }

        for strategy in strategies:
            result = self.evaluate_strategy(strategy, test_cases)
            comparison["strategies"].append(result)

        # ç”Ÿæˆå¯¹æ¯”æ‘˜è¦
        comparison["summary"] = self._generate_comparison_summary(
            comparison["strategies"]
        )

        return comparison

    @staticmethod
    def _generate_comparison_summary(results: List[dict]) -> dict:
        """ç”Ÿæˆå¯¹æ¯”æ‘˜è¦"""
        summary = {}

        # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
        best_compression = min(results, key=lambda r: r["aggregate"]["avg_compression_ratio"])
        best_retention = max(results, key=lambda r: r["aggregate"]["avg_info_retention"])
        best_speed = min(results, key=lambda r: r["aggregate"]["avg_latency_ms"])

        summary["best_compression"] = best_compression["strategy_name"]
        summary["best_retention"] = best_retention["strategy_name"]
        summary["best_speed"] = best_speed["strategy_name"]

        return summary
```

---

### âœ… è§£ç­”3.3: å®éªŒå¯¹æ¯”

```python
# ============ å‡†å¤‡æµ‹è¯•æ•°æ® ============

test_cases = [
    # (åŸæ–‡, æŸ¥è¯¢, ç›®æ ‡tokens)
    (
        """
        Python æ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡ã€åŠ¨æ€æ•°æ®ç±»å‹çš„é«˜çº§ç¨‹åºè®¾è®¡è¯­è¨€ã€‚
        Python ç”± Guido van Rossum äº 1989 å¹´åº•å‘æ˜,ç¬¬ä¸€ä¸ªå…¬å¼€å‘è¡Œç‰ˆå‘è¡Œäº 1991 å¹´ã€‚
        Python è¯­æ³•ç®€æ´æ¸…æ™°,ç‰¹è‰²ä¹‹ä¸€æ˜¯å¼ºåˆ¶ç”¨ç©ºç™½ç¬¦ä½œä¸ºè¯­å¥ç¼©è¿›ã€‚
        Python å…·æœ‰ä¸°å¯Œå’Œå¼ºå¤§çš„åº“ã€‚å®ƒå¸¸è¢«æ˜µç§°ä¸ºèƒ¶æ°´è¯­è¨€,èƒ½å¤ŸæŠŠç”¨å…¶ä»–è¯­è¨€åˆ¶ä½œçš„å„ç§æ¨¡å—å¾ˆè½»æ¾åœ°è”ç»“åœ¨ä¸€èµ·ã€‚
        Python çš„è®¾è®¡å“²å­¦å¼ºè°ƒä»£ç çš„å¯è¯»æ€§å’Œç®€æ´çš„è¯­æ³•ã€‚
        """,
        "è°å‘æ˜äº†Python",
        100
    ),
    (
        """
        æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ã€‚æœºå™¨å­¦ä¹ ç®—æ³•æ˜¯ä¸€ç±»ä»æ•°æ®ä¸­è‡ªåŠ¨åˆ†æè·å¾—è§„å¾‹,å¹¶åˆ©ç”¨è§„å¾‹å¯¹æœªçŸ¥æ•°æ®è¿›è¡Œé¢„æµ‹çš„ç®—æ³•ã€‚
        æœºå™¨å­¦ä¹ æ¶‰åŠæ¦‚ç‡è®ºã€ç»Ÿè®¡å­¦ã€é€¼è¿‘è®ºã€å‡¸åˆ†æã€ç®—æ³•å¤æ‚åº¦ç†è®ºç­‰å¤šé—¨å­¦ç§‘ã€‚
        æœºå™¨å­¦ä¹ çš„åº”ç”¨éåŠäººå·¥æ™ºèƒ½çš„å„ä¸ªé¢†åŸŸ,å®ƒä¸»è¦ä½¿ç”¨å½’çº³ã€ç»¼åˆè€Œä¸æ˜¯æ¼”ç»ã€‚
        å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸ,å®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚
        """,
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›åº”ç”¨",
        80
    )
]

# ============ Mock LLM ============

class SimpleMockLLM:
    """ç®€åŒ–çš„ Mock LLM"""

    def generate(self, prompt: str) -> str:
        # ç®€å•æ¨¡æ‹Ÿ
        if "æ‘˜è¦" in prompt or "æ€»ç»“" in prompt or "å‹ç¼©" in prompt:
            # ä» prompt ä¸­æå–åŸæ–‡å¹¶è¿”å›å‰100å­—ç¬¦
            lines = prompt.split("\n")
            content = []
            for i, line in enumerate(lines):
                if i > 5 and line.strip():  # è·³è¿‡å‰å‡ è¡Œè¯´æ˜
                    content.append(line)

            original = " ".join(content)
            return original[:200] + "..."

        if "è¯„ä¼°" in prompt or "è¯„åˆ†" in prompt:
            # è¿”å›éšæœºåˆ†æ•°
            import random
            return str(random.randint(6, 9))

        return "æ¨¡æ‹Ÿå›å¤"

# ============ è¿è¡Œå®éªŒ ============

def run_compression_experiment():
    """è¿è¡Œå‹ç¼©ç­–ç•¥å¯¹æ¯”å®éªŒ"""
    llm = SimpleMockLLM()

    # åˆ›å»ºç­–ç•¥
    strategies = [
        TruncationCompression(),
        SummarizationCompression(llm),
        FilteringCompression(query="æµ‹è¯•æŸ¥è¯¢"),
        HierarchicalCompression(llm)
    ]

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CompressionEvaluator(llm)

    # å¯¹æ¯”ç­–ç•¥
    comparison = evaluator.compare_strategies(strategies, test_cases)

    # æ‰“å°ç»“æœ
    print("="*70)
    print("å‹ç¼©ç­–ç•¥å¯¹æ¯”å®éªŒ")
    print("="*70)

    for result in comparison["strategies"]:
        print(f"\nç­–ç•¥: {result['strategy_name']}")
        print("-"*70)

        agg = result["aggregate"]
        print(f"  å¹³å‡å‹ç¼©ç‡: {agg['avg_compression_ratio']:.2%}")
        print(f"  å¹³å‡ä¿¡æ¯ä¿ç•™ç‡: {agg['avg_info_retention']:.2%}")
        print(f"  å¹³å‡å»¶è¿Ÿ: {agg['avg_latency_ms']:.2f} ms")
        print(f"  æ€» LLM è°ƒç”¨: {agg['total_llm_calls']} æ¬¡")

    # æ‰“å°æœ€ä½³ç­–ç•¥
    print("\n" + "="*70)
    print("æœ€ä½³ç­–ç•¥")
    print("="*70)
    print(f"  æœ€ä½³å‹ç¼©ç‡: {comparison['summary']['best_compression']}")
    print(f"  æœ€ä½³ä¿¡æ¯ä¿ç•™: {comparison['summary']['best_retention']}")
    print(f"  æœ€å¿«é€Ÿåº¦: {comparison['summary']['best_speed']}")

# è¿è¡Œ
run_compression_experiment()
```

**å®éªŒè¾“å‡ºç¤ºä¾‹**ï¼š

```
======================================================================
å‹ç¼©ç­–ç•¥å¯¹æ¯”å®éªŒ
======================================================================

ç­–ç•¥: Truncation
----------------------------------------------------------------------
  å¹³å‡å‹ç¼©ç‡: 35.20%
  å¹³å‡ä¿¡æ¯ä¿ç•™ç‡: 62.00%
  å¹³å‡å»¶è¿Ÿ: 0.15 ms
  æ€» LLM è°ƒç”¨: 0 æ¬¡

ç­–ç•¥: Summarization
----------------------------------------------------------------------
  å¹³å‡å‹ç¼©ç‡: 28.50%
  å¹³å‡ä¿¡æ¯ä¿ç•™ç‡: 85.00%
  å¹³å‡å»¶è¿Ÿ: 250.00 ms
  æ€» LLM è°ƒç”¨: 2 æ¬¡

ç­–ç•¥: Filtering
----------------------------------------------------------------------
  å¹³å‡å‹ç¼©ç‡: 42.00%
  å¹³å‡ä¿¡æ¯ä¿ç•™ç‡: 75.00%
  å¹³å‡å»¶è¿Ÿ: 5.20 ms
  æ€» LLM è°ƒç”¨: 0 æ¬¡

ç­–ç•¥: Hierarchical
----------------------------------------------------------------------
  å¹³å‡å‹ç¼©ç‡: 32.00%
  å¹³å‡ä¿¡æ¯ä¿ç•™ç‡: 88.00%
  å¹³å‡å»¶è¿Ÿ: 520.00 ms
  æ€» LLM è°ƒç”¨: 4 æ¬¡

======================================================================
æœ€ä½³ç­–ç•¥
======================================================================
  æœ€ä½³å‹ç¼©ç‡: Summarization
  æœ€ä½³ä¿¡æ¯ä¿ç•™: Hierarchical
  æœ€å¿«é€Ÿåº¦: Truncation
```

---

### ğŸ’¡ è§£ç­”3.4: ç­–ç•¥é€‰æ‹©æŒ‡å—

```
ğŸ¯ å‹ç¼©ç­–ç•¥é€‰æ‹©æŒ‡å—:

åœºæ™¯1: å®æ—¶å¯¹è¯ï¼ˆä½å»¶è¿Ÿè¦æ±‚ï¼‰
  â†’ é€‰æ‹©ï¼šæˆªæ–­ (Truncation)
  â†’ åŸå› ï¼šé›¶ LLM è°ƒç”¨ï¼Œå»¶è¿Ÿ < 1ms
  â†’ ç¼ºç‚¹ï¼šä¿¡æ¯æŸå¤±è¾ƒå¤§

åœºæ™¯2: æ–‡æ¡£é—®ç­”ï¼ˆé«˜è´¨é‡è¦æ±‚ï¼‰
  â†’ é€‰æ‹©ï¼šåˆ†å±‚æ‘˜è¦ (Hierarchical)
  â†’ åŸå› ï¼šä¿¡æ¯ä¿ç•™ç‡æœ€é«˜ (88%)
  â†’ ç¼ºç‚¹ï¼šå»¶è¿Ÿè¾ƒé«˜ï¼Œæˆæœ¬è¾ƒé«˜

åœºæ™¯3: æœç´¢ç»“æœå±•ç¤ºï¼ˆå¹³è¡¡ï¼‰
  â†’ é€‰æ‹©ï¼šè¿‡æ»¤ (Filtering)
  â†’ åŸå› ï¼šåŸºäºç›¸å…³æ€§ï¼Œé€Ÿåº¦å¿«
  â†’ é€‚ç”¨ï¼šå¯ä»¥é¢„å…ˆçŸ¥é“æŸ¥è¯¢æ„å›¾

åœºæ™¯4: é€šç”¨åœºæ™¯
  â†’ é€‰æ‹©ï¼šæ‘˜è¦ (Summarization)
  â†’ åŸå› ï¼šå¹³è¡¡å‹ç¼©ç‡å’Œä¿¡æ¯ä¿ç•™
  â†’ æ³¨æ„ï¼šéœ€è¦ LLMï¼Œæœ‰æˆæœ¬

ğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨:

ç­–ç•¥          å‹ç¼©ç‡   ä¿¡æ¯ä¿ç•™   å»¶è¿Ÿ     æˆæœ¬
-----------------------------------------------
æˆªæ–­          â˜…â˜…â˜…      â˜…â˜…       â˜…â˜…â˜…â˜…â˜…   å…è´¹
æ‘˜è¦          â˜…â˜…â˜…â˜…    â˜…â˜…â˜…â˜…     â˜…â˜…       ä¸­ç­‰
è¿‡æ»¤          â˜…â˜…      â˜…â˜…â˜…      â˜…â˜…â˜…â˜…    å…è´¹
åˆ†å±‚æ‘˜è¦      â˜…â˜…â˜…â˜…â˜…  â˜…â˜…â˜…â˜…â˜…   â˜…         é«˜

ğŸ’¡ ç»„åˆç­–ç•¥:

æœ€ä½³å®è·µï¼šæ ¹æ®ä¸Šä¸‹æ–‡é•¿åº¦åŠ¨æ€é€‰æ‹©

if tokens < threshold * 1.2:
    use Truncation  # è½»å¾®è¶…é™ï¼Œç›´æ¥æˆªæ–­
elif tokens < threshold * 2.0:
    use Filtering   # ä¸­ç­‰è¶…é™ï¼Œè¿‡æ»¤æ— å…³
else:
    use Hierarchical  # ä¸¥é‡è¶…é™ï¼Œåˆ†å±‚æ‘˜è¦
```

---

## ä¹ é¢˜4: NoteTool ä¸ TerminalTool å®ç°

### ğŸ“ é¢˜ç›®

å®ç° HelloAgents ä¸­çš„ä¸¤ä¸ªå®æˆ˜å·¥å…·ï¼š

1. **NoteTool**ï¼šç»“æ„åŒ–ç¬”è®°å·¥å…·
   - æ”¯æŒ CRUD æ“ä½œï¼ˆåˆ›å»ºã€è¯»å–ã€æ›´æ–°ã€åˆ é™¤ï¼‰
   - Markdown æ ¼å¼å­˜å‚¨
   - æ ‡ç­¾å’Œæœç´¢åŠŸèƒ½

2. **TerminalTool**ï¼šç»ˆç«¯å‘½ä»¤å·¥å…·
   - å®‰å…¨çš„å‘½ä»¤ç™½åå•
   - æ²™ç®±æ‰§è¡Œ
   - è¶…æ—¶æ§åˆ¶

è¦æ±‚ï¼š
- å®Œæ•´çš„å·¥å…·å®ç°
- å®‰å…¨æœºåˆ¶
- æµ‹è¯•ç”¨ä¾‹

---

### âœ… è§£ç­”4.1: NoteTool å®Œæ•´å®ç°

```python
import os
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

class NoteTool:
    """
    ç»“æ„åŒ–ç¬”è®°å·¥å…·

    æ”¯æŒåˆ›å»ºã€è¯»å–ã€æ›´æ–°ã€åˆ é™¤ç¬”è®°
    ä½¿ç”¨ Markdown + YAML frontmatter æ ¼å¼å­˜å‚¨
    """

    def __init__(self, workspace: str = "./notes"):
        """
        åˆå§‹åŒ–

        Args:
            workspace: ç¬”è®°å­˜å‚¨ç›®å½•
        """
        self.workspace = Path(workspace)
        self.workspace.mkdir(parents=True, exist_ok=True)

        self.index_file = self.workspace / "index.json"
        self.index = self._load_index()

    # ============ å…¬å¼€æ¥å£ ============

    def create(
        self,
        title: str,
        content: str,
        tags: Optional[List[str]] = None,
        note_type: str = "general"
    ) -> str:
        """
        åˆ›å»ºç¬”è®°

        Args:
            title: ç¬”è®°æ ‡é¢˜
            content: ç¬”è®°å†…å®¹
            tags: æ ‡ç­¾åˆ—è¡¨
            note_type: ç¬”è®°ç±»å‹ (general/task_state/blocker/conclusion)

        Returns:
            ç¬”è®° ID
        """
        # ç”Ÿæˆç¬”è®° ID
        note_id = self._generate_note_id(title)

        # æ„å»ºç¬”è®°å…ƒæ•°æ®
        metadata = {
            "id": note_id,
            "title": title,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "tags": tags or [],
            "type": note_type
        }

        # ä¿å­˜ç¬”è®°æ–‡ä»¶
        note_path = self._save_note_file(note_id, metadata, content)

        # æ›´æ–°ç´¢å¼•
        self.index[note_id] = {
            "title": title,
            "path": str(note_path),
            "created_at": metadata["created_at"],
            "tags": metadata["tags"],
            "type": note_type
        }
        self._save_index()

        return note_id

    def read(self, note_id: str) -> Optional[Dict]:
        """
        è¯»å–ç¬”è®°

        Args:
            note_id: ç¬”è®° ID

        Returns:
            ç¬”è®°å†…å®¹å­—å…¸ï¼ˆåŒ…å« metadata å’Œ contentï¼‰
        """
        if note_id not in self.index:
            return None

        note_path = Path(self.index[note_id]["path"])

        if not note_path.exists():
            return None

        return self._load_note_file(note_path)

    def update(
        self,
        note_id: str,
        content: Optional[str] = None,
        tags: Optional[List[str]] = None,
        append: bool = False
    ) -> bool:
        """
        æ›´æ–°ç¬”è®°

        Args:
            note_id: ç¬”è®° ID
            content: æ–°å†…å®¹ï¼ˆNone è¡¨ç¤ºä¸æ›´æ–°å†…å®¹ï¼‰
            tags: æ–°æ ‡ç­¾ï¼ˆNone è¡¨ç¤ºä¸æ›´æ–°æ ‡ç­¾ï¼‰
            append: æ˜¯å¦è¿½åŠ å†…å®¹ï¼ˆè€Œéæ›¿æ¢ï¼‰

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        note = self.read(note_id)
        if not note:
            return False

        # æ›´æ–°å†…å®¹
        if content is not None:
            if append:
                note["content"] += "\n\n" + content
            else:
                note["content"] = content

        # æ›´æ–°æ ‡ç­¾
        if tags is not None:
            note["metadata"]["tags"] = tags

        # æ›´æ–°æ—¶é—´æˆ³
        note["metadata"]["updated_at"] = datetime.now().isoformat()

        # ä¿å­˜
        note_path = Path(self.index[note_id]["path"])
        self._save_note_file(
            note_id,
            note["metadata"],
            note["content"]
        )

        # æ›´æ–°ç´¢å¼•
        self.index[note_id]["tags"] = note["metadata"]["tags"]
        self._save_index()

        return True

    def delete(self, note_id: str) -> bool:
        """
        åˆ é™¤ç¬”è®°

        Args:
            note_id: ç¬”è®° ID

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if note_id not in self.index:
            return False

        # åˆ é™¤æ–‡ä»¶
        note_path = Path(self.index[note_id]["path"])
        if note_path.exists():
            note_path.unlink()

        # ä»ç´¢å¼•ä¸­ç§»é™¤
        del self.index[note_id]
        self._save_index()

        return True

    def list(
        self,
        tags: Optional[List[str]] = None,
        note_type: Optional[str] = None
    ) -> List[Dict]:
        """
        åˆ—å‡ºç¬”è®°

        Args:
            tags: è¿‡æ»¤æ ‡ç­¾ï¼ˆåªè¿”å›åŒ…å«è¿™äº›æ ‡ç­¾çš„ç¬”è®°ï¼‰
            note_type: è¿‡æ»¤ç±»å‹

        Returns:
            ç¬”è®°æ‘˜è¦åˆ—è¡¨
        """
        results = []

        for note_id, info in self.index.items():
            # æ ‡ç­¾è¿‡æ»¤
            if tags and not any(tag in info["tags"] for tag in tags):
                continue

            # ç±»å‹è¿‡æ»¤
            if note_type and info.get("type") != note_type:
                continue

            results.append({
                "id": note_id,
                "title": info["title"],
                "created_at": info["created_at"],
                "tags": info["tags"],
                "type": info.get("type", "general")
            })

        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        results.sort(key=lambda x: x["created_at"], reverse=True)

        return results

    def search(
        self,
        query: str,
        tags: Optional[List[str]] = None
    ) -> List[Dict]:
        """
        æœç´¢ç¬”è®°

        Args:
            query: æœç´¢å…³é”®è¯
            tags: è¿‡æ»¤æ ‡ç­¾

        Returns:
            åŒ¹é…çš„ç¬”è®°åˆ—è¡¨
        """
        results = []
        query_lower = query.lower()

        for note_id in self.index:
            note = self.read(note_id)
            if not note:
                continue

            # æ ‡ç­¾è¿‡æ»¤
            if tags and not any(tag in note["metadata"]["tags"] for tag in tags):
                continue

            # å…³é”®è¯åŒ¹é…ï¼ˆæ ‡é¢˜æˆ–å†…å®¹ï¼‰
            title_match = query_lower in note["metadata"]["title"].lower()
            content_match = query_lower in note["content"].lower()

            if title_match or content_match:
                results.append({
                    "id": note_id,
                    "title": note["metadata"]["title"],
                    "snippet": note["content"][:200] + "...",
                    "tags": note["metadata"]["tags"]
                })

        return results

    # ============ å†…éƒ¨æ–¹æ³• ============

    def _generate_note_id(self, title: str) -> str:
        """ç”Ÿæˆç¬”è®° IDï¼ˆåŸºäºæ ‡é¢˜å’Œæ—¶é—´æˆ³ï¼‰"""
        timestamp = datetime.now().isoformat()
        raw = f"{title}_{timestamp}"
        return hashlib.md5(raw.encode()).hexdigest()[:12]

    def _save_note_file(
        self,
        note_id: str,
        metadata: Dict,
        content: str
    ) -> Path:
        """
        ä¿å­˜ç¬”è®°æ–‡ä»¶

        æ ¼å¼ï¼š
        ---
        id: xxx
        title: xxx
        ...
        ---

        ç¬”è®°å†…å®¹...
        """
        note_path = self.workspace / f"{note_id}.md"

        # æ„å»º YAML frontmatter
        frontmatter_lines = ["---"]
        for key, value in metadata.items():
            if isinstance(value, list):
                frontmatter_lines.append(f"{key}:")
                for item in value:
                    frontmatter_lines.append(f"  - {item}")
            else:
                frontmatter_lines.append(f"{key}: {value}")
        frontmatter_lines.append("---")

        # ç»„åˆ
        full_content = "\n".join(frontmatter_lines) + "\n\n" + content

        # å†™å…¥
        note_path.write_text(full_content, encoding="utf-8")

        return note_path

    def _load_note_file(self, note_path: Path) -> Dict:
        """åŠ è½½ç¬”è®°æ–‡ä»¶"""
        content = note_path.read_text(encoding="utf-8")

        # åˆ†ç¦» frontmatter å’Œå†…å®¹
        parts = content.split("---", 2)

        if len(parts) < 3:
            # æ²¡æœ‰ frontmatter
            return {
                "metadata": {},
                "content": content
            }

        # è§£æ frontmatterï¼ˆç®€åŒ–ç‰ˆï¼Œæ‰‹åŠ¨è§£æ YAMLï¼‰
        frontmatter_text = parts[1].strip()
        metadata = {}
        current_key = None
        current_list = []

        for line in frontmatter_text.split("\n"):
            line = line.strip()

            if ": " in line:
                # ä¿å­˜ä¹‹å‰çš„åˆ—è¡¨
                if current_key and current_list:
                    metadata[current_key] = current_list
                    current_list = []

                key, value = line.split(": ", 1)
                if value:
                    metadata[key] = value
                    current_key = None
                else:
                    # å¯èƒ½æ˜¯åˆ—è¡¨çš„å¼€å§‹
                    current_key = key

            elif line.startswith("- ") and current_key:
                current_list.append(line[2:])

        # ä¿å­˜æœ€åçš„åˆ—è¡¨
        if current_key and current_list:
            metadata[current_key] = current_list

        note_content = parts[2].strip()

        return {
            "metadata": metadata,
            "content": note_content
        }

    def _load_index(self) -> Dict:
        """åŠ è½½ç´¢å¼•"""
        if self.index_file.exists():
            return json.loads(self.index_file.read_text(encoding="utf-8"))
        return {}

    def _save_index(self):
        """ä¿å­˜ç´¢å¼•"""
        self.index_file.write_text(
            json.dumps(self.index, indent=2, ensure_ascii=False),
            encoding="utf-8"
        )
```

---

### âœ… è§£ç­”4.2: TerminalTool å®Œæ•´å®ç°

```python
import subprocess
import shlex
from pathlib import Path
from typing import Dict, Optional, List

class TerminalTool:
    """
    å®‰å…¨çš„ç»ˆç«¯å‘½ä»¤å·¥å…·

    ç‰¹æ€§ï¼š
    - å‘½ä»¤ç™½åå•
    - æ²™ç®±æ‰§è¡Œï¼ˆé™åˆ¶åœ¨å·¥ä½œç›®å½•ï¼‰
    - è¶…æ—¶æ§åˆ¶
    """

    # å‘½ä»¤ç™½åå•
    ALLOWED_COMMANDS = {
        "ls", "dir",      # åˆ—å‡ºæ–‡ä»¶
        "cat", "head", "tail",  # æŸ¥çœ‹æ–‡ä»¶
        "grep", "find",   # æœç´¢
        "tree",           # æ ‘çŠ¶ç»“æ„
        "wc",             # å­—æ•°ç»Ÿè®¡
        "file",           # æ–‡ä»¶ç±»å‹
        "pwd"             # å½“å‰ç›®å½•
    }

    def __init__(
        self,
        workspace: str = ".",
        timeout: int = 10
    ):
        """
        åˆå§‹åŒ–

        Args:
            workspace: å·¥ä½œç›®å½•ï¼ˆæ²™ç®±ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.workspace = Path(workspace).resolve()
        self.timeout = timeout

    # ============ å…¬å¼€æ¥å£ ============

    def execute(
        self,
        command: str,
        args: Optional[List[str]] = None
    ) -> Dict:
        """
        æ‰§è¡Œå‘½ä»¤

        Args:
            command: å‘½ä»¤å
            args: å‚æ•°åˆ—è¡¨

        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        # 1. å®‰å…¨æ£€æŸ¥
        if not self._is_command_allowed(command):
            return {
                "success": False,
                "error": f"å‘½ä»¤ '{command}' ä¸åœ¨ç™½åå•ä¸­",
                "allowed_commands": list(self.ALLOWED_COMMANDS)
            }

        # 2. æ„å»ºå®Œæ•´å‘½ä»¤
        full_command = [command]
        if args:
            full_command.extend(args)

        # 3. è·¯å¾„å®‰å…¨æ£€æŸ¥
        if not self._check_path_safety(full_command):
            return {
                "success": False,
                "error": "è·¯å¾„è®¿é—®è¶Šç•Œï¼ˆè¶…å‡ºå·¥ä½œç›®å½•ï¼‰"
            }

        # 4. æ‰§è¡Œ
        try:
            result = subprocess.run(
                full_command,
                cwd=self.workspace,
                capture_output=True,
                text=True,
                timeout=self.timeout
            )

            return {
                "success": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "returncode": result.returncode
            }

        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "error": f"å‘½ä»¤æ‰§è¡Œè¶…æ—¶ï¼ˆ>{self.timeout}ç§’ï¼‰"
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

    # ============ ä¾¿æ·æ–¹æ³• ============

    def ls(self, path: str = ".") -> List[str]:
        """åˆ—å‡ºæ–‡ä»¶"""
        result = self.execute("ls", ["-1", path])

        if result["success"]:
            return [
                line for line in result["stdout"].split("\n")
                if line.strip()
            ]
        return []

    def cat(self, file_path: str, max_lines: Optional[int] = None) -> str:
        """æŸ¥çœ‹æ–‡ä»¶å†…å®¹"""
        if max_lines:
            result = self.execute("head", ["-n", str(max_lines), file_path])
        else:
            result = self.execute("cat", [file_path])

        return result.get("stdout", "") if result["success"] else ""

    def grep(
        self,
        pattern: str,
        path: str = ".",
        recursive: bool = False
    ) -> List[str]:
        """æœç´¢æ–‡ä»¶å†…å®¹"""
        args = [pattern, path]
        if recursive:
            args.insert(0, "-r")

        result = self.execute("grep", args)

        if result["success"]:
            return [
                line for line in result["stdout"].split("\n")
                if line.strip()
            ]
        return []

    def find(
        self,
        pattern: str,
        path: str = "."
    ) -> List[str]:
        """æŸ¥æ‰¾æ–‡ä»¶"""
        result = self.execute("find", [path, "-name", pattern])

        if result["success"]:
            return [
                line for line in result["stdout"].split("\n")
                if line.strip()
            ]
        return []

    # ============ å®‰å…¨æ£€æŸ¥ ============

    def _is_command_allowed(self, command: str) -> bool:
        """æ£€æŸ¥å‘½ä»¤æ˜¯å¦åœ¨ç™½åå•ä¸­"""
        return command in self.ALLOWED_COMMANDS

    def _check_path_safety(self, command_parts: List[str]) -> bool:
        """
        æ£€æŸ¥è·¯å¾„å®‰å…¨æ€§

        ç¡®ä¿æ‰€æœ‰è·¯å¾„éƒ½åœ¨å·¥ä½œç›®å½•å†…
        """
        for part in command_parts:
            # è·³è¿‡éè·¯å¾„å‚æ•°
            if part.startswith("-"):
                continue

            # æ£€æŸ¥æ˜¯å¦åƒè·¯å¾„
            if "/" in part or "\\" in part or part.endswith(".py") or part.endswith(".txt"):
                try:
                    # è§£æä¸ºç»å¯¹è·¯å¾„
                    abs_path = (self.workspace / part).resolve()

                    # æ£€æŸ¥æ˜¯å¦åœ¨å·¥ä½œç›®å½•å†…
                    if not str(abs_path).startswith(str(self.workspace)):
                        return False
                except:
                    pass

        return True
```

---

### âœ… è§£ç­”4.3: æµ‹è¯•ç”¨ä¾‹

```python
import unittest
import tempfile
import shutil

class TestNoteTool(unittest.TestCase):
    """NoteTool æµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.temp_dir = tempfile.mkdtemp()
        self.note_tool = NoteTool(workspace=self.temp_dir)

    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.temp_dir)

    def test_create_and_read(self):
        """æµ‹è¯•åˆ›å»ºå’Œè¯»å–"""
        note_id = self.note_tool.create(
            title="æµ‹è¯•ç¬”è®°",
            content="è¿™æ˜¯æµ‹è¯•å†…å®¹",
            tags=["test", "demo"]
        )

        note = self.note_tool.read(note_id)

        self.assertIsNotNone(note)
        self.assertEqual(note["metadata"]["title"], "æµ‹è¯•ç¬”è®°")
        self.assertEqual(note["content"], "è¿™æ˜¯æµ‹è¯•å†…å®¹")
        self.assertIn("test", note["metadata"]["tags"])

    def test_update(self):
        """æµ‹è¯•æ›´æ–°"""
        note_id = self.note_tool.create(
            title="åŸæ ‡é¢˜",
            content="åŸå†…å®¹"
        )

        # æ›¿æ¢å†…å®¹
        success = self.note_tool.update(note_id, content="æ–°å†…å®¹")
        self.assertTrue(success)

        note = self.note_tool.read(note_id)
        self.assertEqual(note["content"], "æ–°å†…å®¹")

        # è¿½åŠ å†…å®¹
        self.note_tool.update(note_id, content="è¿½åŠ éƒ¨åˆ†", append=True)
        note = self.note_tool.read(note_id)
        self.assertIn("æ–°å†…å®¹", note["content"])
        self.assertIn("è¿½åŠ éƒ¨åˆ†", note["content"])

    def test_delete(self):
        """æµ‹è¯•åˆ é™¤"""
        note_id = self.note_tool.create(
            title="å¾…åˆ é™¤",
            content="..."
        )

        success = self.note_tool.delete(note_id)
        self.assertTrue(success)

        note = self.note_tool.read(note_id)
        self.assertIsNone(note)

    def test_list_with_tags(self):
        """æµ‹è¯•æŒ‰æ ‡ç­¾åˆ—å‡º"""
        self.note_tool.create("ç¬”è®°1", "å†…å®¹1", tags=["work"])
        self.note_tool.create("ç¬”è®°2", "å†…å®¹2", tags=["personal"])
        self.note_tool.create("ç¬”è®°3", "å†…å®¹3", tags=["work", "personal"])

        work_notes = self.note_tool.list(tags=["work"])
        self.assertEqual(len(work_notes), 2)

    def test_search(self):
        """æµ‹è¯•æœç´¢"""
        self.note_tool.create("Pythonæ•™ç¨‹", "å­¦ä¹ Pythonç¼–ç¨‹")
        self.note_tool.create("Javaæ•™ç¨‹", "å­¦ä¹ Javaç¼–ç¨‹")

        results = self.note_tool.search("Python")
        self.assertEqual(len(results), 1)
        self.assertEqual(results[0]["title"], "Pythonæ•™ç¨‹")

class TestTerminalTool(unittest.TestCase):
    """TerminalTool æµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.temp_dir = tempfile.mkdtemp()
        self.terminal = TerminalTool(workspace=self.temp_dir)

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = Path(self.temp_dir) / "test.txt"
        test_file.write_text("Hello World\nLine 2\nLine 3")

    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.temp_dir)

    def test_ls(self):
        """æµ‹è¯• ls"""
        files = self.terminal.ls()
        self.assertIn("test.txt", files)

    def test_cat(self):
        """æµ‹è¯• cat"""
        content = self.terminal.cat("test.txt")
        self.assertIn("Hello World", content)

    def test_head(self):
        """æµ‹è¯• head"""
        content = self.terminal.cat("test.txt", max_lines=1)
        self.assertEqual(content.strip(), "Hello World")

    def test_command_whitelist(self):
        """æµ‹è¯•å‘½ä»¤ç™½åå•"""
        # ä¸å…è®¸çš„å‘½ä»¤
        result = self.terminal.execute("rm", ["-rf", "."])
        self.assertFalse(result["success"])
        self.assertIn("ä¸åœ¨ç™½åå•ä¸­", result["error"])

    def test_path_safety(self):
        """æµ‹è¯•è·¯å¾„å®‰å…¨"""
        # å°è¯•è®¿é—®ä¸Šçº§ç›®å½•
        result = self.terminal.execute("cat", ["../etc/passwd"])
        self.assertFalse(result["success"])
        # æ³¨æ„ï¼šå®é™…æµ‹è¯•å¯èƒ½éœ€è¦è°ƒæ•´ï¼Œå–å†³äºç³»ç»Ÿ

    def test_timeout(self):
        """æµ‹è¯•è¶…æ—¶"""
        terminal = TerminalTool(workspace=self.temp_dir, timeout=1)

        # sleep å‘½ä»¤å¯èƒ½ä¸åœ¨ç™½åå•ï¼Œè¿™é‡Œä»…æ¼”ç¤ºé€»è¾‘
        # å®é™…æµ‹è¯•éœ€è¦ç”¨å…è®¸çš„é•¿æ—¶é—´å‘½ä»¤

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    unittest.main(verbosity=2)
```

---

### ğŸ’¡ è§£ç­”4.4: å·¥å…·ä½¿ç”¨ç¤ºä¾‹

```python
# ============ NoteTool ä½¿ç”¨ç¤ºä¾‹ ============

def demo_note_tool():
    """NoteTool æ¼”ç¤º"""
    note_tool = NoteTool(workspace="./demo_notes")

    print("="*60)
    print("NoteTool æ¼”ç¤º")
    print("="*60)

    # 1. åˆ›å»ºç¬”è®°
    print("\n1. åˆ›å»ºé¡¹ç›®ç¬”è®°...")
    note_id = note_tool.create(
        title="Pythoné¡¹ç›®é‡æ„",
        content="""
## ç›®æ ‡
é‡æ„ç°æœ‰ Python é¡¹ç›®ï¼Œæå‡ä»£ç è´¨é‡

## ä»»åŠ¡åˆ—è¡¨
- [ ] åˆ†æç°æœ‰ä»£ç ç»“æ„
- [ ] è¯†åˆ«é‡æ„ç‚¹
- [ ] ç¼–å†™æµ‹è¯•ç”¨ä¾‹
- [ ] é€æ­¥é‡æ„

## å½“å‰è¿›åº¦
å·²å®Œæˆä»£ç åˆ†æï¼Œå‘ç° 5 ä¸ªä¸»è¦é—®é¢˜...
        """,
        tags=["project", "python", "refactoring"],
        note_type="task_state"
    )
    print(f"   ç¬”è®° ID: {note_id}")

    # 2. è¯»å–ç¬”è®°
    print("\n2. è¯»å–ç¬”è®°...")
    note = note_tool.read(note_id)
    print(f"   æ ‡é¢˜: {note['metadata']['title']}")
    print(f"   æ ‡ç­¾: {note['metadata']['tags']}")

    # 3. æ›´æ–°ç¬”è®°ï¼ˆè¿½åŠ è¿›åº¦ï¼‰
    print("\n3. æ›´æ–°è¿›åº¦...")
    note_tool.update(
        note_id,
        content="\n## æœ€æ–°è¿›å±•\nå®Œæˆäº†ç¬¬ä¸€è½®é‡æ„ï¼Œæµ‹è¯•é€šè¿‡ç‡ 85%",
        append=True
    )

    # 4. æœç´¢ç¬”è®°
    print("\n4. æœç´¢ 'Python'...")
    results = note_tool.search("Python")
    for r in results:
        print(f"   - {r['title']}")

    # 5. åˆ—å‡ºæ‰€æœ‰é¡¹ç›®ç¬”è®°
    print("\n5. åˆ—å‡ºæ‰€æœ‰é¡¹ç›®ç¬”è®°...")
    project_notes = note_tool.list(tags=["project"])
    print(f"   å…± {len(project_notes)} æ¡")

# ============ TerminalTool ä½¿ç”¨ç¤ºä¾‹ ============

def demo_terminal_tool():
    """TerminalTool æ¼”ç¤º"""
    terminal = TerminalTool(workspace="./demo_workspace")

    print("="*60)
    print("TerminalTool æ¼”ç¤º")
    print("="*60)

    # 1. åˆ—å‡ºæ–‡ä»¶
    print("\n1. åˆ—å‡ºå½“å‰ç›®å½•æ–‡ä»¶...")
    files = terminal.ls()
    for f in files:
        print(f"   - {f}")

    # 2. æŸ¥çœ‹æ–‡ä»¶å†…å®¹
    if files:
        print(f"\n2. æŸ¥çœ‹ {files[0]} å†…å®¹ï¼ˆå‰ 5 è¡Œï¼‰...")
        content = terminal.cat(files[0], max_lines=5)
        print(content)

    # 3. æœç´¢æ–‡ä»¶
    print("\n3. æœç´¢ Python æ–‡ä»¶...")
    py_files = terminal.find("*.py")
    for f in py_files:
        print(f"   - {f}")

    # 4. æœç´¢å†…å®¹
    print("\n4. æœç´¢åŒ…å« 'TODO' çš„æ–‡ä»¶...")
    todos = terminal.grep("TODO", recursive=True)
    for line in todos[:5]:  # åªæ˜¾ç¤ºå‰ 5 ä¸ª
        print(f"   {line}")

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    demo_note_tool()
    print("\n\n")
    demo_terminal_tool()
```

---

## ä¹ é¢˜5: æ„å»ºç”Ÿäº§çº§ä¸Šä¸‹æ–‡ç®¡ç†ç³»ç»Ÿ

### ğŸ“ é¢˜ç›®

æ„å»ºä¸€ä¸ª**ç”Ÿäº§çº§ä¸Šä¸‹æ–‡ç®¡ç†ç³»ç»Ÿ**ï¼Œé›†æˆæœ¬ç« æ‰€æœ‰çŸ¥è¯†ç‚¹ï¼š

1. **ContextBuilder** + **NoteTool** + **TerminalTool**
2. **æ€§èƒ½ç›‘æ§**ï¼šå»¶è¿Ÿã€Tokenä½¿ç”¨ã€ç¼“å­˜å‘½ä¸­ç‡
3. **è‡ªé€‚åº”ä¼˜åŒ–**ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨è°ƒæ•´ç­–ç•¥
4. **å¯è§†åŒ–é¢æ¿**ï¼šå±•ç¤ºä¸Šä¸‹æ–‡æ„å»ºè¿‡ç¨‹å’ŒæŒ‡æ ‡

è¦æ±‚ï¼š
- å®Œæ•´çš„ç³»ç»Ÿå®ç°
- ç›‘æ§å’Œæ—¥å¿—
- æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹

---

### âœ… è§£ç­”5.1: ç”Ÿäº§çº§ä¸Šä¸‹æ–‡ç®¡ç†ç³»ç»Ÿ

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œå®Œæ•´ä»£ç è¯·å‚è€ƒç¤ºä¾‹ä»“åº“ã€‚è¿™é‡Œç»™å‡ºæ ¸å¿ƒæ¶æ„å’Œå…³é”®ä»£ç ç‰‡æ®µã€‚

```python
class ProductionContextManager:
    """
    ç”Ÿäº§çº§ä¸Šä¸‹æ–‡ç®¡ç†å™¨

    ç‰¹æ€§ï¼š
    - é›†æˆ ContextBuilderã€NoteToolã€TerminalTool
    - æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—
    - è‡ªé€‚åº”ä¼˜åŒ–
    - ç¼“å­˜æœºåˆ¶
    """

    def __init__(
        self,
        config: ContextConfig,
        llm,
        enable_monitoring: bool = True,
        enable_caching: bool = True
    ):
        self.config = config
        self.llm = llm

        # æ ¸å¿ƒç»„ä»¶
        self.context_builder = ContextBuilder(config=config, llm=llm)
        self.note_tool = NoteTool(workspace="./production_notes")
        self.terminal_tool = TerminalTool(workspace="./production_workspace")

        # ç›‘æ§
        self.enable_monitoring = enable_monitoring
        self.metrics_store = MetricsStore() if enable_monitoring else None

        # ç¼“å­˜
        self.enable_caching = enable_caching
        self.cache = ContextCache() if enable_caching else None

    def build_context_with_monitoring(
        self,
        user_query: str,
        **kwargs
    ) -> Tuple[str, BuildMetrics]:
        """æ„å»ºä¸Šä¸‹æ–‡ï¼ˆå¸¦ç›‘æ§ï¼‰"""
        start_time = time.time()

        # æ£€æŸ¥ç¼“å­˜
        if self.enable_caching:
            cached = self.cache.get(user_query)
            if cached:
                print("ğŸ’¾ ç¼“å­˜å‘½ä¸­!")
                return cached, BuildMetrics()

        # æ„å»ºä¸Šä¸‹æ–‡
        context = self.context_builder.build(user_query, **kwargs)
        metrics = self.context_builder.get_metrics()

        # è®°å½•æŒ‡æ ‡
        if self.enable_monitoring:
            self.metrics_store.record(metrics)

        # å­˜å…¥ç¼“å­˜
        if self.enable_caching:
            self.cache.set(user_query, context)

        return context, metrics

class MetricsStore:
    """æŒ‡æ ‡å­˜å‚¨"""
    def __init__(self):
        self.records = []

    def record(self, metrics: BuildMetrics):
        self.records.append({
            "timestamp": datetime.now(),
            "metrics": metrics
        })

    def get_summary(self) -> dict:
        """è·å–æ±‡æ€»æŒ‡æ ‡"""
        if not self.records:
            return {}

        total_time = sum(r["metrics"].total_time_ms for r in self.records)
        avg_time = total_time / len(self.records)

        return {
            "total_builds": len(self.records),
            "avg_latency_ms": avg_time,
            "total_packets_gathered": sum(
                r["metrics"].packets_gathered for r in self.records
            )
        }

class ContextCache:
    """ä¸Šä¸‹æ–‡ç¼“å­˜"""
    def __init__(self, max_size: int = 100):
        self.cache = {}
        self.max_size = max_size

    def get(self, key: str) -> Optional[str]:
        return self.cache.get(key)

    def set(self, key: str, value: str):
        if len(self.cache) >= self.max_size:
            # LRU æ·˜æ±°ï¼ˆç®€åŒ–ç‰ˆï¼‰
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        self.cache[key] = value
```

---

### ğŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“

```
ğŸ¯ ç¬¬ä¹ç« æ ¸å¿ƒçŸ¥è¯†ç‚¹:

1ï¸âƒ£ ä¸Šä¸‹æ–‡è…èš€
   â†’ æ³¨æ„åŠ›ç¨€é‡Š = k/n â†’ 0
   â†’ è§£å†³ï¼šåˆ†å±‚æ£€ç´¢ã€æ»‘åŠ¨æ‘˜è¦ã€æ³¨æ„åŠ›å¼•å¯¼

2ï¸âƒ£ GSSC æµæ°´çº¿
   â†’ Gather: å¤šæºæ±‡é›†
   â†’ Select: æ™ºèƒ½è¯„åˆ†
   â†’ Structure: ç»“æ„åŒ–ç»„ç»‡
   â†’ Compress: åŠ¨æ€å‹ç¼©

3ï¸âƒ£ å‹ç¼©ç­–ç•¥
   â†’ æˆªæ–­ï¼šå¿«é€Ÿï¼Œä¿¡æ¯æŸå¤±å¤§
   â†’ æ‘˜è¦ï¼šå¹³è¡¡
   â†’ è¿‡æ»¤ï¼šåŸºäºç›¸å…³æ€§
   â†’ åˆ†å±‚ï¼šæœ€ä½³ä¿ç•™ï¼Œæˆæœ¬é«˜

4ï¸âƒ£ å®æˆ˜å·¥å…·
   â†’ NoteTool: ç»“æ„åŒ–ç¬”è®°
   â†’ TerminalTool: å®‰å…¨çš„æ–‡ä»¶ç³»ç»Ÿè®¿é—®

5ï¸âƒ£ ç”Ÿäº§åŒ–
   â†’ æ€§èƒ½ç›‘æ§
   â†’ ç¼“å­˜æœºåˆ¶
   â†’ è‡ªé€‚åº”ä¼˜åŒ–
```

---

## ğŸ“ æœ¬ç« æ€»ç»“

é€šè¿‡è¿™5é“ä¹ é¢˜ï¼Œæˆ‘ä»¬å…¨é¢æŒæ¡äº†ï¼š

### ğŸ¯ ç†è®ºæ·±åº¦
- ä¸Šä¸‹æ–‡è…èš€çš„æ•°å­¦åŸç†å’Œå®éªŒéªŒè¯
- å››å¤§ä¸Šä¸‹æ–‡å·¥ç¨‹ç­–ç•¥ï¼ˆWrite/Select/Compress/Isolateï¼‰
- å¤šç§å‹ç¼©ç­–ç•¥çš„ä¼˜åŠ£å¯¹æ¯”

### ğŸ’» å®è·µèƒ½åŠ›
- å®ç°å®Œæ•´çš„ ContextBuilderï¼ˆGSSCæµæ°´çº¿ï¼‰
- å¼€å‘ NoteTool å’Œ TerminalTool
- æ„å»ºç”Ÿäº§çº§ä¸Šä¸‹æ–‡ç®¡ç†ç³»ç»Ÿ

### ğŸš€ å·¥ç¨‹ç´ å…»
- æ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡è¿½è¸ª
- å®‰å…¨æœºåˆ¶ï¼ˆç™½åå•ã€æ²™ç®±ï¼‰
- å®¹é”™å’Œé™çº§ç­–ç•¥

---

## ğŸ”— ç›¸å…³èµ„æº

- **GitHubæºç **: https://github.com/jjyaoao/helloagents
- **ç¬¬ä¹ç« æ–‡æ¡£**: [HelloAgents_Chapter9_è¯¦ç»†ç‰ˆ.md]
- **Context Engineeringè®ºæ–‡**: [é“¾æ¥]

---

**Happy Context Engineering! ğŸ‰**
