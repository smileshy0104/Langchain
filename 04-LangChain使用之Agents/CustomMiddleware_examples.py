#!/usr/bin/env python3
"""
LangChain v1.0 è‡ªå®šä¹‰ä¸­é—´ä»¶ç¤ºä¾‹

TODO Custom Middleware è‡ªå®šä¹‰ä¸­é—´ä»¶
å±•ç¤ºå¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ä¸­é—´ä»¶æ¥åŠ¨æ€æ§åˆ¶Agentè¡Œä¸ºï¼š
1. æ ¹æ®ç”¨æˆ·çº§åˆ«åŠ¨æ€é€‰æ‹©æ¨¡å‹å’Œå·¥å…·
2. æˆæœ¬ä¼˜åŒ–ä¸­é—´ä»¶
3. æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶
4. åŸºäºæ—¶é—´çš„æ™ºèƒ½è·¯ç”±
5. å¤šè¯­è¨€é€‚é…ä¸­é—´ä»¶

åŸºäº GLM-4.6 æ¨¡å‹å®ç°
"""

from __future__ import annotations

import os
import time
from dataclasses import dataclass, field
from typing import Any, Callable, Literal

import dotenv
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_core.tools import BaseTool, tool
from langchain.agents import create_agent
from langchain.agents.middleware import (
    AgentMiddleware,
    ModelRequest,
)
from langchain.agents.middleware.types import ModelResponse

# ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½ .env
dotenv.load_dotenv(dotenv_path="../.env")


# ========== æ•°æ®ç±»å®šä¹‰ ==========

@dataclass
class UserContext:
    """ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    user_id: str
    expertise: Literal["beginner", "intermediate", "expert"] = "beginner" # ä¸“ä¸šç¨‹åº¦
    language: str = "zh"
    budget_tier: Literal["free", "standard", "premium"] = "standard" # é¢„ç®—ç­‰çº§
    request_count: int = 0 # è¯·æ±‚æ¬¡æ•°
    last_request_time: float = field(default_factory=time.time) # ä¸Šæ¬¡è¯·æ±‚æ—¶é—´
    avg_response_time: float = 0.0 # å¹³å‡å“åº”æ—¶é—´


@dataclass
class RequestMetrics:
    """è¯·æ±‚æŒ‡æ ‡"""
    start_time: float = 0.0 
    end_time: float = 0.0
    model_used: str = ""
    tools_used: list[str] = field(default_factory=list)
    tokens_consumed: int = 0
    success: bool = True
    error_message: str = ""


# ========== ç¤ºä¾‹å·¥å…· ==========

@tool
def simple_search(query: str) -> str:
    """ç®€å•çš„æœç´¢å·¥å…·ï¼ˆé€‚åˆåˆå­¦è€…ï¼‰

    Args:
        query: æœç´¢å…³é”®è¯
    """
    return f"ã€ç®€å•æœç´¢ã€‘é’ˆå¯¹ '{query}' çš„åŸºç¡€æœç´¢ç»“æœï¼šè¿™æ˜¯åŸºæœ¬ç­”æ¡ˆï¼Œé€‚åˆåˆå­¦è€…ç†è§£ã€‚"


@tool
def advanced_search(query: str) -> str:
    """é«˜çº§æœç´¢å·¥å…·ï¼ˆé€‚åˆä¸“å®¶ï¼‰

    Args:
        query: æœç´¢å…³é”®è¯
    """
    return f"""ã€é«˜çº§æœç´¢ã€‘é’ˆå¯¹ '{query}' çš„æ·±åº¦åˆ†æï¼š

=== æŠ€æœ¯ç»†èŠ‚ ===
- ç®—æ³•å¤æ‚åº¦ï¼šO(n log n)
- æ•°æ®æ¥æºï¼šå­¦æœ¯è®ºæ–‡ã€ä¸“åˆ©æ•°æ®åº“
- åˆ†æç»´åº¦ï¼šæŠ€æœ¯å¯è¡Œæ€§ã€å¸‚åœºæ½œåŠ›ã€åˆ›æ–°åº¦

=== ä¸“å®¶ç»“è®º ===
åŸºäºæœ€æ–°ç ”ç©¶ï¼Œè¯¥é—®é¢˜éœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªæŠ€æœ¯é¢†åŸŸã€‚
å»ºè®®æ·±å…¥ç ”è¯»ç›¸å…³æ–‡çŒ®ï¼Œå¹¶ç»“åˆå®é™…æ¡ˆä¾‹éªŒè¯ã€‚"""


@tool
def basic_calculator(expression: str) -> str:
    """åŸºç¡€è®¡ç®—å™¨ï¼ˆé€‚åˆåˆå­¦è€…ï¼‰

    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ '2 + 3 * 4'
    """
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœï¼š{expression} = {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯ï¼š{str(e)}"


@tool
def advanced_analysis(data: str, method: str = "statistical") -> str:
    """é«˜çº§æ•°æ®åˆ†æå·¥å…·ï¼ˆé€‚åˆä¸“å®¶ï¼‰

    Args:
        data: å¾…åˆ†æçš„æ•°æ®
        method: åˆ†ææ–¹æ³•ï¼šstatistical, ML, deep_learning
    """
    analysis_methods = {
        "statistical": "ç»Ÿè®¡åˆ†æï¼šå‡å€¼ã€æ–¹å·®ã€ç›¸å…³æ€§åˆ†æ",
        "ML": "æœºå™¨å­¦ä¹ ï¼šèšç±»ã€å›å½’ã€åˆ†ç±»æ¨¡å‹",
        "deep_learning": "æ·±åº¦å­¦ä¹ ï¼šç¥ç»ç½‘ç»œã€Transformeræ¶æ„"
    }

    return f"""ã€é«˜çº§æ•°æ®åˆ†æã€‘
æ–¹æ³•ï¼š{method}
æ•°æ®ï¼š{data}

{analysis_methods.get(method, 'æœªçŸ¥æ–¹æ³•')}

è¯¦ç»†æŠ¥å‘Šï¼š
1. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
2. ç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©
3. æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯
4. ç»“æœè§£é‡Šå’Œå¯è§†åŒ–

å»ºè®®ï¼šä½¿ç”¨ä¸“ä¸šå·¥å…·è¿›è¡Œæ·±å…¥åˆ†æã€‚"""


@tool
def get_weather(city: str) -> str:
    """è·å–å¤©æ°”ä¿¡æ¯

    Args:
        city: åŸå¸‚åç§°
    """
    import random
    conditions = ["æ™´å¤©", "å¤šäº‘", "å°é›¨"]
    temp = random.randint(15, 25)
    condition = random.choice(conditions)
    return f"{city}ä»Šå¤©å¤©æ°”ï¼š{condition}ï¼Œæ¸©åº¦ {temp}Â°C"


# ========== è‡ªå®šä¹‰ä¸­é—´ä»¶å®ç° ==========

class ExpertiseBasedMiddleware(AgentMiddleware):
    """æ ¹æ®ç”¨æˆ·ä¸“ä¸šç¨‹åº¦åŠ¨æ€é€‰æ‹©æ¨¡å‹å’Œå·¥å…·çš„ä¸­é—´ä»¶

    åŠŸèƒ½ï¼š
    - beginner: ä½¿ç”¨ç®€å•æ¨¡å‹ + åŸºç¡€å·¥å…·
    - intermediate: ä½¿ç”¨æ ‡å‡†æ¨¡å‹ + å¸¸è§„å·¥å…·
    - expert: ä½¿ç”¨é«˜çº§æ¨¡å‹ + ä¸“ä¸šå·¥å…·
    """

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        # è·å–ç”¨æˆ·ä¸“ä¸šç¨‹åº¦
        user_level = request.runtime.context.expertise

        # æ ¹æ®ä¸“ä¸šç¨‹åº¦é€‰æ‹©ä¸åŒçš„æ¨¡å‹å’Œå·¥å…·
        if user_level == "expert":
            # ä¸“å®¶ï¼šä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹å’Œå·¥å…·
            model = ChatZhipuAI(
                model="glm-4.6",
                temperature=0.2,
                api_key=os.getenv("ZHIPUAI_API_KEY"),
                max_tokens=4000
            )
            tools = [advanced_search, advanced_analysis, get_weather]
            print(f"ğŸ“ ä½¿ç”¨ä¸“å®¶æ¨¡å¼ï¼šé«˜çº§æ¨¡å‹ + ä¸“ä¸šå·¥å…·")

        elif user_level == "intermediate":
            # ä¸­çº§ï¼šä½¿ç”¨æ ‡å‡†é…ç½®
            model = ChatZhipuAI(
                model="glm-4.6",
                temperature=0.5,
                api_key=os.getenv("ZHIPUAI_API_KEY"),
                max_tokens=2000
            )
            tools = [simple_search, basic_calculator, get_weather]
            print(f"ğŸ“š ä½¿ç”¨ä¸­çº§æ¨¡å¼ï¼šæ ‡å‡†æ¨¡å‹ + å¸¸è§„å·¥å…·")

        else:
            # åˆå­¦è€…ï¼šä½¿ç”¨ç®€åŒ–é…ç½®
            model = ChatZhipuAI(
                model="glm-4.6",
                temperature=0.7,
                api_key=os.getenv("ZHIPUAI_API_KEY"),
                max_tokens=1000
            )
            tools = [simple_search, get_weather]
            print(f"ğŸŒ± ä½¿ç”¨åˆå­¦è€…æ¨¡å¼ï¼šç®€åŒ–æ¨¡å‹ + åŸºç¡€å·¥å…·")

        # æ›´æ–°è¯·æ±‚
        request.model = model
        request.tools = tools

        return handler(request)


class CostOptimizationMiddleware(AgentMiddleware):
    """æˆæœ¬ä¼˜åŒ–ä¸­é—´ä»¶

    åŠŸèƒ½ï¼š
    - æ ¹æ®é¢„ç®—ç­‰çº§é™åˆ¶æ¨¡å‹ä½¿ç”¨
    - ç›‘æ§tokenæ¶ˆè€—
    - æ™ºèƒ½é™çº§ç­–ç•¥
    """

    def __init__(self):
        self.total_tokens = 0
        self.request_count = 0

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        # è·å–ç”¨æˆ·é¢„ç®—ç­‰çº§
        budget_tier = request.runtime.context.budget_tier

        if budget_tier == "free":
            # å…è´¹ç”¨æˆ·ï¼šä¸¥æ ¼é™åˆ¶
            if request.model.max_tokens and request.model.max_tokens > 1000:
                print(f"ğŸ’° å…è´¹ç”¨æˆ·ï¼šé™åˆ¶max_tokens=1000")
                request.model.max_tokens = 1000

        elif budget_tier == "standard":
            # æ ‡å‡†ç”¨æˆ·ï¼šé€‚åº¦é™åˆ¶
            if request.model.max_tokens and request.model.max_tokens > 2000:
                print(f"ğŸ’° æ ‡å‡†ç”¨æˆ·ï¼šé™åˆ¶max_tokens=2000")
                request.model.max_tokens = 2000

        # premiumç”¨æˆ·ä¸å—é™åˆ¶
        print(f"ğŸ’ é«˜çº§ç”¨æˆ·ï¼šæ— é™åˆ¶")

        # æ‰§è¡Œè¯·æ±‚å¹¶ç»Ÿè®¡
        start_time = time.time()
        response = handler(request)
        end_time = time.time()

        # è®°å½•æŒ‡æ ‡
        self.request_count += 1
        self.total_tokens += getattr(response, 'token_usage', 0)

        print(f"ğŸ“Š è¯·æ±‚ #{self.request_count} - è€—æ—¶: {end_time - start_time:.2f}s")

        return response


class PerformanceMonitoringMiddleware(AgentMiddleware):
    """æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶

    åŠŸèƒ½ï¼š
    - è®°å½•å“åº”æ—¶é—´
    - ç›‘æ§æ¨¡å‹æ€§èƒ½
    - ç»Ÿè®¡å·¥å…·ä½¿ç”¨é¢‘ç‡
    - æä¾›æ€§èƒ½æŠ¥å‘Š
    """

    def __init__(self):
        self.metrics: list[RequestMetrics] = []

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        metric = RequestMetrics(start_time=time.time())

        try:
            response = handler(request)
            metric.success = True
            metric.model_used = getattr(request.model, 'model', 'unknown')
            metric.tools_used = [t.name for t in request.tools]

        except Exception as e:
            metric.success = False
            metric.error_message = str(e)
            raise

        finally:
            metric.end_time = time.time()
            metric.tokens_consumed = 0  # å®é™…åº”ä»å“åº”ä¸­è·å–
            self.metrics.append(metric)

        return response

    def get_report(self) -> dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.metrics:
            return {"message": "æš‚æ— æ•°æ®"}

        total_requests = len(self.metrics)
        successful_requests = sum(1 for m in self.metrics if m.success)
        avg_response_time = sum(
            (m.end_time - m.start_time) for m in self.metrics
        ) / total_requests

        return {
            "æ€»è¯·æ±‚æ•°": total_requests,
            "æˆåŠŸè¯·æ±‚æ•°": successful_requests,
            "æˆåŠŸç‡": f"{successful_requests / total_requests * 100:.1f}%",
            "å¹³å‡å“åº”æ—¶é—´": f"{avg_response_time:.2f}s",
            "æœ€æ…¢å“åº”": f"{max((m.end_time - m.start_time) for m in self.metrics):.2f}s",
            "æœ€å¿«å“åº”": f"{min((m.end_time - m.start_time) for m in self.metrics):.2f}s",
        }


class TimeBasedRoutingMiddleware(AgentMiddleware):
    """åŸºäºæ—¶é—´çš„æ™ºèƒ½è·¯ç”±ä¸­é—´ä»¶

    åŠŸèƒ½ï¼š
    - æ ¹æ®æ—¶é—´æ®µé€‰æ‹©ä¸åŒæ¨¡å‹
    - å³°å€¼æ—¶æ®µé™çº§ç­–ç•¥
    - æ™ºèƒ½è´Ÿè½½å‡è¡¡
    """

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelResponse:
        current_hour = time.localtime().tm_hour

        # å®šä¹‰æ—¶é—´ç­–ç•¥
        if 9 <= current_hour <= 18:  # å·¥ä½œæ—¶é—´
            print(f"â° å·¥ä½œæ—¶é—´ ({current_hour}:00): ä½¿ç”¨æ ‡å‡†æ¨¡å‹")
            request.model.temperature = 0.5
            request.model.max_tokens = min(
                request.model.max_tokens or 2000, 2000
            )
        elif 22 <= current_hour or current_hour <= 7:  # å¤œé—´
            print(f"ğŸŒ™ å¤œé—´ ({current_hour}:00): ä½¿ç”¨é«˜æ•ˆæ¨¡å‹")
            request.model.temperature = 0.3
            request.model.max_tokens = min(
                request.model.max_tokens or 3000, 3000
            )
        else:  # å…¶ä»–æ—¶é—´
            print(f"â˜€ï¸ é—²æ—¶ ({current_hour}:00): ä½¿ç”¨é«˜æ€§èƒ½æ¨¡å‹")
            request.model.temperature = 0.7
            # ä¸é™åˆ¶max_tokens

        return handler(request)


class MultilingualMiddleware(AgentMiddleware):
    """å¤šè¯­è¨€é€‚é…ä¸­é—´ä»¶

    åŠŸèƒ½ï¼š
    - æ ¹æ®ç”¨æˆ·è¯­è¨€åå¥½è°ƒæ•´æç¤ºè¯
    - åŠ¨æ€é€‰æ‹©æ”¯æŒçš„è¯­è¨€æ¨¡å‹
    - è‡ªåŠ¨ç¿»è¯‘è¾…åŠ©
    """

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse]
    ) -> ModelRequest:
        user_language = request.runtime.context.language

        # æ ¹æ®è¯­è¨€è°ƒæ•´ç³»ç»Ÿæç¤ºè¯
        system_prompts = {
            "zh": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ä¸­æ–‡AIåŠ©æ‰‹ã€‚",
            "en": "You are a helpful AI assistant.",
            "ja": "ã‚ãªãŸã¯æœ‰ç”¨ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
            "es": "Eres un asistente de IA Ãºtil.",
        }

        # è·å–å½“å‰ç³»ç»Ÿæç¤ºè¯
        current_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"
        for msg in request.messages:
            if isinstance(msg, SystemMessage):
                current_prompt = msg.content
                break

        # æ·»åŠ è¯­è¨€ç‰¹å®šçš„æŒ‡å¯¼
        language_guide = system_prompts.get(user_language, system_prompts["en"])
        if current_prompt != language_guide:
            # æ›´æ–°ç³»ç»Ÿæ¶ˆæ¯
            for i, msg in enumerate(request.messages):
                if isinstance(msg, SystemMessage):
                    request.messages[i] = SystemMessage(content=language_guide)
                    break
            else:
                # å¦‚æœæ²¡æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ·»åŠ ä¸€ä¸ª
                request.messages.insert(0, SystemMessage(content=language_guide))

        print(f"ğŸŒ ä½¿ç”¨è¯­è¨€ï¼š{user_language} - {language_guide}")

        return request


# ========== æ¼”ç¤ºå‡½æ•° ==========

def demo_expertise_based():
    """æ¼”ç¤ºåŸºäºä¸“ä¸šç¨‹åº¦çš„åŠ¨æ€è·¯ç”±"""
    print("=" * 70)
    print("ğŸ“ ä¸“ä¸šç¨‹åº¦è‡ªé€‚åº”æ¼”ç¤º")
    print("=" * 70)

    # åˆ›å»ºä¸åŒä¸“ä¸šç¨‹åº¦çš„ç”¨æˆ·ä¸Šä¸‹æ–‡
    users = [
        UserContext(user_id="user1", expertise="beginner"),
        UserContext(user_id="user2", expertise="intermediate"),
        UserContext(user_id="user3", expertise="expert"),
    ]

    for user in users:
        print(f"\nğŸ‘¤ ç”¨æˆ·ï¼š{user.user_id} (ä¸“ä¸šç¨‹åº¦ï¼š{user.expertise})")

        agent = create_agent(
            model=ChatZhipuAI(
                model="glm-4.6",
                temperature=0.5,
                api_key=os.getenv("ZHIPUAI_API_KEY")
            ),
            tools=[simple_search, advanced_search, basic_calculator, advanced_analysis],
            middleware=[ExpertiseBasedMiddleware()],
            context_schema=type("UserContext", (), user.__dict__),  # åŠ¨æ€åˆ›å»ºschema
        )

        # æ¨¡æ‹Ÿè¯·æ±‚
        try:
            result = agent.invoke({
                "messages": [HumanMessage(content="è¯·æœç´¢ 'AIå‘å±•'")],
                "context": user.__dict__
            })
            print(f"âœ… æˆåŠŸå¤„ç†è¯·æ±‚")
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ï¼š{e}")


def demo_cost_optimization():
    """æ¼”ç¤ºæˆæœ¬ä¼˜åŒ–"""
    print("\n" + "=" * 70)
    print("ğŸ’° æˆæœ¬ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 70)

    # åˆ›å»ºä¸åŒé¢„ç®—ç­‰çº§çš„ç”¨æˆ·
    budgets = ["free", "standard", "premium"]

    for budget in budgets:
        user = UserContext(user_id=f"user_{budget}", budget_tier=budget)

        middleware = CostOptimizationMiddleware()

        agent = create_agent(
            model=ChatZhipuAI(
                model="glm-4.6",
                temperature=0.5,
                api_key=os.getenv("ZHIPUAI_API_KEY"),
                max_tokens=4000  # åˆå§‹è®¾ç½®ï¼Œåç»­ä¼šè¢«ä¸­é—´ä»¶è°ƒæ•´
            ),
            tools=[simple_search, basic_calculator],
            middleware=[middleware],
            context_schema=type("UserContext", (), user.__dict__),
        )

        try:
            result = agent.invoke({
                "messages": [HumanMessage(content="è®¡ç®— 100 + 200")],
                "context": user.__dict__
            })
            print(f"âœ… {budget} ç”¨æˆ·è¯·æ±‚å¤„ç†å®Œæˆ")
        except Exception as e:
            print(f"âŒ {budget} ç”¨æˆ·è¯·æ±‚å¤±è´¥ï¼š{e}")


def demo_performance_monitoring():
    """æ¼”ç¤ºæ€§èƒ½ç›‘æ§"""
    print("\n" + "=" * 70)
    print("ğŸ“Š æ€§èƒ½ç›‘æ§æ¼”ç¤º")
    print("=" * 70)

    user = UserContext(user_id="test_user")
    middleware = PerformanceMonitoringMiddleware()

    agent = create_agent(
        model=ChatZhipuAI(
            model="glm-4.6",
            temperature=0.5,
            api_key=os.getenv("ZHIPUAI_API_KEY")
        ),
        tools=[simple_search, basic_calculator],
        middleware=[middleware],
        context_schema=type("UserContext", (), user.__dict__),
    )

    # å‘é€å¤šä¸ªè¯·æ±‚
    for i in range(3):
        print(f"\nğŸ“¤ å‘é€è¯·æ±‚ #{i+1}")
        try:
            result = agent.invoke({
                "messages": [HumanMessage(content=f"æŸ¥è¯¢å¤©æ°” (è¯·æ±‚ #{i+1})")],
                "context": user.__dict__
            })
            print(f"âœ… è¯·æ±‚å®Œæˆ")
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥ï¼š{e}")

    # æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
    print("\nğŸ“ˆ æ€§èƒ½æŠ¥å‘Š:")
    report = middleware.get_report()
    for key, value in report.items():
        print(f"  {key}ï¼š{value}")


def demo_time_based_routing():
    """æ¼”ç¤ºåŸºäºæ—¶é—´çš„æ™ºèƒ½è·¯ç”±"""
    print("\n" + "=" * 70)
    print("â° åŸºäºæ—¶é—´çš„æ™ºèƒ½è·¯ç”±æ¼”ç¤º")
    print("=" * 70)

    current_hour = time.localtime().tm_hour
    print(f"ğŸ• å½“å‰æ—¶é—´ï¼š{current_hour}:00")

    user = UserContext(user_id="time_user")
    middleware = TimeBasedRoutingMiddleware()

    agent = create_agent(
        model=ChatZhipuAI(
            model="glm-4.6",
            temperature=0.5,
            api_key=os.getenv("ZHIPUAI_API_KEY"),
            max_tokens=2000
        ),
        tools=[simple_search],
        middleware=[middleware],
        context_schema=type("UserContext", (), user.__dict__),
    )

    try:
        result = agent.invoke({
            "messages": [HumanMessage(content="ä½ å¥½")],
            "context": user.__dict__
        })
        print(f"âœ… è¯·æ±‚å¤„ç†å®Œæˆ")
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥ï¼š{e}")


def demo_multilingual():
    """æ¼”ç¤ºå¤šè¯­è¨€é€‚é…"""
    print("\n" + "=" * 70)
    print("ğŸŒ å¤šè¯­è¨€é€‚é…æ¼”ç¤º")
    print("=" * 70)

    languages = ["zh", "en", "ja"]

    for lang in languages:
        user = UserContext(user_id=f"user_{lang}", language=lang)
        middleware = MultilingualMiddleware()

        agent = create_agent(
            model=ChatZhipuAI(
                model="glm-4.6",
                temperature=0.5,
                api_key=os.getenv("ZHIPUAI_API_KEY")
            ),
            tools=[simple_search],
            middleware=[middleware],
            context_schema=type("UserContext", (), user.__dict__),
        )

        try:
            result = agent.invoke({
                "messages": [HumanMessage(content="ä½ å¥½")],
                "context": user.__dict__
            })
            print(f"âœ… {lang} è¯­è¨€è¯·æ±‚å¤„ç†å®Œæˆ")
        except Exception as e:
            print(f"âŒ {lang} è¯­è¨€è¯·æ±‚å¤±è´¥ï¼š{e}")


def explain_custom_middleware():
    """è§£é‡Šè‡ªå®šä¹‰ä¸­é—´ä»¶æœºåˆ¶"""
    print("\n" + "=" * 70)
    print("ğŸ“š è‡ªå®šä¹‰ä¸­é—´ä»¶æœºåˆ¶è¯¦è§£")
    print("=" * 70)

    print("""
ğŸ”§ è‡ªå®šä¹‰ä¸­é—´ä»¶çš„æ ¸å¿ƒï¼š

1. ç»§æ‰¿ AgentMiddleware åŸºç±»
2. å®ç° wrap_model_call æ–¹æ³•
3. åœ¨æ–¹æ³•ä¸­ï¼š
   - è·å–è¯·æ±‚å’Œè¿è¡Œæ—¶ä¸Šä¸‹æ–‡
   - æ ¹æ®éœ€è¦ä¿®æ”¹è¯·æ±‚ï¼ˆæ¨¡å‹ã€å·¥å…·ã€æç¤ºè¯ç­‰ï¼‰
   - è°ƒç”¨ handler(request) æ‰§è¡Œè¯·æ±‚
   - å¤„ç†å“åº”å¹¶è¿”å›

4. ä½¿ç”¨åœºæ™¯ï¼š
   âœ… åŠ¨æ€æ¨¡å‹é€‰æ‹© - æ ¹æ®ç”¨æˆ·çº§åˆ«è°ƒæ•´
   âœ… æˆæœ¬æ§åˆ¶ - é™åˆ¶èµ„æºä½¿ç”¨
   âœ… æ€§èƒ½ç›‘æ§ - è¿½è¸ªç³»ç»ŸæŒ‡æ ‡
   âœ… å®‰å…¨ç­–ç•¥ - å®æ–½è®¿é—®æ§åˆ¶
   âœ… æ™ºèƒ½è·¯ç”± - åŸºäºæ—¶é—´/ä½ç½®çš„ä¼˜åŒ–
   âœ… å¤šè¯­è¨€æ”¯æŒ - åŠ¨æ€è°ƒæ•´è¯­è¨€åå¥½

5. å…³é”®æ–¹æ³•ï¼š
   wrap_model_call(request, handler) â†’ response
   - request: ModelRequest å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è¯·æ±‚ä¿¡æ¯
   - handler: å®é™…å¤„ç†è¯·æ±‚çš„å‡½æ•°
   - è¿”å›: ModelResponse å¯¹è±¡

6. è®¿é—®ä¸Šä¸‹æ–‡ï¼š
   request.runtime.context - è·å–ç”¨æˆ·ä¸Šä¸‹æ–‡
   request.model - è®¿é—®å½“å‰æ¨¡å‹
   request.tools - è®¿é—®å·¥å…·åˆ—è¡¨
   request.messages - è®¿é—®æ¶ˆæ¯å†å²

7. åŠ¨æ€ä¿®æ”¹ï¼š
   - request.model = new_model
   - request.tools = new_tools
   - request.max_tokens = new_limit
   - request.messages = modified_messages
    """)


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("ğŸš€ LangChain v1.0 è‡ªå®šä¹‰ä¸­é—´ä»¶å®Œæ•´ç¤ºä¾‹")
    print("=" * 80)
    print("""
âœ¨ æ¼”ç¤ºå†…å®¹ï¼š
1. ğŸ“ ExpertiseBasedMiddleware - ä¸“ä¸šç¨‹åº¦è‡ªé€‚åº”
2. ğŸ’° CostOptimizationMiddleware - æˆæœ¬ä¼˜åŒ–
3. ğŸ“Š PerformanceMonitoringMiddleware - æ€§èƒ½ç›‘æ§
4. â° TimeBasedRoutingMiddleware - æ—¶é—´æ™ºèƒ½è·¯ç”±
5. ğŸŒ MultilingualMiddleware - å¤šè¯­è¨€é€‚é…

åŸºäº GLM-4.6 æ¨¡å‹
    """)

    # æ£€æŸ¥ API å¯†é’¥
    api_key = os.getenv("ZHIPUAI_API_KEY")
    if not api_key or api_key.startswith("your-"):
        print("âŒ é”™è¯¯ï¼šè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ ZHIPUAI_API_KEY")
        print("ğŸ“ è·å– API å¯†é’¥ï¼šhttps://open.bigmodel.cn/")
        return

    try:
        # explain_custom_middleware()
        demo_expertise_based()
        # demo_cost_optimization()
        # demo_performance_monitoring()
        # demo_time_based_routing()
        # demo_multilingual()

        print("\n" + "=" * 70)
        print("ğŸ‰ æ‰€æœ‰è‡ªå®šä¹‰ä¸­é—´ä»¶æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 70)
        print("\nğŸ’¡ æç¤ºï¼šå®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¯·æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´ä¸­é—´ä»¶é€»è¾‘")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¨‹åºã€‚")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
