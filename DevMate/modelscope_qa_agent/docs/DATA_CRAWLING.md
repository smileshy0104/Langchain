# æ•°æ®çˆ¬å–ä¸å¤„ç†ç³»ç»Ÿ

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•çˆ¬å–é­”æ­ç¤¾åŒºå®˜æ–¹æ•°æ®èµ„æºå¹¶å¯¼å…¥åˆ°å‘é‡æ•°æ®åº“ã€‚

## ğŸ“‹ ç›®å½•

- [æ•°æ®æ¥æº](#æ•°æ®æ¥æº)
- [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†ä½¿ç”¨](#è¯¦ç»†ä½¿ç”¨)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## æ•°æ®æ¥æº

æ ¹æ®ç«èµ›éœ€æ±‚æ–‡æ¡£(`need.md`)ï¼Œç³»ç»Ÿçˆ¬å–ä»¥ä¸‹å®˜æ–¹æ•°æ®èµ„æº:

1. **é­”æ­ç¤¾åŒºå®˜æ–¹æ–‡æ¡£** - https://www.modelscope.cn/docs/overview
2. **ç ”ä¹ ç¤¾** - https://modelscope.cn/learn
3. **GitHubä»“åº“** - https://github.com/modelscope
4. **æ¨¡å‹åº“** - https://modelscope.cn/models
5. **æ•°æ®é›†** - https://modelscope.cn/datasets
6. **åˆ›ç©ºé—´** - https://modelscope.cn/studios
7. **MCP** - https://www.modelscope.cn/mcp
8. **AIGC** - https://www.modelscope.cn/aigc

---

## ç³»ç»Ÿæ¶æ„

### æ¨¡å—ç»„æˆ

```
crawlers/
â”œâ”€â”€ __init__.py              # æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ base_crawler.py          # çˆ¬è™«åŸºç±»
â”œâ”€â”€ docs_crawler.py          # å®˜æ–¹æ–‡æ¡£çˆ¬è™«
â”œâ”€â”€ learn_crawler.py         # ç ”ä¹ ç¤¾çˆ¬è™«
â”œâ”€â”€ github_crawler.py        # GitHubçˆ¬è™«
â”œâ”€â”€ catalog_crawler.py       # èµ„æºç›®å½•çˆ¬è™«
â””â”€â”€ data_processor.py        # æ•°æ®å¤„ç†å™¨

scripts/
â”œâ”€â”€ crawl_and_process.py     # çˆ¬å–å’Œå¤„ç†è„šæœ¬
â””â”€â”€ ingest_crawled_data.py   # æ•°æ®å¯¼å…¥è„šæœ¬

data/
â”œâ”€â”€ crawled/                 # åŸå§‹çˆ¬å–æ•°æ®
â”‚   â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ learn/
â”‚   â”œâ”€â”€ github/
â”‚   â””â”€â”€ catalog/
â””â”€â”€ processed/               # å¤„ç†åæ•°æ®
    â”œâ”€â”€ docs_processed.json
    â”œâ”€â”€ learn_processed.json
    â”œâ”€â”€ github_processed.json
    â”œâ”€â”€ catalog_processed.json
    â””â”€â”€ all_documents.jsonl
```

### æ•°æ®æµç¨‹

```
1. çˆ¬å–é˜¶æ®µ
   å„ä¸ªçˆ¬è™« â†’ åŸå§‹JSONæ–‡ä»¶ â†’ data/crawled/

2. å¤„ç†é˜¶æ®µ
   data/crawled/ â†’ DataProcessor â†’ data/processed/

3. å¯¼å…¥é˜¶æ®µ
   data/processed/all_documents.jsonl â†’ MilvusVectorStore
```

---

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
conda activate langchain-env
pip install requests beautifulsoup4 pymilvus
```

### 2. çˆ¬å–æ‰€æœ‰æ•°æ®å¹¶å¯¼å…¥

```bash
# çˆ¬å–æ‰€æœ‰æ•°æ®æºå¹¶å¤„ç†
python scripts/crawl_and_process.py --all --process

# å¯¼å…¥åˆ°å‘é‡æ•°æ®åº“
python scripts/ingest_crawled_data.py
```

---

## è¯¦ç»†ä½¿ç”¨

### çˆ¬å–ç‰¹å®šæ•°æ®æº

```bash
# åªçˆ¬å–å®˜æ–¹æ–‡æ¡£
python scripts/crawl_and_process.py --docs

# çˆ¬å–ç ”ä¹ ç¤¾å’ŒGitHub
python scripts/crawl_and_process.py --learn --github

# çˆ¬å–èµ„æºç›®å½•
python scripts/crawl_and_process.py --catalog
```

### å¤„ç†å·²çˆ¬å–çš„æ•°æ®

```bash
# åªå¤„ç†æ•°æ®(ä¸çˆ¬å–)
python scripts/crawl_and_process.py --process-only
```

### è‡ªå®šä¹‰è¾“å‡ºç›®å½•

```bash
# æŒ‡å®šè¾“å‡ºç›®å½•
python scripts/crawl_and_process.py --all \
    --output-dir my_data/raw \
    --processed-dir my_data/processed
```

### æ•°æ®å¯¼å…¥é€‰é¡¹

```bash
# æŒ‡å®šè¾“å…¥æ–‡ä»¶
python scripts/ingest_crawled_data.py --input data/processed/all_documents.jsonl

# è°ƒæ•´æ‰¹å¤„ç†å¤§å°
python scripts/ingest_crawled_data.py --batch-size 100

# ä»…æµ‹è¯•åŠ è½½(ä¸å®é™…å¯¼å…¥)
python scripts/ingest_crawled_data.py --dry-run
```

---

## é…ç½®è¯´æ˜

### çˆ¬è™«é…ç½®

#### é€Ÿç‡é™åˆ¶

æ¯ä¸ªçˆ¬è™«éƒ½æœ‰é€Ÿç‡é™åˆ¶ä»¥é¿å…è¢«å°ç¦:

- **DocsCrawler**: 1.5ç§’/è¯·æ±‚
- **LearnCrawler**: 1.5ç§’/è¯·æ±‚
- **GitHubCrawler**: 2.0ç§’/è¯·æ±‚
- **CatalogCrawler**: 1.5ç§’/è¯·æ±‚

å¯åœ¨åˆå§‹åŒ–æ—¶è°ƒæ•´:

```python
crawler = DocsCrawler(rate_limit=2.0)  # 2ç§’/è¯·æ±‚
```

#### GitHub Token (å¯é€‰)

ä¸ºæé«˜GitHub APIé€Ÿç‡é™åˆ¶:

```bash
export GITHUB_TOKEN="your_github_token_here"
```

### æ•°æ®å¤„ç†é…ç½®

#### æ–‡æœ¬åˆ†å—å‚æ•°

åœ¨ `data_processor.py` ä¸­:

```python
def chunk_text(self, text: str, chunk_size: int = 800, chunk_overlap: int = 150):
    # chunk_size: æ¯å—æ–‡æœ¬å¤§å°
    # chunk_overlap: å—ä¹‹é—´é‡å å¤§å°
```

è¿™ä¸ç³»ç»Ÿé…ç½®(`config/settings.yaml`)ä¿æŒä¸€è‡´ã€‚

---

## çˆ¬è™«è¯¦ç»†è¯´æ˜

### 1. DocsCrawler - å®˜æ–¹æ–‡æ¡£çˆ¬è™«

**ç‰¹ç‚¹:**
- é€’å½’çˆ¬å–,æœ€å¤§æ·±åº¦ä¸º3
- è‡ªåŠ¨å‘ç°æ–‡æ¡£é“¾æ¥
- æå–æ ‡é¢˜ã€æ­£æ–‡ã€ä»£ç å—
- æ”¯æŒæ–­ç‚¹ç»­çˆ¬

**æ•°æ®ç»“æ„:**
```json
{
  "url": "https://www.modelscope.cn/docs/...",
  "title": "æ–‡æ¡£æ ‡é¢˜",
  "content": "æ–‡æ¡£å†…å®¹...",
  "code_blocks": ["ä»£ç å—1", "ä»£ç å—2"],
  "source": "modelscope_docs",
  "type": "documentation"
}
```

### 2. LearnCrawler - ç ”ä¹ ç¤¾çˆ¬è™«

**ç‰¹ç‚¹:**
- çˆ¬å–ç ”ä¹ ç¤¾æ–‡ç« 
- æå–ä½œè€…ã€æ—¥æœŸã€æ ‡ç­¾
- è¿‡æ»¤è¿‡çŸ­å†…å®¹
- æ”¯æŒæ–­ç‚¹ç»­çˆ¬

**æ•°æ®ç»“æ„:**
```json
{
  "url": "https://modelscope.cn/learn/...",
  "title": "æ–‡ç« æ ‡é¢˜",
  "author": "ä½œè€…",
  "date": "å‘å¸ƒæ—¥æœŸ",
  "content": "æ–‡ç« å†…å®¹...",
  "code_blocks": ["..."],
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
  "source": "modelscope_learn",
  "type": "article"
}
```

### 3. GitHubCrawler - GitHubä»“åº“çˆ¬è™«

**ç‰¹ç‚¹:**
- ä½¿ç”¨GitHub API
- çˆ¬å–ç»„ç»‡æ‰€æœ‰ä»“åº“
- è·å–READMEå†…å®¹
- æå–ä»“åº“å…ƒæ•°æ®(stars, forksç­‰)

**æ•°æ®ç»“æ„:**
```json
{
  "name": "modelscope",
  "full_name": "modelscope/modelscope",
  "description": "ä»“åº“æè¿°",
  "url": "https://github.com/modelscope/modelscope",
  "stars": 1234,
  "forks": 567,
  "language": "Python",
  "topics": ["ml", "ai"],
  "readme": "READMEå†…å®¹...",
  "source": "github",
  "type": "repository"
}
```

### 4. CatalogCrawler - èµ„æºç›®å½•çˆ¬è™«

**ç‰¹ç‚¹:**
- çˆ¬å–å¤šä¸ªèµ„æºç›®å½•
- æå–èµ„æºå¡ç‰‡ä¿¡æ¯
- æ¯ä¸ªç›®å½•é™åˆ¶50é¡¹(å¯è°ƒæ•´)

**æ”¯æŒç›®å½•:**
- models (æ¨¡å‹åº“)
- datasets (æ•°æ®é›†)
- studios (åˆ›ç©ºé—´)
- mcp (MCP)
- aigc (AIGC)

**æ•°æ®ç»“æ„:**
```json
{
  "title": "èµ„æºåç§°",
  "url": "èµ„æºURL",
  "description": "èµ„æºæè¿°",
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
  "catalog_type": "models",
  "source": "modelscope_models",
  "type": "catalog_item"
}
```

---

## æ•°æ®å¤„ç†æµç¨‹

### 1. æ–‡æœ¬æ¸…ç†

- ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
- æ ‡å‡†åŒ–æ–‡æœ¬æ ¼å¼

### 2. æ–‡æœ¬åˆ†å—

- é»˜è®¤å—å¤§å°: 800å­—ç¬¦
- é»˜è®¤é‡å : 150å­—ç¬¦
- æ™ºèƒ½åˆ†å‰²(å°è¯•åœ¨å¥å·ã€æ®µè½å¤„åˆ†å‰²)

### 3. å…ƒæ•°æ®æå–

æ¯ä¸ªæ–‡æ¡£å—åŒ…å«:

```json
{
  "content": "æ–‡æ¡£å—å†…å®¹",
  "metadata": {
    "source_type": "docs",
    "title": "æ–‡æ¡£æ ‡é¢˜",
    "url": "åŸå§‹URL",
    "original_source": "modelscope_docs",
    "chunk_id": 0,
    "total_chunks": 5,
    "author": "ä½œè€…(å¦‚æœæœ‰)",
    "tags": "æ ‡ç­¾1,æ ‡ç­¾2",
    "stars": 1234
  }
}
```

### 4. å¯¼å‡ºæ ¼å¼

æœ€ç»ˆå¯¼å‡ºä¸ºJSONLæ ¼å¼ (`all_documents.jsonl`):
- æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡
- æ–¹ä¾¿æµå¼å¤„ç†
- æ˜“äºå¢é‡å¯¼å…¥

---

## å‘é‡æ•°æ®åº“å¯¼å…¥

### å¯¼å…¥æµç¨‹

1. **åŠ è½½æ–‡æ¡£**: ä»JSONLæ–‡ä»¶åŠ è½½
2. **è½¬æ¢æ ¼å¼**: è½¬ä¸ºLangChain Documentå¯¹è±¡
3. **æ‰¹é‡å¯¼å…¥**: åˆ†æ‰¹å¯¼å…¥å‘é‡åº“(é»˜è®¤50ä¸ª/æ‰¹)
4. **éªŒè¯**: æ£€æŸ¥å¯¼å…¥ç»Ÿè®¡

### æ€§èƒ½ä¼˜åŒ–

- **æ‰¹å¤„ç†**: å‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°
- **é”™è¯¯å¤„ç†**: å•æ‰¹å¤±è´¥ä¸å½±å“å…¶ä»–æ‰¹
- **ç»Ÿè®¡ä¿¡æ¯**: å®æ—¶æ˜¾ç¤ºå¯¼å…¥è¿›åº¦

### æŸ¥çœ‹å¯¼å…¥ç»“æœ

```bash
# å¯åŠ¨æœåŠ¡å™¨åè®¿é—®
curl http://localhost:8000/api/status

# æˆ–é€šè¿‡Webç•Œé¢æŸ¥çœ‹æ–‡æ¡£ç»Ÿè®¡
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜1: çˆ¬å–è¢«å°ç¦

**ç—‡çŠ¶**: å¤§é‡è¯·æ±‚å¤±è´¥,è¿”å›403/429é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ é€Ÿç‡é™åˆ¶: `rate_limit=3.0` (3ç§’/è¯·æ±‚)
2. ä½¿ç”¨ä»£ç†(å¦‚éœ€è¦)
3. åˆ†å¤šæ¬¡çˆ¬å–

### é—®é¢˜2: GitHub APIé™åˆ¶

**ç—‡çŠ¶**: GitHubçˆ¬å–å¤±è´¥,æç¤ºrate limit

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è®¾ç½®GitHub Token
export GITHUB_TOKEN="your_token"

# æœªè®¤è¯: 60æ¬¡/å°æ—¶
# å·²è®¤è¯: 5000æ¬¡/å°æ—¶
```

### é—®é¢˜3: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: å¤„ç†å¤§é‡æ•°æ®æ—¶å†…å­˜æº¢å‡º

**è§£å†³æ–¹æ¡ˆ**:
1. åˆ†æ‰¹å¤„ç†:
```bash
# åˆ†åˆ«çˆ¬å–å’Œå¤„ç†
python scripts/crawl_and_process.py --docs --process
python scripts/crawl_and_process.py --learn --process
# ...
```

2. å‡å°æ‰¹å¤„ç†å¤§å°:
```bash
python scripts/ingest_crawled_data.py --batch-size 20
```

### é—®é¢˜4: æ–­ç‚¹ç»­çˆ¬

**ç—‡çŠ¶**: çˆ¬å–ä¸­æ–­

**è§£å†³æ–¹æ¡ˆ**:
çˆ¬è™«è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹åˆ° `checkpoint.json`ã€‚
é‡æ–°è¿è¡Œå‘½ä»¤å³å¯ä»æ–­ç‚¹ç»§ç»­ã€‚

### é—®é¢˜5: é¡µé¢ç»“æ„å˜åŒ–

**ç—‡çŠ¶**: æå–ä¸åˆ°å†…å®¹

**è§£å†³æ–¹æ¡ˆ**:
æ£€æŸ¥å¹¶æ›´æ–°çˆ¬è™«çš„é€‰æ‹©å™¨:
```python
# åœ¨å¯¹åº”çš„çˆ¬è™«æ–‡ä»¶ä¸­è°ƒæ•´
content_elem = soup.find('article') or soup.find('main')
```

---

## æœ€ä½³å®è·µ

### 1. å®šæœŸæ›´æ–°æ•°æ®

å»ºè®®æ¯å‘¨æˆ–æ¯æœˆé‡æ–°çˆ¬å–:
```bash
# å®Œæ•´æ›´æ–°æµç¨‹
python scripts/crawl_and_process.py --all --process
python scripts/ingest_crawled_data.py
```

### 2. å¢é‡æ›´æ–°

å¯¹äºé¢‘ç¹æ›´æ–°çš„æº(å¦‚ç ”ä¹ ç¤¾):
```bash
# åªçˆ¬å–learnå¹¶å¯¼å…¥
python scripts/crawl_and_process.py --learn --process
python scripts/ingest_crawled_data.py --input data/processed/learn_processed.json
```

### 3. ç›‘æ§æ•°æ®è´¨é‡

å®šæœŸæ£€æŸ¥:
- æ–‡æ¡£æ•°é‡ç»Ÿè®¡
- å†…å®¹å®Œæ•´æ€§
- å…ƒæ•°æ®å‡†ç¡®æ€§

### 4. å¤‡ä»½åŸå§‹æ•°æ®

```bash
# å¤‡ä»½çˆ¬å–çš„åŸå§‹æ•°æ®
tar -czf backup_$(date +%Y%m%d).tar.gz data/crawled/
```

---

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„çˆ¬è™«

1. ç»§æ‰¿ `BaseCrawler`:

```python
from crawlers.base_crawler import BaseCrawler

class MyCrawler(BaseCrawler):
    def __init__(self, output_dir: str = "data/crawled/my_source"):
        super().__init__(output_dir)

    def crawl(self) -> List[Dict]:
        # å®ç°çˆ¬å–é€»è¾‘
        pass
```

2. åœ¨ `__init__.py` ä¸­æ³¨å†Œ:

```python
from .my_crawler import MyCrawler

__all__ = [..., 'MyCrawler']
```

3. åœ¨è„šæœ¬ä¸­ä½¿ç”¨:

```python
def crawl_my_source(output_dir: str = "data/crawled"):
    crawler = MyCrawler(output_dir=f"{output_dir}/my_source")
    crawler.crawl()
```

---

## è®¸å¯ä¸æ³¨æ„äº‹é¡¹

- **éµå®ˆrobots.txt**: çˆ¬è™«å°Šé‡ç½‘ç«™çš„robots.txtè§„åˆ™
- **é€Ÿç‡é™åˆ¶**: é¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›
- **ç‰ˆæƒ**: çˆ¬å–çš„å†…å®¹ä»…ç”¨äºæ¯”èµ›å’Œå­¦ä¹ ç›®çš„
- **æ›´æ–°é¢‘ç‡**: å»ºè®®åˆç†å®‰æ’çˆ¬å–é¢‘ç‡

---

## ç›¸å…³æ–‡æ¡£

- [need.md](../../need.md) - ç«èµ›éœ€æ±‚æ–‡æ¡£
- [README.md](../README.md) - é¡¹ç›®æ€»è§ˆ
- [APIæ–‡æ¡£](../api/README.md) - APIè¯´æ˜

---

**æœ€åæ›´æ–°**: 2025-12-02
