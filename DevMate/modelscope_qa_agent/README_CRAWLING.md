# é­”æ­ç¤¾åŒºæ•°æ®çˆ¬å–ç³»ç»Ÿ

## æ¦‚è¿°

æ ¹æ®éœ€æ±‚æ–‡æ¡£ (`need.md`) ä¸­"4. å®˜æ–¹æŒ‡å®šæ•°æ®èµ„æº"çš„è¦æ±‚,æœ¬ç³»ç»Ÿå®ç°äº†å®Œæ•´çš„æ•°æ®çˆ¬å–ã€å¤„ç†å’Œå¯¼å…¥æµç¨‹ã€‚

## å¿«é€Ÿå¼€å§‹

### 1. çˆ¬å–æ‰€æœ‰æ•°æ®æº

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate langchain-env

# çˆ¬å–æ‰€æœ‰æ•°æ®æºå¹¶å¤„ç†
python scripts/crawl_and_process.py --all --process
```

è¿™å°†çˆ¬å–:
- é­”æ­ç¤¾åŒºå®˜æ–¹æ–‡æ¡£ (https://www.modelscope.cn/docs/overview)
- ç ”ä¹ ç¤¾ (https://modelscope.cn/learn)
- GitHubä»“åº“ (https://github.com/modelscope)
- æ¨¡å‹/æ•°æ®é›†/åˆ›ç©ºé—´ç­‰ç›®å½•

### 2. å¯¼å…¥åˆ°å‘é‡æ•°æ®åº“

```bash
# å¯¼å…¥å¤„ç†åçš„æ•°æ®
python scripts/ingest_crawled_data.py --input data/processed/all_documents.jsonl
```

## ç›®å½•ç»“æ„

```
crawlers/                        # çˆ¬è™«æ¨¡å—
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base_crawler.py             # åŸºç±»
â”œâ”€â”€ docs_crawler.py             # æ–‡æ¡£çˆ¬è™«
â”œâ”€â”€ learn_crawler.py            # ç ”ä¹ ç¤¾çˆ¬è™«
â”œâ”€â”€ github_crawler.py           # GitHubçˆ¬è™«
â”œâ”€â”€ catalog_crawler.py          # ç›®å½•çˆ¬è™«
â””â”€â”€ data_processor.py           # æ•°æ®å¤„ç†

scripts/
â”œâ”€â”€ crawl_and_process.py        # çˆ¬å–å’Œå¤„ç†è„šæœ¬
â””â”€â”€ ingest_crawled_data.py      # æ•°æ®å¯¼å…¥è„šæœ¬

data/
â”œâ”€â”€ crawled/                    # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ learn/
â”‚   â”œâ”€â”€ github/
â”‚   â””â”€â”€ catalog/
â””â”€â”€ processed/                  # å¤„ç†åæ•°æ®
    â””â”€â”€ all_documents.jsonl

docs/
â””â”€â”€ DATA_CRAWLING.md            # è¯¦ç»†æ–‡æ¡£
```

## ä½¿ç”¨æŒ‡å—

### çˆ¬å–ç‰¹å®šæ•°æ®æº

```bash
# åªçˆ¬å–å®˜æ–¹æ–‡æ¡£
python scripts/crawl_and_process.py --docs --process

# çˆ¬å–ç ”ä¹ ç¤¾å’ŒGitHub
python scripts/crawl_and_process.py --learn --github --process

# åªçˆ¬å–ç›®å½•
python scripts/crawl_and_process.py --catalog --process
```

### åªå¤„ç†å·²çˆ¬å–çš„æ•°æ®

```bash
python scripts/crawl_and_process.py --process-only
```

### è‡ªå®šä¹‰å¯¼å…¥å‚æ•°

```bash
# è°ƒæ•´æ‰¹å¤„ç†å¤§å°
python scripts/ingest_crawled_data.py --batch-size 100

# æµ‹è¯•åŠ è½½(ä¸å®é™…å¯¼å…¥)
python scripts/ingest_crawled_data.py --dry-run
```

## åŠŸèƒ½ç‰¹ç‚¹

### 1. å¤šæºçˆ¬å–
- âœ… å®˜æ–¹æ–‡æ¡£ (é€’å½’çˆ¬å–,æ·±åº¦3å±‚)
- âœ… ç ”ä¹ ç¤¾æ–‡ç«  (å«ä½œè€…ã€æ ‡ç­¾ã€ä»£ç å—)
- âœ… GitHubä»“åº“ (READMEã€å…ƒæ•°æ®ã€stars)
- âœ… èµ„æºç›®å½• (models/datasets/studios/mcp/aigc)

### 2. æ•°æ®å¤„ç†
- âœ… æ–‡æœ¬æ¸…ç†å’Œæ ‡å‡†åŒ–
- âœ… æ™ºèƒ½åˆ†å— (chunk_size=800, overlap=150)
- âœ… å…ƒæ•°æ®æå–å’Œå¢å¼º
- âœ… JSONLæ ¼å¼å¯¼å‡º

### 3. æ–­ç‚¹ç»­çˆ¬
- âœ… è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹
- âœ… ä¸­æ–­åå¯ç»§ç»­
- âœ… é¿å…é‡å¤çˆ¬å–

### 4. é€Ÿç‡é™åˆ¶
- âœ… éµå®ˆç½‘ç«™è§„åˆ™
- âœ… å¯é…ç½®è¯·æ±‚é—´éš”
- âœ… è‡ªåŠ¨é‡è¯•æœºåˆ¶

## é…ç½®è¯´æ˜

### çˆ¬è™«é€Ÿç‡é™åˆ¶

åœ¨çˆ¬è™«åˆå§‹åŒ–æ—¶é…ç½®:

```python
from crawlers import DocsCrawler

crawler = DocsCrawler(
    output_dir="data/crawled/docs",
    rate_limit=2.0  # 2ç§’/è¯·æ±‚
)
```

### GitHub Token (å¯é€‰)

ä¸ºæé«˜APIé€Ÿç‡é™åˆ¶:

```bash
export GITHUB_TOKEN="your_github_token_here"
```

### æ–‡æœ¬åˆ†å—å‚æ•°

åœ¨ `crawlers/data_processor.py` ä¸­:

```python
chunks = processor.chunk_text(
    text,
    chunk_size=800,      # ä¸ç³»ç»Ÿé…ç½®ä¸€è‡´
    chunk_overlap=150     # å—é‡å 
)
```

## æ•°æ®ç»Ÿè®¡

çˆ¬å–å®Œæˆå,æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯:

```bash
# æŸ¥çœ‹æ±‡æ€»
cat data/crawled/docs/summary.json
cat data/crawled/learn/summary.json
cat data/crawled/github/summary.json
cat data/crawled/catalog/summary.json

# æŸ¥çœ‹å¤„ç†åç»Ÿè®¡
cat data/processed/summary.json
```

## æ•…éšœæ’é™¤

### é—®é¢˜1: è¯·æ±‚è¢«å°ç¦

**ç—‡çŠ¶**: å¤§é‡403/429é”™è¯¯

**è§£å†³**:
```bash
# å¢åŠ é€Ÿç‡é™åˆ¶
# åœ¨crawleråˆå§‹åŒ–æ—¶è®¾ç½® rate_limit=3.0
```

### é—®é¢˜2: GitHub APIé™åˆ¶

**ç—‡çŠ¶**: GitHubçˆ¬å–å¤±è´¥

**è§£å†³**:
```bash
# è®¾ç½®GitHub Token
export GITHUB_TOKEN="your_token"
```

### é—®é¢˜3: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: å¤„ç†å¤§æ•°æ®æ—¶å†…å­˜æº¢å‡º

**è§£å†³**:
```bash
# åˆ†æ‰¹å¤„ç†
python scripts/crawl_and_process.py --docs --process
python scripts/crawl_and_process.py --learn --process
# ç„¶ååˆ†åˆ«å¯¼å…¥
```

### é—®é¢˜4: å¯¼å…¥å¤±è´¥

**ç—‡çŠ¶**: å‘é‡åº“å¯¼å…¥æŠ¥é”™

**è§£å†³**:
```bash
# ç¡®ä¿Milvusè¿è¡Œæ­£å¸¸
docker ps | grep milvus

# æ£€æŸ¥é…ç½®
cat config/settings.yaml

# å‡å°æ‰¹å¤„ç†å¤§å°
python scripts/ingest_crawled_data.py --batch-size 20
```

## æµ‹è¯•

```bash
# è¿è¡Œçˆ¬è™«æµ‹è¯•
pytest tests/test_crawlers.py -v

# æµ‹è¯•å•ä¸ªåŠŸèƒ½
pytest tests/test_crawlers.py::TestDocsCrawler -v
```

## æœ€ä½³å®è·µ

### 1. å®šæœŸæ›´æ–°

å»ºè®®æ¯å‘¨æ›´æ–°ä¸€æ¬¡:

```bash
# å®Œæ•´æ›´æ–°æµç¨‹
./update_data.sh  # è§ä¸‹æ–¹è„šæœ¬
```

åˆ›å»º `update_data.sh`:

```bash
#!/bin/bash
set -e

echo "å¼€å§‹æ›´æ–°æ•°æ®..."

# çˆ¬å–æ‰€æœ‰æ•°æ®æº
python scripts/crawl_and_process.py --all --process

# å¯¼å…¥å‘é‡åº“
python scripts/ingest_crawled_data.py --input data/processed/all_documents.jsonl

# å¤‡ä»½
tar -czf backup_$(date +%Y%m%d).tar.gz data/crawled/

echo "æ›´æ–°å®Œæˆ!"
```

### 2. å¢é‡æ›´æ–°

å¯¹äºé¢‘ç¹æ›´æ–°çš„æº(å¦‚ç ”ä¹ ç¤¾):

```bash
# æ¯å¤©æ›´æ–°learn
python scripts/crawl_and_process.py --learn --process
python scripts/ingest_crawled_data.py --input data/processed/learn_processed.json
```

### 3. ç›‘æ§æ•°æ®è´¨é‡

```bash
# æ£€æŸ¥æ–‡æ¡£æ•°é‡
python -c "
import json
with open('data/processed/summary.json') as f:
    data = json.load(f)
    print(f'Total chunks: {data[\"total_chunks\"]}')
    for source, count in data['by_source'].items():
        print(f'  {source}: {count}')
"
```

## ç›¸å…³æ–‡æ¡£

- ğŸ“˜ [è¯¦ç»†çˆ¬å–æ–‡æ¡£](docs/DATA_CRAWLING.md) - å®Œæ•´çš„æŠ€æœ¯æ–‡æ¡£
- ğŸ“‹ [éœ€æ±‚æ–‡æ¡£](../need.md) - ç«èµ›éœ€æ±‚
- ğŸ—ï¸ [é¡¹ç›®README](README.md) - é¡¹ç›®æ€»è§ˆ

## æ•°æ®æ¥æº

æ ¹æ®ç«èµ›éœ€æ±‚æ–‡æ¡£ (`need.md`) ç¬¬4èŠ‚:

| æ•°æ®æº | URL | çŠ¶æ€ |
|-------|-----|-----|
| å®˜æ–¹æ–‡æ¡£ | https://www.modelscope.cn/docs/overview | âœ… å·²å®ç° |
| ç ”ä¹ ç¤¾ | https://modelscope.cn/learn | âœ… å·²å®ç° |
| GitHub | https://github.com/modelscope | âœ… å·²å®ç° |
| æ¨¡å‹åº“ | https://modelscope.cn/models | âœ… å·²å®ç° |
| æ•°æ®é›† | https://modelscope.cn/datasets | âœ… å·²å®ç° |
| åˆ›ç©ºé—´ | https://modelscope.cn/studios | âœ… å·²å®ç° |
| MCP | https://www.modelscope.cn/mcp | âœ… å·²å®ç° |
| AIGC | https://www.modelscope.cn/aigc | âœ… å·²å®ç° |

## æ³¨æ„äº‹é¡¹

- âš ï¸ éµå®ˆrobots.txtè§„åˆ™
- âš ï¸ åˆç†è®¾ç½®é€Ÿç‡é™åˆ¶
- âš ï¸ çˆ¬å–çš„å†…å®¹ä»…ç”¨äºæ¯”èµ›å’Œå­¦ä¹ 
- âš ï¸ å®šæœŸå¤‡ä»½åŸå§‹æ•°æ®
- âš ï¸ ç›‘æ§çˆ¬å–è´¨é‡å’Œå®Œæ•´æ€§

## æ”¯æŒ

å¦‚æœ‰é—®é¢˜,è¯·:
1. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: `docs/DATA_CRAWLING.md`
2. è¿è¡Œæµ‹è¯•: `pytest tests/test_crawlers.py -v`
3. æ£€æŸ¥æ—¥å¿—è¾“å‡º

---

**æœ€åæ›´æ–°**: 2025-12-02
**ä½œè€…**: DevMate Team
