"""
Data Processor

å¤„ç†çˆ¬å–çš„æ•°æ®å¹¶å‡†å¤‡å¯¼å…¥å‘é‡æ•°æ®åº“
"""

import json
import os
from typing import List, Dict
from pathlib import Path
from datetime import datetime


class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""

    def __init__(self, crawled_data_dir: str = "data/crawled", processed_data_dir: str = "data/processed"):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            crawled_data_dir: çˆ¬å–æ•°æ®ç›®å½•
            processed_data_dir: å¤„ç†åæ•°æ®ç›®å½•
        """
        self.crawled_data_dir = Path(crawled_data_dir)
        self.processed_data_dir = Path(processed_data_dir)
        self.processed_data_dir.mkdir(parents=True, exist_ok=True)

    def load_json_files(self, directory: Path) -> List[Dict]:
        """
        åŠ è½½ç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶

        Args:
            directory: ç›®å½•è·¯å¾„

        Returns:
            JSONæ•°æ®åˆ—è¡¨
        """
        data = []

        if not directory.exists():
            print(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨: {directory}")
            return data

        for json_file in directory.glob("**/*.json"):
            # è·³è¿‡æ±‡æ€»æ–‡ä»¶å’Œæ£€æŸ¥ç‚¹æ–‡ä»¶
            if json_file.name in ['summary.json', 'checkpoint.json']:
                continue

            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    content = json.load(f)
                    # å¦‚æœæ˜¯åŒ…å«itemsçš„ç›®å½•æ–‡ä»¶,å±•å¼€items
                    if isinstance(content, dict) and 'items' in content:
                        data.extend(content['items'])
                    elif isinstance(content, dict):
                        data.append(content)
                    elif isinstance(content, list):
                        data.extend(content)
            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {json_file}")
                print(f"   é”™è¯¯: {e}")

        return data

    def clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""

        # ç§»é™¤å¤šä½™ç©ºç™½
        import re
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()

        return text

    def chunk_text(self, text: str, chunk_size: int = 800, chunk_overlap: int = 150) -> List[str]:
        """
        åˆ†å—æ–‡æœ¬

        Args:
            text: æ–‡æœ¬
            chunk_size: å—å¤§å°
            chunk_overlap: å—é‡å 

        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text or len(text) <= chunk_size:
            return [text] if text else []

        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]

            # å°è¯•åœ¨å¥å·ã€æ¢è¡Œç¬¦ç­‰å¤„åˆ†å‰²
            if end < len(text):
                for sep in ['\n\n', '\n', 'ã€‚', '. ', '! ', '? ']:
                    last_sep = chunk.rfind(sep)
                    if last_sep > chunk_size * 0.5:  # è‡³å°‘ä¿ç•™ä¸€åŠå†…å®¹
                        chunk = text[start:start + last_sep + len(sep)]
                        break

            chunks.append(chunk.strip())
            start += len(chunk) - chunk_overlap

        return chunks

    def process_document(self, doc: Dict, source_type: str) -> List[Dict]:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£

        Args:
            doc: æ–‡æ¡£æ•°æ®
            source_type: æºç±»å‹

        Returns:
            å¤„ç†åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        processed_chunks = []

        # æå–å†…å®¹
        content = doc.get('content', '') or doc.get('description', '') or doc.get('readme', '')

        if not content or len(content) < 50:
            return []

        # æ¸…ç†æ–‡æœ¬
        content = self.clean_text(content)

        # åˆ†å—
        chunks = self.chunk_text(content)

        # ç”Ÿæˆå…ƒæ•°æ®
        base_metadata = {
            'source_type': source_type,
            'title': doc.get('title', 'Untitled'),
            'url': doc.get('url', ''),
            'original_source': doc.get('source', source_type),
        }

        # æ·»åŠ é¢å¤–å…ƒæ•°æ®
        if 'author' in doc:
            base_metadata['author'] = doc['author']
        if 'date' in doc:
            base_metadata['date'] = doc['date']
        if 'tags' in doc and doc['tags']:
            base_metadata['tags'] = ','.join(doc['tags'])
        if 'language' in doc:
            base_metadata['language'] = doc['language']
        if 'stars' in doc:
            base_metadata['stars'] = doc['stars']

        # ä¸ºæ¯ä¸ªå—åˆ›å»ºæ–‡æ¡£
        for i, chunk in enumerate(chunks):
            chunk_doc = {
                'content': chunk,
                'metadata': {
                    **base_metadata,
                    'chunk_id': i,
                    'total_chunks': len(chunks),
                }
            }
            processed_chunks.append(chunk_doc)

        return processed_chunks

    def process_all(self) -> Dict[str, List[Dict]]:
        """
        å¤„ç†æ‰€æœ‰çˆ¬å–çš„æ•°æ®

        Returns:
            æŒ‰æºç±»å‹åˆ†ç»„çš„å¤„ç†åæ•°æ®
        """
        print("=" * 70)
        print("å¼€å§‹å¤„ç†çˆ¬å–æ•°æ®")
        print("=" * 70)

        results = {}

        # å¤„ç†å„ç±»æ•°æ®
        source_dirs = {
            'docs': self.crawled_data_dir / 'docs',
            'learn': self.crawled_data_dir / 'learn',
            'github': self.crawled_data_dir / 'github',
            'catalog': self.crawled_data_dir / 'catalog',
        }

        total_processed = 0

        for source_type, source_dir in source_dirs.items():
            print(f"\nğŸ“‚ å¤„ç† {source_type} æ•°æ®...")

            # åŠ è½½åŸå§‹æ•°æ®
            raw_data = self.load_json_files(source_dir)
            print(f"   åŠ è½½äº† {len(raw_data)} ä¸ªåŸå§‹æ–‡æ¡£")

            # å¤„ç†æ¯ä¸ªæ–‡æ¡£
            processed_docs = []
            for doc in raw_data:
                chunks = self.process_document(doc, source_type)
                processed_docs.extend(chunks)

            print(f"   ç”Ÿæˆäº† {len(processed_docs)} ä¸ªæ–‡æ¡£å—")
            total_processed += len(processed_docs)

            results[source_type] = processed_docs

            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            if processed_docs:
                output_file = self.processed_data_dir / f"{source_type}_processed.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(processed_docs, f, ensure_ascii=False, indent=2)
                print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")

        # ä¿å­˜æ±‡æ€»
        summary = {
            'total_chunks': total_processed,
            'by_source': {k: len(v) for k, v in results.items()},
            'processed_at': datetime.now().isoformat(),
        }

        summary_file = self.processed_data_dir / 'summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print("\n" + "=" * 70)
        print(f"âœ… å¤„ç†å®Œæˆ! å…± {total_processed} ä¸ªæ–‡æ¡£å—")
        for source, count in summary['by_source'].items():
            print(f"   {source}: {count} å—")
        print("=" * 70)

        return results

    def export_for_ingestion(self, output_file: str = "data/processed/all_documents.jsonl"):
        """
        å¯¼å‡ºä¸ºJSONLæ ¼å¼,ç”¨äºå¯¼å…¥å‘é‡æ•°æ®åº“

        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print(f"\nğŸ“¤ å¯¼å‡ºæ•°æ®åˆ° {output_file}...")

        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        count = 0
        with open(output_path, 'w', encoding='utf-8') as f:
            for json_file in self.processed_data_dir.glob("*_processed.json"):
                try:
                    with open(json_file, 'r', encoding='utf-8') as jf:
                        docs = json.load(jf)
                        for doc in docs:
                            f.write(json.dumps(doc, ensure_ascii=False) + '\n')
                            count += 1
                except Exception as e:
                    print(f"âŒ å¯¼å‡ºå¤±è´¥: {json_file}")
                    print(f"   é”™è¯¯: {e}")

        print(f"âœ… å·²å¯¼å‡º {count} ä¸ªæ–‡æ¡£åˆ° {output_path}")
        return count
