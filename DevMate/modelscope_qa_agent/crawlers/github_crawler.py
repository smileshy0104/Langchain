"""
GitHub Repositories Crawler

çˆ¬å–é­”æ­ç¤¾åŒºGitHubä»“åº“: https://github.com/modelscope
"""

from typing import List, Dict, Optional
import os
from .base_crawler import BaseCrawler


class GitHubCrawler(BaseCrawler):
    """GitHubä»“åº“çˆ¬è™«"""

    def __init__(self, output_dir: str = "data/crawled/github", rate_limit: float = 2.0):
        super().__init__(output_dir, rate_limit)
        self.org_name = "modelscope"
        self.api_base = "https://api.github.com"
        self.github_token = os.getenv("GITHUB_TOKEN")  # å¯é€‰: æé«˜APIé™åˆ¶

        if self.github_token:
            self.session.headers.update({
                'Authorization': f'token {self.github_token}'
            })

    def fetch_repositories(self) -> List[Dict]:
        """
        è·å–ç»„ç»‡çš„æ‰€æœ‰ä»“åº“

        Returns:
            ä»“åº“åˆ—è¡¨
        """
        repos = []
        page = 1
        per_page = 100

        print(f"ğŸ“¦ è·å– {self.org_name} ç»„ç»‡çš„ä»“åº“...")

        while True:
            url = f"{self.api_base}/orgs/{self.org_name}/repos?page={page}&per_page={per_page}"
            self._rate_limit_wait()

            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                page_repos = response.json()

                if not page_repos:
                    break

                repos.extend(page_repos)
                print(f"   é¡µ {page}: è·å–äº† {len(page_repos)} ä¸ªä»“åº“")
                page += 1

            except Exception as e:
                print(f"âŒ è·å–ä»“åº“åˆ—è¡¨å¤±è´¥: {e}")
                break

        print(f"âœ… å…±æ‰¾åˆ° {len(repos)} ä¸ªä»“åº“")
        return repos

    def fetch_readme(self, repo_full_name: str) -> Optional[str]:
        """
        è·å–ä»“åº“README

        Args:
            repo_full_name: ä»“åº“å…¨å (å¦‚ modelscope/modelscope)

        Returns:
            READMEå†…å®¹
        """
        url = f"{self.api_base}/repos/{repo_full_name}/readme"
        self._rate_limit_wait()

        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            data = response.json()

            # READMEå†…å®¹æ˜¯base64ç¼–ç çš„
            import base64
            content = base64.b64decode(data['content']).decode('utf-8')
            return content

        except Exception as e:
            print(f"   âš ï¸  æ— æ³•è·å–README: {e}")
            return None

    def extract_repo_info(self, repo: Dict) -> Dict:
        """
        æå–ä»“åº“ä¿¡æ¯

        Args:
            repo: GitHub APIè¿”å›çš„ä»“åº“æ•°æ®

        Returns:
            æ ¼å¼åŒ–çš„ä»“åº“ä¿¡æ¯
        """
        readme = self.fetch_readme(repo['full_name'])

        return {
            'name': repo['name'],
            'full_name': repo['full_name'],
            'description': repo.get('description', ''),
            'url': repo['html_url'],
            'stars': repo['stargazers_count'],
            'forks': repo['forks_count'],
            'language': repo.get('language', ''),
            'topics': repo.get('topics', []),
            'created_at': repo['created_at'],
            'updated_at': repo['updated_at'],
            'readme': readme,
            'source': 'github',
            'type': 'repository'
        }

    def crawl(self) -> List[Dict]:
        """
        æ‰§è¡Œçˆ¬å–

        Returns:
            çˆ¬å–çš„ä»“åº“ä¿¡æ¯åˆ—è¡¨
        """
        print("=" * 70)
        print(f"å¼€å§‹çˆ¬å– GitHub {self.org_name} ç»„ç»‡ä»“åº“")
        print("=" * 70)

        try:
            # è·å–æ‰€æœ‰ä»“åº“
            repos = self.fetch_repositories()

            # æå–è¯¦ç»†ä¿¡æ¯
            repo_data_list = []
            for i, repo in enumerate(repos, 1):
                print(f"\nğŸ“¦ [{i}/{len(repos)}] å¤„ç†ä»“åº“: {repo['name']}")
                print(f"   â­ {repo['stargazers_count']} stars | ğŸ´ {repo['forks_count']} forks")

                repo_data = self.extract_repo_info(repo)
                repo_data_list.append(repo_data)

                # ä¿å­˜JSONæ ¼å¼
                filename = f"repo_{repo['name']}.json"
                self.save_json(repo_data, filename)

                # ä¿å­˜Markdownæ ¼å¼
                md_content = self.convert_to_markdown(repo_data)
                md_filename = f"repo_{repo['name']}"
                self.save_markdown(md_content, md_filename)

            # ä¿å­˜æ±‡æ€»
            summary = {
                'total_repositories': len(repo_data_list),
                'repositories': [{'name': r['name'], 'stars': r['stars']} for r in repo_data_list],
                'metadata': self.get_metadata()
            }
            self.save_json(summary, 'summary.json')

            print("\n" + "=" * 70)
            print(f"âœ… çˆ¬å–å®Œæˆ! å…± {len(repo_data_list)} ä¸ªä»“åº“")
            print("=" * 70)

            return repo_data_list

        except Exception as e:
            print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
