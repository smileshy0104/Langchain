"""
Base Crawler Class

æ‰€æœ‰çˆ¬è™«çš„åŸºç±»ï¼Œæä¾›é€šç”¨åŠŸèƒ½
"""

import os
import json
import time
from abc import ABC, abstractmethod
from typing import List, Dict, Optional
from pathlib import Path
import requests
from bs4 import BeautifulSoup
from datetime import datetime


class BaseCrawler(ABC):
    """çˆ¬è™«åŸºç±»"""

    def __init__(self, output_dir: str = "data/crawled", rate_limit: float = 1.0):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            output_dir: è¾“å‡ºç›®å½•
            rate_limit: è¯·æ±‚é—´éš”(ç§’)
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºmarkdownå­ç›®å½•
        self.markdown_dir = self.output_dir / "markdown"
        self.markdown_dir.mkdir(parents=True, exist_ok=True)

        self.rate_limit = rate_limit
        self.last_request_time = 0

        # è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }

        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def _rate_limit_wait(self):
        """é€Ÿç‡é™åˆ¶ç­‰å¾…"""
        elapsed = time.time() - self.last_request_time
        if elapsed < self.rate_limit:
            time.sleep(self.rate_limit - elapsed)
        self.last_request_time = time.time()

    def fetch_page(self, url: str, max_retries: int = 3) -> Optional[str]:
        """
        è·å–é¡µé¢å†…å®¹

        Args:
            url: é¡µé¢URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            é¡µé¢HTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        self._rate_limit_wait()

        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response.text
            except requests.RequestException as e:
                print(f"âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {url}")
                print(f"   é”™è¯¯: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    return None

        return None

    def parse_html(self, html: str) -> BeautifulSoup:
        """
        è§£æHTML

        Args:
            html: HTMLå†…å®¹

        Returns:
            BeautifulSoupå¯¹è±¡
        """
        return BeautifulSoup(html, 'html.parser')

    def save_json(self, data: Dict, filename: str):
        """
        ä¿å­˜JSONæ•°æ®

        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            filename: æ–‡ä»¶å
        """
        filepath = self.output_dir / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ… å·²ä¿å­˜: {filepath}")

    def save_text(self, content: str, filename: str):
        """
        ä¿å­˜æ–‡æœ¬æ•°æ®

        Args:
            content: æ–‡æœ¬å†…å®¹
            filename: æ–‡ä»¶å
        """
        filepath = self.output_dir / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"âœ… å·²ä¿å­˜: {filepath}")

    def save_markdown(self, content: str, filename: str):
        """
        ä¿å­˜Markdownæ–‡æ¡£

        Args:
            content: Markdownå†…å®¹
            filename: æ–‡ä»¶å (è‡ªåŠ¨æ·»åŠ .mdåç¼€)
        """
        if not filename.endswith('.md'):
            filename = filename + '.md'

        filepath = self.markdown_dir / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"ğŸ“ å·²ä¿å­˜Markdown: {filepath}")

    def convert_to_markdown(self, data: Dict) -> str:
        """
        å°†æ•°æ®è½¬æ¢ä¸ºMarkdownæ ¼å¼

        Args:
            data: æ–‡æ¡£æ•°æ®å­—å…¸

        Returns:
            Markdownæ ¼å¼æ–‡æœ¬
        """
        md_lines = []

        # æ ‡é¢˜
        title = data.get('title', 'Untitled')
        md_lines.append(f"# {title}\n")

        # å…ƒæ•°æ®
        md_lines.append("---\n")
        if 'url' in data and data['url']:
            md_lines.append(f"**URL**: {data['url']}\n")
        if 'author' in data and data['author']:
            md_lines.append(f"**ä½œè€…**: {data['author']}\n")
        if 'date' in data and data['date']:
            md_lines.append(f"**æ—¥æœŸ**: {data['date']}\n")
        if 'source' in data:
            md_lines.append(f"**æ¥æº**: {data['source']}\n")
        if 'tags' in data and data['tags']:
            tags = ', '.join(data['tags']) if isinstance(data['tags'], list) else data['tags']
            md_lines.append(f"**æ ‡ç­¾**: {tags}\n")
        if 'language' in data and data['language']:
            md_lines.append(f"**è¯­è¨€**: {data['language']}\n")
        if 'stars' in data:
            md_lines.append(f"**Stars**: â­ {data['stars']}\n")
        if 'forks' in data:
            md_lines.append(f"**Forks**: ğŸ´ {data['forks']}\n")
        md_lines.append("---\n\n")

        # æè¿°
        if 'description' in data and data['description']:
            md_lines.append("## æè¿°\n\n")
            md_lines.append(f"{data['description']}\n\n")

        # ä¸»è¦å†…å®¹
        if 'content' in data and data['content']:
            md_lines.append("## å†…å®¹\n\n")
            md_lines.append(f"{data['content']}\n\n")

        # READMEå†…å®¹
        if 'readme' in data and data['readme']:
            md_lines.append("## README\n\n")
            md_lines.append(f"{data['readme']}\n\n")

        # ä»£ç å—
        if 'code_blocks' in data and data['code_blocks']:
            md_lines.append("## ä»£ç ç¤ºä¾‹\n\n")
            for i, code in enumerate(data['code_blocks'], 1):
                md_lines.append(f"### ç¤ºä¾‹ {i}\n\n")
                md_lines.append("```\n")
                md_lines.append(f"{code}\n")
                md_lines.append("```\n\n")

        return ''.join(md_lines)

    def load_checkpoint(self, checkpoint_file: str = "checkpoint.json") -> Dict:
        """
        åŠ è½½æ£€æŸ¥ç‚¹

        Args:
            checkpoint_file: æ£€æŸ¥ç‚¹æ–‡ä»¶å

        Returns:
            æ£€æŸ¥ç‚¹æ•°æ®
        """
        filepath = self.output_dir / checkpoint_file
        if filepath.exists():
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_checkpoint(self, data: Dict, checkpoint_file: str = "checkpoint.json"):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹

        Args:
            data: æ£€æŸ¥ç‚¹æ•°æ®
            checkpoint_file: æ£€æŸ¥ç‚¹æ–‡ä»¶å
        """
        filepath = self.output_dir / checkpoint_file
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    @abstractmethod
    def crawl(self) -> List[Dict]:
        """
        æ‰§è¡Œçˆ¬å–

        Returns:
            çˆ¬å–çš„æ•°æ®åˆ—è¡¨
        """
        pass

    def get_metadata(self) -> Dict:
        """
        è·å–çˆ¬è™«å…ƒæ•°æ®

        Returns:
            å…ƒæ•°æ®å­—å…¸
        """
        return {
            'crawler_name': self.__class__.__name__,
            'output_dir': str(self.output_dir),
            'timestamp': datetime.now().isoformat(),
        }
