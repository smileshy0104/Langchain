"""
ModelScope Catalog Crawler

çˆ¬å–æ¨¡å‹åº“ã€æ•°æ®é›†ã€åˆ›ç©ºé—´ç­‰èµ„æºç›®å½•
"""

from typing import List, Dict, Optional
from .base_crawler import BaseCrawler


class CatalogCrawler(BaseCrawler):
    """èµ„æºç›®å½•çˆ¬è™«"""

    def __init__(self, output_dir: str = "data/crawled/catalog", rate_limit: float = 1.5):
        super().__init__(output_dir, rate_limit)
        self.catalogs = {
            'models': 'https://modelscope.cn/models',
            'datasets': 'https://modelscope.cn/datasets',
            'studios': 'https://modelscope.cn/studios',
            'mcp': 'https://www.modelscope.cn/mcp',
            'aigc': 'https://www.modelscope.cn/aigc',
        }

    def extract_catalog_items(self, soup, catalog_type: str) -> List[Dict]:
        """
        æå–ç›®å½•é¡¹

        Args:
            soup: BeautifulSoupå¯¹è±¡
            catalog_type: ç›®å½•ç±»å‹ (models/datasets/studiosç­‰)

        Returns:
            ç›®å½•é¡¹åˆ—è¡¨
        """
        items = []

        # æŸ¥æ‰¾å¡ç‰‡å…ƒç´  (æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´)
        # è¿™é‡Œæä¾›é€šç”¨çš„é€‰æ‹©å™¨ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®é¡µé¢ç»“æ„è°ƒæ•´
        card_selectors = [
            'div.card',
            'div.item',
            'div.model-card',
            'div.dataset-card',
            'article',
            'li.list-item'
        ]

        cards = []
        for selector in card_selectors:
            found = soup.select(selector)
            if found:
                cards = found
                break

        print(f"   æ‰¾åˆ° {len(cards)} ä¸ªå¡ç‰‡å…ƒç´ ")

        for card in cards[:50]:  # é™åˆ¶æ¯ä¸ªç›®å½•æœ€å¤š50é¡¹
            try:
                # æå–æ ‡é¢˜
                title = None
                title_elem = card.find(['h2', 'h3', 'h4', 'a'])
                if title_elem:
                    title = title_elem.get_text(strip=True)

                # æå–é“¾æ¥
                url = None
                link_elem = card.find('a', href=True)
                if link_elem:
                    from urllib.parse import urljoin
                    url = urljoin(self.catalogs[catalog_type], link_elem['href'])

                # æå–æè¿°
                description = None
                desc_elem = card.find(['p', 'div'], class_=lambda x: x and ('desc' in x or 'description' in x))
                if desc_elem:
                    description = desc_elem.get_text(strip=True)

                # æå–æ ‡ç­¾
                tags = []
                tag_elems = card.find_all(class_=lambda x: x and 'tag' in x)
                for tag_elem in tag_elems:
                    tag = tag_elem.get_text(strip=True)
                    if tag:
                        tags.append(tag)

                if title:
                    items.append({
                        'title': title,
                        'url': url,
                        'description': description or '',
                        'tags': tags,
                        'catalog_type': catalog_type,
                        'source': f'modelscope_{catalog_type}',
                        'type': 'catalog_item'
                    })

            except Exception as e:
                continue

        return items

    def crawl_catalog(self, catalog_type: str, url: str) -> List[Dict]:
        """
        çˆ¬å–å•ä¸ªç›®å½•

        Args:
            catalog_type: ç›®å½•ç±»å‹
            url: ç›®å½•URL

        Returns:
            ç›®å½•é¡¹åˆ—è¡¨
        """
        print(f"\nğŸ“š çˆ¬å– {catalog_type} ç›®å½•...")
        print(f"   URL: {url}")

        html = self.fetch_page(url)
        if not html:
            print(f"âŒ æ— æ³•è·å– {catalog_type} ç›®å½•")
            return []

        soup = self.parse_html(html)
        items = self.extract_catalog_items(soup, catalog_type)

        print(f"âœ… ä» {catalog_type} æå–äº† {len(items)} é¡¹")

        # ä¿å­˜ç›®å½•æ•°æ®
        if items:
            filename = f"{catalog_type}_catalog.json"
            self.save_json({
                'catalog_type': catalog_type,
                'url': url,
                'total_items': len(items),
                'items': items
            }, filename)

            # ä¸ºæ¯ä¸ªç›®å½•é¡¹ä¿å­˜Markdown
            for i, item in enumerate(items, 1):
                md_content = self.convert_to_markdown(item)
                md_filename = f"{catalog_type}_{i}"
                self.save_markdown(md_content, md_filename)

        return items

    def crawl(self) -> List[Dict]:
        """
        æ‰§è¡Œçˆ¬å–

        Returns:
            çˆ¬å–çš„ç›®å½•é¡¹åˆ—è¡¨
        """
        print("=" * 70)
        print("å¼€å§‹çˆ¬å–é­”æ­ç¤¾åŒºèµ„æºç›®å½•")
        print("=" * 70)

        all_items = []

        try:
            for catalog_type, url in self.catalogs.items():
                items = self.crawl_catalog(catalog_type, url)
                all_items.extend(items)

            # ä¿å­˜æ±‡æ€»
            summary = {
                'total_items': len(all_items),
                'by_catalog': {
                    catalog: len([i for i in all_items if i['catalog_type'] == catalog])
                    for catalog in self.catalogs.keys()
                },
                'metadata': self.get_metadata()
            }
            self.save_json(summary, 'summary.json')

            print("\n" + "=" * 70)
            print(f"âœ… çˆ¬å–å®Œæˆ! å…± {len(all_items)} ä¸ªç›®å½•é¡¹")
            for catalog, count in summary['by_catalog'].items():
                print(f"   {catalog}: {count} é¡¹")
            print("=" * 70)

            return all_items

        except Exception as e:
            print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
