"""
ModelScope Learn Section Crawler

çˆ¬å–é­”æ­ç ”ä¹ ç¤¾: https://modelscope.cn/learn
"""

from typing import List, Dict, Optional
from urllib.parse import urljoin
from .base_crawler import BaseCrawler


class LearnCrawler(BaseCrawler):
    """é­”æ­ç ”ä¹ ç¤¾çˆ¬è™«"""

    def __init__(self, output_dir: str = "data/crawled/learn", rate_limit: float = 1.5):
        super().__init__(output_dir, rate_limit)
        self.base_url = "https://modelscope.cn/learn"
        self.visited_articles = set()

    def extract_article_links(self, soup) -> List[str]:
        """
        æå–æ–‡ç« é“¾æ¥

        Args:
            soup: BeautifulSoupå¯¹è±¡

        Returns:
            æ–‡ç« é“¾æ¥åˆ—è¡¨
        """
        links = []

        # æŸ¥æ‰¾æ–‡ç« é“¾æ¥ (æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´é€‰æ‹©å™¨)
        for a in soup.find_all('a', href=True):
            href = a['href']
            if '/learn/' in href or '/article/' in href:
                full_url = urljoin(self.base_url, href)
                if full_url not in self.visited_articles:
                    links.append(full_url)

        return list(set(links))

    def extract_article_content(self, soup, url: str) -> Optional[Dict]:
        """
        æå–æ–‡ç« å†…å®¹

        Args:
            soup: BeautifulSoupå¯¹è±¡
            url: æ–‡ç« URL

        Returns:
            æ–‡ç« æ•°æ®å­—å…¸
        """
        try:
            # æå–æ ‡é¢˜
            title = None
            title_elem = soup.find('h1') or soup.find('title')
            if title_elem:
                title = title_elem.get_text(strip=True)

            # æå–ä½œè€…
            author = None
            author_elem = soup.find(class_='author') or soup.find('meta', attrs={'name': 'author'})
            if author_elem:
                author = author_elem.get('content') if author_elem.name == 'meta' else author_elem.get_text(strip=True)

            # æå–å‘å¸ƒæ—¥æœŸ
            date = None
            date_elem = soup.find(class_='date') or soup.find('time')
            if date_elem:
                date = date_elem.get('datetime') or date_elem.get_text(strip=True)

            # æå–æ­£æ–‡å†…å®¹
            content_elem = (
                soup.find('article') or
                soup.find(class_='content') or
                soup.find(class_='article-body') or
                soup.find('main')
            )

            if not content_elem:
                print(f"âš ï¸  æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸ: {url}")
                return None

            # æå–æ®µè½
            paragraphs = []
            for elem in content_elem.find_all(['p', 'h2', 'h3', 'h4', 'li']):
                text = elem.get_text(strip=True)
                if text and len(text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
                    paragraphs.append(text)

            content = '\n\n'.join(paragraphs)

            if not content or len(content) < 100:
                print(f"âš ï¸  å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º: {url}")
                return None

            # æå–ä»£ç å—
            code_blocks = []
            for code in content_elem.find_all(['pre', 'code']):
                code_text = code.get_text(strip=True)
                if code_text and len(code_text) > 20:
                    code_blocks.append(code_text)

            # æå–æ ‡ç­¾
            tags = []
            for tag_elem in soup.find_all(class_='tag'):
                tag = tag_elem.get_text(strip=True)
                if tag:
                    tags.append(tag)

            return {
                'url': url,
                'title': title or 'Untitled',
                'author': author,
                'date': date,
                'content': content,
                'code_blocks': code_blocks,
                'tags': tags,
                'source': 'modelscope_learn',
                'type': 'article'
            }

        except Exception as e:
            print(f"âŒ è§£ææ–‡ç« å¤±è´¥: {url}")
            print(f"   é”™è¯¯: {e}")
            return None

    def crawl(self) -> List[Dict]:
        """
        æ‰§è¡Œçˆ¬å–

        Returns:
            çˆ¬å–çš„æ–‡ç« åˆ—è¡¨
        """
        print("=" * 70)
        print("å¼€å§‹çˆ¬å–é­”æ­ç ”ä¹ ç¤¾")
        print(f"èµ·å§‹URL: {self.base_url}")
        print("=" * 70)

        articles = []

        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = self.load_checkpoint()
        if checkpoint.get('visited_articles'):
            self.visited_articles = set(checkpoint['visited_articles'])
            print(f"ğŸ“Œ ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œå·²çˆ¬å– {len(self.visited_articles)} ç¯‡æ–‡ç« ")

        try:
            # è·å–ä¸»é¡µ
            print(f"ğŸ“„ è·å–ç ”ä¹ ç¤¾ä¸»é¡µ...")
            html = self.fetch_page(self.base_url)
            if not html:
                print("âŒ æ— æ³•è·å–ç ”ä¹ ç¤¾ä¸»é¡µ")
                return []

            soup = self.parse_html(html)

            # æå–æ–‡ç« é“¾æ¥
            article_links = self.extract_article_links(soup)
            print(f"   å‘ç° {len(article_links)} ç¯‡æ–‡ç« ")

            # çˆ¬å–æ¯ç¯‡æ–‡ç« 
            for i, url in enumerate(article_links, 1):
                if url in self.visited_articles:
                    continue

                print(f"\nğŸ“„ [{i}/{len(article_links)}] çˆ¬å–æ–‡ç« : {url}")

                html = self.fetch_page(url)
                if not html:
                    continue

                soup = self.parse_html(html)
                article_data = self.extract_article_content(soup, url)

                if article_data:
                    articles.append(article_data)
                    # ä¿å­˜å•ç¯‡æ–‡ç« 
                    filename = f"article_{len(articles)}.json"
                    self.save_json(article_data, filename)
                    self.visited_articles.add(url)

                # æ¯10ç¯‡ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
                if len(articles) % 10 == 0:
                    self.save_checkpoint({'visited_articles': list(self.visited_articles)})

            # ä¿å­˜æ±‡æ€»
            summary = {
                'total_articles': len(articles),
                'visited_articles': list(self.visited_articles),
                'metadata': self.get_metadata()
            }
            self.save_json(summary, 'summary.json')

            print("\n" + "=" * 70)
            print(f"âœ… çˆ¬å–å®Œæˆ! å…± {len(articles)} ç¯‡æ–‡ç« ")
            print("=" * 70)

            return articles

        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜æ£€æŸ¥ç‚¹...")
            self.save_checkpoint({'visited_articles': list(self.visited_articles)})
            raise

        except Exception as e:
            print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
            self.save_checkpoint({'visited_articles': list(self.visited_articles)})
            raise
