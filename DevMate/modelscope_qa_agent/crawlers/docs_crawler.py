"""
ModelScope Documentation Crawler

çˆ¬å–é­”æ­ç¤¾åŒºå®˜æ–¹æ–‡æ¡£: https://www.modelscope.cn/docs/overview
"""

from typing import List, Dict, Set, Optional
from urllib.parse import urljoin, urlparse
from .base_crawler import BaseCrawler


class DocsCrawler(BaseCrawler):
    """é­”æ­æ–‡æ¡£çˆ¬è™«"""

    def __init__(self, output_dir: str = "data/crawled/docs", rate_limit: float = 1.5):
        super().__init__(output_dir, rate_limit)
        self.base_url = "https://www.modelscope.cn/docs/"
        self.start_url = "https://www.modelscope.cn/docs/overview"
        self.visited_urls: Set[str] = set()
        self.max_depth = 3

    def is_valid_doc_url(self, url: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ–‡æ¡£URL

        Args:
            url: URL

        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        parsed = urlparse(url)
        return (
            parsed.netloc == "www.modelscope.cn" and
            parsed.path.startswith("/docs/") and
            url not in self.visited_urls
        )

    def extract_doc_links(self, soup, current_url: str) -> List[str]:
        """
        æå–æ–‡æ¡£é“¾æ¥

        Args:
            soup: BeautifulSoupå¯¹è±¡
            current_url: å½“å‰URL

        Returns:
            é“¾æ¥åˆ—è¡¨
        """
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            full_url = urljoin(current_url, href)

            # ç§»é™¤URLä¸­çš„fragment (#éƒ¨åˆ†)
            full_url = full_url.split('#')[0]

            if self.is_valid_doc_url(full_url):
                links.append(full_url)

        return list(set(links))  # å»é‡

    def extract_doc_content(self, soup, url: str) -> Optional[Dict]:
        """
        æå–æ–‡æ¡£å†…å®¹

        Args:
            soup: BeautifulSoupå¯¹è±¡
            url: æ–‡æ¡£URL

        Returns:
            æ–‡æ¡£æ•°æ®å­—å…¸
        """
        try:
            # æŸ¥æ‰¾æ–‡æ¡£æ ‡é¢˜
            title = None
            title_elem = soup.find('h1')
            if title_elem:
                title = title_elem.get_text(strip=True)

            # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
            content_elem = soup.find('article') or soup.find('main') or soup.find('div', class_='content')

            if not content_elem:
                print(f"âš ï¸  æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸ: {url}")
                return None

            # æå–æ–‡æœ¬å†…å®¹
            paragraphs = []
            for elem in content_elem.find_all(['p', 'h2', 'h3', 'h4', 'pre', 'code', 'li']):
                text = elem.get_text(strip=True)
                if text:
                    paragraphs.append(text)

            content = '\n\n'.join(paragraphs)

            if not content:
                print(f"âš ï¸  å†…å®¹ä¸ºç©º: {url}")
                return None

            # æå–ä»£ç å—
            code_blocks = []
            for code in content_elem.find_all('pre'):
                code_text = code.get_text(strip=True)
                if code_text:
                    code_blocks.append(code_text)

            return {
                'url': url,
                'title': title or 'Untitled',
                'content': content,
                'code_blocks': code_blocks,
                'source': 'modelscope_docs',
                'type': 'documentation'
            }

        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {url}")
            print(f"   é”™è¯¯: {e}")
            return None

    def crawl_page(self, url: str, depth: int = 0) -> List[Dict]:
        """
        é€’å½’çˆ¬å–é¡µé¢

        Args:
            url: é¡µé¢URL
            depth: å½“å‰æ·±åº¦

        Returns:
            çˆ¬å–çš„æ–‡æ¡£åˆ—è¡¨
        """
        if depth > self.max_depth or url in self.visited_urls:
            return []

        self.visited_urls.add(url)
        print(f"ğŸ“„ çˆ¬å– (æ·±åº¦={depth}): {url}")

        html = self.fetch_page(url)
        if not html:
            return []

        soup = self.parse_html(html)
        documents = []

        # æå–å½“å‰é¡µé¢å†…å®¹
        doc_data = self.extract_doc_content(soup, url)
        if doc_data:
            documents.append(doc_data)
            # ä¿å­˜å•ä¸ªæ–‡æ¡£
            filename = f"doc_{len(self.visited_urls)}.json"
            self.save_json(doc_data, filename)

        # é€’å½’çˆ¬å–é“¾æ¥
        if depth < self.max_depth:
            links = self.extract_doc_links(soup, url)
            print(f"   å‘ç° {len(links)} ä¸ªé“¾æ¥")

            for link in links:
                documents.extend(self.crawl_page(link, depth + 1))

        return documents

    def crawl(self) -> List[Dict]:
        """
        æ‰§è¡Œçˆ¬å–

        Returns:
            çˆ¬å–çš„æ–‡æ¡£åˆ—è¡¨
        """
        print("=" * 70)
        print("å¼€å§‹çˆ¬å–é­”æ­ç¤¾åŒºå®˜æ–¹æ–‡æ¡£")
        print(f"èµ·å§‹URL: {self.start_url}")
        print(f"æœ€å¤§æ·±åº¦: {self.max_depth}")
        print("=" * 70)

        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = self.load_checkpoint()
        if checkpoint.get('visited_urls'):
            self.visited_urls = set(checkpoint['visited_urls'])
            print(f"ğŸ“Œ ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œå·²çˆ¬å– {len(self.visited_urls)} ä¸ªé¡µé¢")

        try:
            documents = self.crawl_page(self.start_url)

            # ä¿å­˜æ±‡æ€»æ•°æ®
            summary = {
                'total_documents': len(documents),
                'visited_urls': list(self.visited_urls),
                'metadata': self.get_metadata()
            }
            self.save_json(summary, 'summary.json')

            print("\n" + "=" * 70)
            print(f"âœ… çˆ¬å–å®Œæˆ! å…± {len(documents)} ä¸ªæ–‡æ¡£")
            print(f"   è®¿é—®äº† {len(self.visited_urls)} ä¸ªURL")
            print("=" * 70)

            return documents

        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜æ£€æŸ¥ç‚¹...")
            self.save_checkpoint({'visited_urls': list(self.visited_urls)})
            raise

        except Exception as e:
            print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
            self.save_checkpoint({'visited_urls': list(self.visited_urls)})
            raise
