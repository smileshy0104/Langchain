"""
é­”æ­ç¤¾åŒºé—®ç­” Agent

å®ç°åŸºäº LangGraph çš„æŠ€æœ¯é—®ç­”æ™ºèƒ½ä»£ç†,åŒ…å«æ–‡æ¡£æ£€ç´¢ã€ç­”æ¡ˆç”Ÿæˆå’ŒéªŒè¯èŠ‚ç‚¹ã€‚

æ ¸å¿ƒåŠŸèƒ½:
- æ–‡æ¡£æ£€ç´¢: ä½¿ç”¨æ··åˆæ£€ç´¢å™¨è·å–ç›¸å…³æ–‡æ¡£
- ç­”æ¡ˆç”Ÿæˆ: åŸºäº LLM ç”Ÿæˆç»“æ„åŒ–æŠ€æœ¯å›ç­”
- ç­”æ¡ˆéªŒè¯: Self-RAG éªŒè¯ç­”æ¡ˆè´¨é‡(å¯é€‰)
- å¯¹è¯æŒä¹…åŒ–: MemorySaver æ”¯æŒå¤šè½®å¯¹è¯
"""

from typing import Optional, Dict, Any
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_community.chat_models import ChatTongyi
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

from models.schemas import ConversationState, TechnicalAnswer
from retrievers.hybrid_retriever import HybridRetriever
from tools.clarification_tool import ClarificationTool
from core.memory_manager import MemoryManager


class ModelScopeQAAgent:
    """é­”æ­ç¤¾åŒºé—®ç­” Agent

    åŸºäº LangGraph å®ç°çš„æŠ€æœ¯é—®ç­”ä»£ç†,æ”¯æŒ:
    - RAG (Retrieval-Augmented Generation) æ¶æ„
    - å¤šè½®å¯¹è¯çŠ¶æ€ç®¡ç†
    - ç­”æ¡ˆè´¨é‡éªŒè¯
    - æ£€æŸ¥ç‚¹æŒä¹…åŒ–

    Attributes:
        retriever: HybridRetriever æ··åˆæ£€ç´¢å™¨å®ä¾‹
        llm: ChatTongyi LLM å®¢æˆ·ç«¯
        clarification_tool: ClarificationTool æ¾„æ¸…é—®é¢˜å·¥å…·
        workflow: StateGraph LangGraph å·¥ä½œæµ
        checkpointer: MemorySaver æ£€æŸ¥ç‚¹æŒä¹…åŒ–å™¨
        app: CompiledGraph ç¼–è¯‘åçš„å·¥ä½œæµåº”ç”¨

    Example:
        >>> from core.vector_store import VectorStoreManager
        >>> from retrievers.hybrid_retriever import HybridRetriever
        >>>
        >>> # åˆå§‹åŒ–æ£€ç´¢å™¨
        >>> manager = VectorStoreManager()
        >>> vector_store = manager.get_vector_store()
        >>> retriever = HybridRetriever(vector_store, documents)
        >>>
        >>> # åˆ›å»º Agent
        >>> agent = ModelScopeQAAgent(retriever, api_key="your-api-key")
        >>>
        >>> # å•è½®é—®ç­”
        >>> answer = agent.invoke("å¦‚ä½•ä½¿ç”¨ Qwen æ¨¡å‹?")
        >>> print(answer["summary"])
    """

    def __init__(
        self,
        retriever: HybridRetriever,
        llm_api_key: str,
        model: str = "qwen-plus",
        temperature: float = 0.3,
        top_p: float = 0.8
    ):
        """åˆå§‹åŒ–é­”æ­ç¤¾åŒºé—®ç­” Agent

        Args:
            retriever: HybridRetriever å®ä¾‹,ç”¨äºæ–‡æ¡£æ£€ç´¢
            llm_api_key: é€šä¹‰åƒé—® API å¯†é’¥
            model: æ¨¡å‹åç§°(é»˜è®¤ qwen-plus)
            temperature: æ¸©åº¦å‚æ•°,æ§åˆ¶éšæœºæ€§(é»˜è®¤ 0.3)
            top_p: Top-p é‡‡æ ·å‚æ•°(é»˜è®¤ 0.8)

        Raises:
            ValueError: å¦‚æœ retriever ä¸º None æˆ– API å¯†é’¥ä¸ºç©º

        Example:
            >>> agent = ModelScopeQAAgent(
            ...     retriever=my_retriever,
            ...     llm_api_key="sk-xxx",
            ...     temperature=0.2
            ... )
        """
        if retriever is None:
            raise ValueError("retriever ä¸èƒ½ä¸º None")
        if not llm_api_key or not llm_api_key.strip():
            raise ValueError("llm_api_key ä¸èƒ½ä¸ºç©º")

        self.retriever = retriever

        # å­˜å‚¨é…ç½®å‚æ•°(å› ä¸º ChatTongyi ä¸æš´éœ²è¿™äº›å±æ€§)
        self._model = model
        self._temperature = temperature
        self._top_p = top_p

        # åˆå§‹åŒ–é€šä¹‰åƒé—® LLM
        self.llm = ChatTongyi(
            model=model,
            temperature=temperature,
            top_p=top_p,
            dashscope_api_key=llm_api_key
        )

        # åˆå§‹åŒ–æ¾„æ¸…å·¥å…· (Phase 3.6: ä¸»åŠ¨æ¾„æ¸…æœºåˆ¶)
        self.clarification_tool = ClarificationTool(
            llm_api_key=llm_api_key,
            model=model,
            temperature=temperature
        )

        # åˆå§‹åŒ–å¯¹è¯è®°å¿†ç®¡ç†å™¨ (Phase 4.1: å¯¹è¯å†å²ç®¡ç†)
        self.memory_manager = MemoryManager(
            llm=self.llm,
            max_turns=10,  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
            max_tokens=4000
        )

        # æ„å»º LangGraph å·¥ä½œæµ
        self.workflow = StateGraph(ConversationState)
        self._build_graph()

        # æ·»åŠ æ£€æŸ¥ç‚¹å™¨æ”¯æŒå¯¹è¯æŒä¹…åŒ–
        self.checkpointer = MemorySaver()
        self.app = self.workflow.compile(checkpointer=self.checkpointer)

        print(f"âœ… ModelScopeQAAgent åˆå§‹åŒ–æˆåŠŸ")
        print(f"   - LLM æ¨¡å‹: {model}")
        print(f"   - æ¸©åº¦: {temperature}")
        print(f"   - Top-P: {top_p}")
        print(f"   - æ£€ç´¢å™¨: {type(retriever).__name__}")

    def _build_graph(self):
        """æ„å»º LangGraph å·¥ä½œæµ

        å·¥ä½œæµèŠ‚ç‚¹:
        1. clarify: æ¾„æ¸…é—®é¢˜èŠ‚ç‚¹ (Phase 3.6)
        2. retrieve: æ–‡æ¡£æ£€ç´¢èŠ‚ç‚¹
        3. generate: ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹
        4. validate: ç­”æ¡ˆéªŒè¯èŠ‚ç‚¹(å¯é€‰)

        å·¥ä½œæµ:
        START â†’ clarify â†’ [æ¡ä»¶åˆ†æ”¯]
                           â”œâ”€> END (éœ€è¦æ¾„æ¸…, è¿”å›æ¾„æ¸…é—®é¢˜)
                           â””â”€> retrieve â†’ generate â†’ [æ¡ä»¶åˆ†æ”¯]
                                                       â”œâ”€> validate â†’ END (ç½®ä¿¡åº¦ < 0.8)
                                                       â””â”€> END (ç½®ä¿¡åº¦ â‰¥ 0.8)
        """
        # æ·»åŠ èŠ‚ç‚¹
        self.workflow.add_node("clarify", self._clarify_question)
        self.workflow.add_node("retrieve", self._retrieve_documents)
        self.workflow.add_node("generate", self._generate_answer)
        self.workflow.add_node("validate", self._validate_answer)

        # è®¾ç½®å…¥å£ç‚¹: ä»æ¾„æ¸…èŠ‚ç‚¹å¼€å§‹
        self.workflow.set_entry_point("clarify")

        # æ¡ä»¶åˆ†æ”¯1: æ¾„æ¸…åå†³å®šæ˜¯ç»§ç»­è¿˜æ˜¯è¿”å›æ¾„æ¸…é—®é¢˜
        self.workflow.add_conditional_edges(
            "clarify",
            self._should_retrieve_or_clarify,
            {
                "retrieve": "retrieve",  # ä¸éœ€è¦æ¾„æ¸…,ç»§ç»­æ£€ç´¢
                "end": END  # éœ€è¦æ¾„æ¸…,è¿”å›æ¾„æ¸…é—®é¢˜
            }
        )

        # æ·»åŠ è¾¹: retrieve â†’ generate
        self.workflow.add_edge("retrieve", "generate")

        # æ¡ä»¶åˆ†æ”¯2: æ ¹æ®ç½®ä¿¡åº¦å†³å®šæ˜¯å¦éªŒè¯
        self.workflow.add_conditional_edges(
            "generate",
            self._should_validate,
            {
                "validate": "validate",
                "end": END
            }
        )

        self.workflow.add_edge("validate", END)

        print("âœ… LangGraph å·¥ä½œæµæ„å»ºå®Œæˆ")
        print("   èŠ‚ç‚¹: clarify â†’ [retrieve â†’ generate â†’ validate]")

    def _clarify_question(self, state: ConversationState) -> ConversationState:
        """æ¾„æ¸…é—®é¢˜èŠ‚ç‚¹ (Phase 3.6: ä¸»åŠ¨æ¾„æ¸…æœºåˆ¶)

        æ£€æµ‹ç”¨æˆ·é—®é¢˜æ˜¯å¦ç¼ºå¤±å…³é”®ä¿¡æ¯,å¦‚æœéœ€è¦åˆ™ç”Ÿæˆæ¾„æ¸…é—®é¢˜ã€‚

        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€

        Returns:
            ConversationState: æ›´æ–°åçš„çŠ¶æ€,åŒ…å«æ¾„æ¸…æ£€æŸ¥ç»“æœ

        Updates:
            - needs_clarification: æ˜¯å¦éœ€è¦æ¾„æ¸…
            - clarification_questions: æ¾„æ¸…é—®é¢˜åˆ—è¡¨
        """
        # è·å–ç”¨æˆ·é—®é¢˜
        question = state["messages"][-1].content

        # ä½¿ç”¨æ¾„æ¸…å·¥å…·æ£€æŸ¥é—®é¢˜
        try:
            clarification_result = self.clarification_tool.check_and_clarify(question)

            # æ›´æ–°çŠ¶æ€
            state["needs_clarification"] = clarification_result.needs_clarification
            state["clarification_questions"] = clarification_result.clarification_questions

            if clarification_result.needs_clarification:
                print(f"â“ éœ€è¦æ¾„æ¸…, ç”Ÿæˆäº† {len(clarification_result.clarification_questions)} ä¸ªé—®é¢˜")
            else:
                print(f"âœ… é—®é¢˜ä¿¡æ¯å……åˆ†, æ— éœ€æ¾„æ¸…")

        except Exception as e:
            print(f"âš ï¸  æ¾„æ¸…æ£€æŸ¥å¤±è´¥: {e}")
            # é™çº§: å‡è®¾ä¸éœ€è¦æ¾„æ¸…,ç»§ç»­å¤„ç†
            state["needs_clarification"] = False
            state["clarification_questions"] = []

        return state

    def _should_retrieve_or_clarify(self, state: ConversationState) -> str:
        """æ¡ä»¶åˆ†æ”¯: åˆ¤æ–­æ˜¯ç»§ç»­æ£€ç´¢è¿˜æ˜¯è¿”å›æ¾„æ¸…é—®é¢˜

        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€

        Returns:
            str: "retrieve" æˆ– "end"

        é€»è¾‘:
            - å¦‚æœéœ€è¦æ¾„æ¸…: è¿”å› "end" (ç»“æŸæµç¨‹,è¿”å›æ¾„æ¸…é—®é¢˜)
            - å¦‚æœä¸éœ€è¦æ¾„æ¸…: è¿”å› "retrieve" (ç»§ç»­æ£€ç´¢æµç¨‹)
        """
        if state["needs_clarification"]:
            print(f"ğŸ”€ éœ€è¦æ¾„æ¸…, ç»ˆæ­¢æ£€ç´¢æµç¨‹")
            return "end"
        else:
            print(f"ğŸ”€ æ— éœ€æ¾„æ¸…, ç»§ç»­æ£€ç´¢æµç¨‹")
            return "retrieve"

    def _retrieve_documents(self, state: ConversationState) -> ConversationState:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£èŠ‚ç‚¹

        ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–é—®é¢˜,ä½¿ç”¨æ··åˆæ£€ç´¢å™¨è·å–ç›¸å…³æ–‡æ¡£ã€‚

        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€

        Returns:
            ConversationState: æ›´æ–°åçš„çŠ¶æ€,åŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£

        Updates:
            - current_question: å½“å‰ç”¨æˆ·é—®é¢˜
            - retrieved_documents: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        # è·å–æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºé—®é¢˜
        question = state["messages"][-1].content

        # æ‰§è¡Œæ··åˆæ£€ç´¢
        try:
            docs = self.retriever.retrieve(question, k=3)
            print(f"ğŸ“¥ æ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        except Exception as e:
            print(f"âš ï¸  æ£€ç´¢å¤±è´¥: {e}")
            docs = []

        # æ›´æ–°çŠ¶æ€
        state["current_question"] = question
        state["retrieved_documents"] = docs

        return state

    def _generate_answer(self, state: ConversationState) -> ConversationState:
        """ç”ŸæˆæŠ€æœ¯å›ç­”èŠ‚ç‚¹

        åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ã€ç”¨æˆ·é—®é¢˜å’Œå¯¹è¯å†å²,ä½¿ç”¨ LLM ç”Ÿæˆç»“æ„åŒ–æŠ€æœ¯å›ç­”ã€‚

        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€

        Returns:
            ConversationState: æ›´æ–°åçš„çŠ¶æ€,åŒ…å«ç”Ÿæˆçš„ç­”æ¡ˆ

        Updates:
            - generated_answer: ç”Ÿæˆçš„ TechnicalAnswer å­—å…¸

        Prompt ç»“æ„:
            - System: å®šä¹‰è§’è‰²ã€ä»»åŠ¡å’Œè¾“å‡ºæ ¼å¼
            - Conversation History: å¯¹è¯å†å²æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
            - Context: æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
            - Human: å½“å‰ç”¨æˆ·é—®é¢˜
        """
        # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"æ–‡æ¡£ {i+1}:\n{doc.page_content}"
            for i, doc in enumerate(state["retrieved_documents"])
        ])

        if not context.strip():
            context = "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ã€‚"

        # æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆT107: æ”¯æŒå¯¹è¯å†å²å¼•ç”¨ï¼‰
        conversation_history = self._build_conversation_history(state)

        # ç³»ç»Ÿæç¤ºè¯ï¼ˆT108: æ·»åŠ å¯¹è¯å†å²å ä½ç¬¦ï¼‰
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯é­”æ­ç¤¾åŒºçš„æŠ€æœ¯æ”¯æŒä¸“å®¶ã€‚

**ä»»åŠ¡**: åŸºäºæä¾›çš„æ–‡æ¡£ä¸Šä¸‹æ–‡å’Œå¯¹è¯å†å²,å›ç­”ç”¨æˆ·çš„æŠ€æœ¯é—®é¢˜ã€‚

**è¦æ±‚**:
1. å›ç­”å¿…é¡»åŸºäºæ–‡æ¡£å†…å®¹,ä¸å¾—ç¼–é€ 
2. å¦‚æœç”¨æˆ·é—®é¢˜å¼•ç”¨äº†ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼ˆå¦‚"åˆšæ‰ä½ å»ºè®®çš„æ–¹æ³•"ã€"ä½ åˆšè¯´çš„"ã€"ä¹‹å‰æåˆ°çš„"),è¦å‡†ç¡®ç†è§£ä»£è¯æŒ‡å‘,ç»“åˆå¯¹è¯å†å²ç»™å‡ºå›ç­”
3. æä¾›è‡³å°‘1ç§å¯æ‰§è¡Œçš„è§£å†³æ–¹æ¡ˆ
4. åŒ…å«å®Œæ•´çš„ä»£ç ç¤ºä¾‹ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
5. å¼•ç”¨ä¿¡æ¯æ¥æº
6. å¦‚æœæ–‡æ¡£ä¸è¶³ä»¥å›ç­”é—®é¢˜,æ˜ç¡®è¯´æ˜

{conversation_history_section}

**ä¸Šä¸‹æ–‡æ–‡æ¡£**:
{context}

**è¾“å‡ºæ ¼å¼**: è¯·ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼:
{format_instructions}
"""),
            ("human", "{question}")
        ])

        # é…ç½® Pydantic è¾“å‡ºè§£æå™¨
        parser = PydanticOutputParser(pydantic_object=TechnicalAnswer)

        # æ„å»ºç”Ÿæˆé“¾
        chain = prompt | self.llm | parser

        try:
            # ç”Ÿæˆç­”æ¡ˆï¼ˆåŒ…å«å¯¹è¯å†å²ï¼‰
            answer = chain.invoke({
                "context": context,
                "conversation_history_section": conversation_history,
                "question": state["current_question"],
                "format_instructions": parser.get_format_instructions()
            })

            print(f"âœ… ç­”æ¡ˆç”ŸæˆæˆåŠŸ")
            print(f"   - è§£å†³æ–¹æ¡ˆæ•°: {len(answer.solutions)}")
            print(f"   - ç½®ä¿¡åº¦: {answer.confidence_score}")

            # æ›´æ–°çŠ¶æ€
            state["generated_answer"] = answer.model_dump()

        except Exception as e:
            print(f"âš ï¸  ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            # é™çº§: è¿”å›åŸºæœ¬å›ç­”
            fallback_answer = TechnicalAnswer(
                summary=f"æŠ±æ­‰,ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {str(e)}",
                problem_analysis="ç­”æ¡ˆç”Ÿæˆå¤±è´¥",
                solutions=["è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ"],
                code_examples=[],
                references=[],
                confidence_score=0.0
            )
            state["generated_answer"] = fallback_answer.model_dump()

        return state

    def _validate_answer(self, state: ConversationState) -> ConversationState:
        """éªŒè¯ç­”æ¡ˆè´¨é‡èŠ‚ç‚¹

        å¯¹ç”Ÿæˆçš„ç­”æ¡ˆè¿›è¡Œè´¨é‡éªŒè¯(Self-RAG)ã€‚
        å½“ç½®ä¿¡åº¦è¾ƒä½æ—¶,å¯ä»¥é‡æ–°æ£€ç´¢æˆ–ä¼˜åŒ–ç­”æ¡ˆã€‚

        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€

        Returns:
            ConversationState: æ›´æ–°åçš„çŠ¶æ€

        TODO: å®ç° Self-RAG éªŒè¯é€»è¾‘
            - æ£€æŸ¥ç­”æ¡ˆä¸æ–‡æ¡£çš„ç›¸å…³æ€§
            - éªŒè¯å¼•ç”¨æ¥æºçš„å‡†ç¡®æ€§
            - è¯„ä¼°è§£å†³æ–¹æ¡ˆçš„å¯è¡Œæ€§
        """
        # TODO: å®ç°ç­”æ¡ˆéªŒè¯é€»è¾‘
        # ç›®å‰ä»…ä½œä¸ºå ä½èŠ‚ç‚¹
        print("ğŸ” æ‰§è¡Œç­”æ¡ˆéªŒè¯...")

        # æœªæ¥å¯ä»¥æ·»åŠ :
        # 1. ç›¸å…³æ€§è¯„åˆ†
        # 2. å¼•ç”¨éªŒè¯
        # 3. ä»£ç å¯æ‰§è¡Œæ€§æ£€æŸ¥
        # 4. å¦‚æœéªŒè¯å¤±è´¥,è§¦å‘é‡æ–°æ£€ç´¢

        return state

    def _should_validate(self, state: ConversationState) -> str:
        """æ¡ä»¶åˆ†æ”¯: åˆ¤æ–­æ˜¯å¦éœ€è¦éªŒè¯ç­”æ¡ˆ

        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€

        Returns:
            str: "validate" æˆ– "end"

        é€»è¾‘:
            - ç½®ä¿¡åº¦ < 0.8: éœ€è¦éªŒè¯
            - ç½®ä¿¡åº¦ â‰¥ 0.8: ç›´æ¥ç»“æŸ
        """
        confidence = state["generated_answer"].get("confidence_score", 0.0)

        if confidence < 0.8:
            print(f"âš ï¸  ç½®ä¿¡åº¦è¾ƒä½ ({confidence:.2f}), æ‰§è¡ŒéªŒè¯")
            return "validate"
        else:
            print(f"âœ… ç½®ä¿¡åº¦è¾ƒé«˜ ({confidence:.2f}), ç›´æ¥è¿”å›")
            return "end"

    def _build_conversation_history(self, state: ConversationState) -> str:
        """æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡ï¼ˆT107: æ”¯æŒå¯¹è¯å†å²å¼•ç”¨ï¼‰

        ä½¿ç”¨ MemoryManager è·å–ä¼˜åŒ–çš„å¯¹è¯çª—å£ï¼ŒåŒ…å«æ—©æœŸå¯¹è¯æ‘˜è¦å’Œæœ€è¿‘å¯¹è¯ã€‚

        Args:
            state: å½“å‰å¯¹è¯çŠ¶æ€

        Returns:
            str: æ ¼å¼åŒ–çš„å¯¹è¯å†å²æ–‡æœ¬ï¼Œå¦‚æœæ˜¯é¦–è½®å¯¹è¯åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²

        å¤„ç†æµç¨‹:
            1. æ£€æŸ¥æ˜¯å¦æœ‰å¯¹è¯å†å²ï¼ˆæ¶ˆæ¯æ•° > 2: System + å½“å‰é—®é¢˜ï¼‰
            2. å¦‚æœéœ€è¦æ‘˜è¦ï¼Œç”Ÿæˆæ—©æœŸå¯¹è¯æ‘˜è¦
            3. ä½¿ç”¨ MemoryManager è·å–ä¼˜åŒ–çš„å¯¹è¯çª—å£
            4. æ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬

        Example Output:
            **å¯¹è¯å†å²**:
            æ—©æœŸå¯¹è¯æ‘˜è¦: ç”¨æˆ·è¯¢é—®äº†æ¨¡å‹åŠ è½½é—®é¢˜...

            ç”¨æˆ·: å¦‚ä½•åŠ è½½ Qwen-7B æ¨¡å‹?
            Agent: ä½¿ç”¨ AutoModelForCausalLM.from_pretrained()...
            ç”¨æˆ·: CUDA å†…å­˜ä¸è¶³æ€ä¹ˆåŠ?
            Agent: å¯ä»¥é™ä½ batch_size...
        """
        messages = state.get("messages", [])

        # å¦‚æœåªæœ‰å½“å‰é—®é¢˜ï¼ˆæ²¡æœ‰å†å²å¯¹è¯ï¼‰ï¼Œè¿”å›ç©º
        # é€šå¸¸ç¬¬ä¸€è½®å¯¹è¯ä¼šæœ‰: SystemMessage (if any) + HumanMessage
        if len(messages) <= 2:
            return ""

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
        conversation_summary = state.get("conversation_summary")
        if self.memory_manager.should_generate_summary(messages):
            # è·å–éœ€è¦æ‘˜è¦çš„æ—©æœŸæ¶ˆæ¯
            early_messages = self.memory_manager.get_early_messages(messages)
            if early_messages:
                # ç”Ÿæˆæˆ–æ›´æ–°æ‘˜è¦
                conversation_summary = self.memory_manager.summarize_early_messages(
                    early_messages,
                    current_summary=conversation_summary
                )
                # æ›´æ–°çŠ¶æ€ä¸­çš„æ‘˜è¦ï¼ˆä¾›ä¸‹æ¬¡ä½¿ç”¨ï¼‰
                state["conversation_summary"] = conversation_summary

        # è·å–ä¼˜åŒ–çš„å¯¹è¯çª—å£ï¼ˆåŒ…å«æ‘˜è¦ + æœ€è¿‘å¯¹è¯ï¼‰
        conversation_window = self.memory_manager.get_conversation_window(
            messages[:-1],  # æ’é™¤å½“å‰é—®é¢˜ï¼ˆæœ€åä¸€æ¡æ¶ˆæ¯ï¼‰
            summary=conversation_summary
        )

        # æ ¼å¼åŒ–å¯¹è¯å†å²
        if not conversation_window:
            return ""

        history_lines = ["**å¯¹è¯å†å²**:"]

        for msg in conversation_window:
            if hasattr(msg, '__class__') and msg.__class__.__name__ == 'SystemMessage':
                # ç³»ç»Ÿæ¶ˆæ¯ï¼ˆåŒ…æ‹¬æ‘˜è¦ï¼‰
                if "æ—©æœŸå¯¹è¯æ‘˜è¦" in msg.content:
                    history_lines.append(msg.content)
                # è·³è¿‡å…¶ä»–ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚åˆå§‹çš„ System Promptï¼‰
            elif hasattr(msg, '__class__') and msg.__class__.__name__ == 'HumanMessage':
                history_lines.append(f"ç”¨æˆ·: {msg.content}")
            elif hasattr(msg, '__class__') and msg.__class__.__name__ == 'AIMessage':
                # ç®€åŒ– AI å›ç­”ï¼ˆåªæ˜¾ç¤ºæ‘˜è¦ï¼Œä¸æ˜¾ç¤ºå®Œæ•´çš„ç»“æ„åŒ–è¾“å‡ºï¼‰
                content = msg.content
                if len(content) > 200:
                    content = content[:200] + "..."
                history_lines.append(f"Agent: {content}")

        return "\n".join(history_lines) if len(history_lines) > 1 else ""

    def invoke(
        self,
        question: str,
        thread_id: str = "default"
    ) -> Dict[str, Any]:
        """è°ƒç”¨ Agent è¿›è¡Œå•è½®é—®ç­”

        Args:
            question: ç”¨æˆ·é—®é¢˜
            thread_id: çº¿ç¨‹ID,ç”¨äºå¤šè½®å¯¹è¯ç®¡ç†(é»˜è®¤ "default")

        Returns:
            Dict[str, Any]: TechnicalAnswer å­—å…¸,åŒ…å«:
                - summary: ç­”æ¡ˆæ‘˜è¦
                - problem_analysis: é—®é¢˜åˆ†æ
                - solutions: è§£å†³æ–¹æ¡ˆåˆ—è¡¨
                - code_examples: ä»£ç ç¤ºä¾‹
                - references: å¼•ç”¨æ¥æº
                - confidence_score: ç½®ä¿¡åº¦è¯„åˆ†

        Raises:
            Exception: å·¥ä½œæµæ‰§è¡Œå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸

        Example:
            >>> answer = agent.invoke("å¦‚ä½•ä½¿ç”¨ Qwen æ¨¡å‹?")
            >>> print(answer["summary"])
            >>> for solution in answer["solutions"]:
            ...     print(f"- {solution}")
        """
        if not question or not question.strip():
            raise ValueError("é—®é¢˜ä¸èƒ½ä¸ºç©º")

        print(f"\n{'='*70}")
        print(f"ğŸ¤– ModelScopeQAAgent å¤„ç†é—®é¢˜")
        print(f"{'='*70}")
        print(f"é—®é¢˜: {question}")
        print(f"çº¿ç¨‹ID: {thread_id}")
        print(f"{'='*70}\n")

        try:
            # è°ƒç”¨å·¥ä½œæµ
            result = self.app.invoke(
                {
                    "messages": [HumanMessage(content=question)],
                    "current_question": "",
                    "retrieved_documents": [],
                    "generated_answer": {},
                    "needs_clarification": False,  # Phase 3.6: æ¾„æ¸…æ ‡è®°
                    "clarification_questions": [],  # Phase 3.6: æ¾„æ¸…é—®é¢˜åˆ—è¡¨
                    "turn_count": 0
                },
                config={"configurable": {"thread_id": thread_id}}
            )

            print(f"\n{'='*70}")
            print(f"âœ… å¤„ç†å®Œæˆ")
            print(f"{'='*70}\n")

            # å¦‚æœéœ€è¦æ¾„æ¸…,è¿”å›æ¾„æ¸…é—®é¢˜è€Œä¸æ˜¯ç­”æ¡ˆ
            if result["needs_clarification"]:
                print(f"â“ éœ€è¦ç”¨æˆ·æ¾„æ¸…ä¿¡æ¯")
                return {
                    "needs_clarification": True,
                    "clarification_questions": result["clarification_questions"],
                    "summary": "ä¸ºäº†æ›´å¥½åœ°å¸®åŠ©æ‚¨,æˆ‘éœ€è¦äº†è§£ä»¥ä¸‹ä¿¡æ¯:",
                    "problem_analysis": "é—®é¢˜æè¿°ä¸å¤Ÿæ¸…æ™°",
                    "solutions": result["clarification_questions"],
                    "code_examples": [],
                    "references": [],
                    "confidence_score": 0.0
                }
            else:
                return result["generated_answer"]

        except Exception as e:
            print(f"\nâš ï¸  å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            raise

    def get_state(self, thread_id: str = "default") -> Optional[ConversationState]:
        """è·å–æŒ‡å®šçº¿ç¨‹çš„å¯¹è¯çŠ¶æ€

        Args:
            thread_id: çº¿ç¨‹ID

        Returns:
            Optional[ConversationState]: å¯¹è¯çŠ¶æ€,å¦‚æœä¸å­˜åœ¨è¿”å› None

        Example:
            >>> state = agent.get_state("user123")
            >>> if state:
            ...     print(f"å†å²æ¶ˆæ¯æ•°: {len(state['messages'])}")
        """
        try:
            config = {"configurable": {"thread_id": thread_id}}
            snapshot = self.app.get_state(config)
            return snapshot.values if snapshot else None
        except Exception as e:
            print(f"âš ï¸  è·å–çŠ¶æ€å¤±è´¥: {e}")
            return None

    def get_stats(self) -> Dict[str, Any]:
        """è·å– Agent ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯å­—å…¸

        Example:
            >>> stats = agent.get_stats()
            >>> print(f"æ£€ç´¢å™¨: {stats['retriever_type']}")
            >>> print(f"LLM: {stats['llm_model']}")
        """
        return {
            "retriever_type": type(self.retriever).__name__,
            "retriever_stats": self.retriever.get_stats(),
            "llm_model": self._model,
            "llm_temperature": self._temperature,
            "llm_top_p": self._top_p,
            "has_checkpointer": self.checkpointer is not None,
            "workflow_nodes": ["retrieve", "generate", "validate"]
        }
