"""
æ–‡æ¡£å¤„ç†å™¨

è´Ÿè´£æ–‡æ¡£çš„åŠ è½½ã€æ¸…æ´—ã€åˆ†å—å’Œè´¨é‡è¯„åˆ†ã€‚
æ”¯æŒ Markdown è¯­ä¹‰åˆ†å—ã€ä»£ç å—å®Œæ•´æ€§ä¿æŠ¤ã€å¤šæºæ–‡æ¡£åŠ è½½ã€‚
"""

from typing import List, Optional, Dict
import re
from langchain_core.documents import Document
from langchain_text_splitters import (
    MarkdownHeaderTextSplitter,
    RecursiveCharacterTextSplitter
)
from langchain_community.document_loaders import WebBaseLoader
from bs4 import BeautifulSoup
from markdownify import markdownify as md


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨

    ç®¡ç†æ–‡æ¡£åŠ è½½ã€æ¸…æ´—ã€åˆ†å—å’Œè´¨é‡è¯„åˆ†çš„å®Œæ•´æµç¨‹ã€‚

    Features:
        - åŠ è½½å¤šæºæ–‡æ¡£(Webã€GitHubã€æœ¬åœ°æ–‡ä»¶)
        - æ™ºèƒ½æ¸…æ´—HTMLå’Œæ ¼å¼åŒ–é—®é¢˜
        - è¯­ä¹‰åˆ†å—(åŸºäº Markdown æ ‡é¢˜)
        - ä»£ç å—å®Œæ•´æ€§ä¿æŠ¤
        - æ–‡æ¡£è´¨é‡è¯„åˆ†

    Attributes:
        markdown_splitter: Markdown æ ‡é¢˜åˆ†å—å™¨
        text_splitter: é€’å½’å­—ç¬¦åˆ†å—å™¨
        chunk_size: åˆ†å—å¤§å°(é»˜è®¤1000å­—ç¬¦)
        chunk_overlap: åˆ†å—é‡å (é»˜è®¤200å­—ç¬¦)
    """

    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        """åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨

        Args:
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°(å­—ç¬¦æ•°)
            chunk_overlap: åˆ†å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°

        Example:
            >>> processor = DocumentProcessor(chunk_size=800, chunk_overlap=150)
            >>> docs = processor.load_modelscope_docs()
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # Markdown æ ‡é¢˜åˆ†å—å™¨
        self.markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
            ],
            strip_headers=False  # ä¿ç•™æ ‡é¢˜åœ¨å†…å®¹ä¸­
        )

        # é€’å½’å­—ç¬¦åˆ†å—å™¨(ç”¨äºè¿›ä¸€æ­¥æ‹†åˆ†å¤§å—)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "!", "?", ";", ",", " ", ""],
            length_function=len
        )

        print(f"âœ… DocumentProcessor åˆå§‹åŒ–æˆåŠŸ")
        print(f"   - chunk_size: {chunk_size}")
        print(f"   - chunk_overlap: {chunk_overlap}")

    def load_modelscope_docs(
        self,
        urls: Optional[List[str]] = None
    ) -> List[Document]:
        """åŠ è½½é­”æ­ç¤¾åŒºå®˜æ–¹æ–‡æ¡£

        Args:
            urls: è¦åŠ è½½çš„URLåˆ—è¡¨ã€‚å¦‚æœä¸º None,ä½¿ç”¨é»˜è®¤URLåˆ—è¡¨ã€‚

        Returns:
            List[Document]: åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨

        Raises:
            Exception: åŠ è½½å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸

        Example:
            >>> processor = DocumentProcessor()
            >>> docs = processor.load_modelscope_docs()
            >>> print(f"åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£")
        """
        if urls is None:
            # é»˜è®¤é­”æ­ç¤¾åŒºæ–‡æ¡£URLåˆ—è¡¨
            urls = [
                "https://www.modelscope.cn/docs/overview",
                "https://www.modelscope.cn/docs/models",
                "https://www.modelscope.cn/docs/datasets",
                "https://www.modelscope.cn/docs/pipelines",
            ]

        documents = []
        for url in urls:
            try:
                print(f"ğŸ“¥ åŠ è½½æ–‡æ¡£: {url}")
                loader = WebBaseLoader(
                    web_paths=[url],
                    bs_kwargs={
                        "parse_only": BeautifulSoup.SoupStrainer(
                            ["article", "main", "div"]
                        )
                    }
                )
                docs = loader.load()

                # æ·»åŠ å…ƒæ•°æ®
                for doc in docs:
                    doc.metadata["source_url"] = url
                    doc.metadata["source_type"] = "official_docs"
                    doc.metadata["document_type"] = "tutorial"  # å¯æ ¹æ®URLè°ƒæ•´

                documents.extend(docs)
                print(f"   âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£æ®µè½")

            except Exception as e:
                print(f"   âš ï¸  åŠ è½½å¤±è´¥: {e}")
                continue

        print(f"\nâœ… æ€»å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents

    def clean_document(self, doc: Document) -> Document:
        """æ¸…æ´—æ–‡æ¡£å†…å®¹

        æ¸…ç†HTMLæ ‡ç­¾ã€è§„èŒƒåŒ–ç©ºç™½ã€ç»Ÿä¸€ä»£ç å—æ ¼å¼ã€‚

        Args:
            doc: åŸå§‹æ–‡æ¡£

        Returns:
            Document: æ¸…æ´—åçš„æ–‡æ¡£

        Example:
            >>> doc = Document(page_content="<p>Hello</p>\\n\\n\\nWorld")
            >>> clean_doc = processor.clean_document(doc)
            >>> print(clean_doc.page_content)
            Hello

            World
        """
        content = doc.page_content

        # 1. ç§»é™¤ HTML æ ‡ç­¾(å¦‚æœæœ‰æ®‹ç•™)
        content = re.sub(r'<[^>]+>', '', content)

        # 2. ç§»é™¤å¤šä½™ç©ºç™½è¡Œ(ä¿ç•™åŒæ¢è¡Œè¡¨ç¤ºæ®µè½åˆ†éš”)
        content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)

        # 3. ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        lines = [line.strip() for line in content.split('\n')]
        content = '\n'.join(lines)

        # 4. ç»Ÿä¸€ä»£ç å—æ ¼å¼(ç¡®ä¿å‰åæœ‰ç©ºè¡Œ)
        content = re.sub(
            r'```(\w+)?\n(.*?)```',
            r'\n```\1\n\2\n```\n',
            content,
            flags=re.DOTALL
        )

        # 5. ç§»é™¤ç‰¹æ®Šå­—ç¬¦(ä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€å¸¸ç”¨æ ‡ç‚¹)
        # æ³¨æ„:ä¿ç•™ä»£ç å—ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        code_blocks = []
        def save_code_block(match):
            code_blocks.append(match.group(0))
            return f"__CODE_BLOCK_{len(code_blocks) - 1}__"

        # æš‚å­˜ä»£ç å—
        content = re.sub(r'```[\s\S]*?```', save_code_block, content)

        # æ¸…ç†éä»£ç å†…å®¹ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        # content = re.sub(r'[^\w\s\u4e00-\u9fffã€‚,!?;:ã€""''()ï¼ˆï¼‰ã€ã€‘ã€Šã€‹\-\n#*`]', '', content)

        # æ¢å¤ä»£ç å—
        for i, code_block in enumerate(code_blocks):
            content = content.replace(f"__CODE_BLOCK_{i}__", code_block)

        # 6. æœ€ç»ˆæ¸…ç†:ç§»é™¤é¦–å°¾ç©ºç™½
        content = content.strip()

        # æ›´æ–°æ–‡æ¡£å†…å®¹
        doc.page_content = content
        return doc

    def split_with_code_protection(self, doc: Document) -> List[Document]:
        """è¯­ä¹‰åˆ†å—(ä¿æŠ¤ä»£ç å—å®Œæ•´æ€§)

        åŸºäº Markdown æ ‡é¢˜è¿›è¡Œè¯­ä¹‰åˆ†å—,ç¡®ä¿ä»£ç å—ä¸è¢«æ‹†åˆ†ã€‚

        åˆ†å—ç­–ç•¥:
            1. é¦–å…ˆæŒ‰ Markdown æ ‡é¢˜åˆ†å—
            2. å¯¹äºåŒ…å«ä»£ç å—çš„chunk,ä¿æŒå®Œæ•´æ€§
            3. å¯¹äºçº¯æ–‡æœ¬chunk,å¯ä»¥è¿›ä¸€æ­¥æ‹†åˆ†

        Args:
            doc: å¾…åˆ†å—çš„æ–‡æ¡£

        Returns:
            List[Document]: åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨

        Example:
            >>> doc = Document(page_content="# Title\\n\\nText\\n\\n```python\\ncode\\n```")
            >>> chunks = processor.split_with_code_protection(doc)
            >>> print(len(chunks))
        """
        content = doc.page_content

        # æ£€æŸ¥æ˜¯å¦æ˜¯ Markdown æ ¼å¼(åŒ…å«æ ‡é¢˜)
        has_markdown_headers = bool(re.search(r'^#+\s', content, re.MULTILINE))

        if has_markdown_headers:
            # 1. å…ˆæŒ‰ Markdown æ ‡é¢˜åˆ†å—
            try:
                header_chunks = self.markdown_splitter.split_text(content)
            except Exception as e:
                print(f"âš ï¸  Markdown åˆ†å—å¤±è´¥,ä½¿ç”¨é»˜è®¤åˆ†å—: {e}")
                header_chunks = [Document(page_content=content, metadata=doc.metadata.copy())]
        else:
            # æ²¡æœ‰ Markdown æ ‡é¢˜,ä½œä¸ºå•ä¸ªæ–‡æ¡£å¤„ç†
            header_chunks = [Document(page_content=content, metadata=doc.metadata.copy())]

        # 2. å¯¹æ¯ä¸ªæ ‡é¢˜å—è¿›ä¸€æ­¥å¤„ç†
        final_chunks = []

        for chunk in header_chunks:
            # ç»§æ‰¿åŸæ–‡æ¡£çš„å…ƒæ•°æ®
            if not chunk.metadata:
                chunk.metadata = doc.metadata.copy()
            else:
                # åˆå¹¶å…ƒæ•°æ®(chunk çš„å…ƒæ•°æ®å¯èƒ½åŒ…å«æ ‡é¢˜ä¿¡æ¯)
                chunk.metadata = {**doc.metadata, **chunk.metadata}

            chunk_content = chunk.page_content

            # æ£€æµ‹ä»£ç å—
            code_blocks = re.findall(r'```[\s\S]*?```', chunk_content)

            if code_blocks and len(chunk_content) > self.chunk_size:
                # æœ‰ä»£ç å—ä¸”å†…å®¹è¿‡é•¿
                # ç­–ç•¥:å°è¯•åœ¨ä»£ç å—è¾¹ç•Œå¤„æ‹†åˆ†

                # è®¡ç®—ä»£ç å—æ€»é•¿åº¦
                code_length = sum(len(cb) for cb in code_blocks)
                text_length = len(chunk_content) - code_length

                if code_length > self.chunk_size * 0.8:
                    # ä»£ç å—å æ¯”è¿‡å¤§,ä¿æŒå®Œæ•´(å³ä½¿è¶…é•¿)
                    final_chunks.append(chunk)
                else:
                    # å°è¯•æŒ‰æ®µè½æ‹†åˆ†(é¿å…æ‹†æ•£ä»£ç å—)
                    sub_chunks = self._split_around_code_blocks(chunk)
                    final_chunks.extend(sub_chunks)

            elif len(chunk_content) <= self.chunk_size:
                # å†…å®¹åˆé€‚,ç›´æ¥ä½¿ç”¨
                final_chunks.append(chunk)

            else:
                # æ— ä»£ç å—æˆ–å†…å®¹è¾ƒçŸ­,å¯ä»¥è¿›ä¸€æ­¥æ‹†åˆ†
                sub_chunks = self.text_splitter.split_documents([chunk])
                final_chunks.extend(sub_chunks)

        # 3. ä¸ºæ¯ä¸ªchunkæ·»åŠ è¾¹ç•Œç±»å‹å…ƒæ•°æ®
        for i, chunk in enumerate(final_chunks):
            # åˆ¤æ–­chunk_boundaryç±»å‹
            if "Header 1" in chunk.metadata or "Header 2" in chunk.metadata:
                chunk.metadata["chunk_boundary"] = "section"
            elif "Header 3" in chunk.metadata:
                chunk.metadata["chunk_boundary"] = "subsection"
            elif "```" in chunk.page_content:
                chunk.metadata["chunk_boundary"] = "code_block"
            else:
                chunk.metadata["chunk_boundary"] = "paragraph"

        return final_chunks

    def _split_around_code_blocks(self, doc: Document) -> List[Document]:
        """å›´ç»•ä»£ç å—æ‹†åˆ†æ–‡æ¡£

        å°†æ–‡æ¡£åœ¨ä»£ç å—è¾¹ç•Œå¤„æ‹†åˆ†,ä¿æŒä»£ç å—å®Œæ•´æ€§ã€‚

        Args:
            doc: å¾…æ‹†åˆ†çš„æ–‡æ¡£

        Returns:
            List[Document]: æ‹†åˆ†åçš„æ–‡æ¡£åˆ—è¡¨
        """
        content = doc.page_content
        chunks = []

        # æ‰¾åˆ°æ‰€æœ‰ä»£ç å—çš„ä½ç½®
        pattern = r'(```[\s\S]*?```)'
        parts = re.split(pattern, content)

        current_chunk = ""
        for part in parts:
            if part.startswith("```"):
                # è¿™æ˜¯ä»£ç å—
                if current_chunk:
                    # å…ˆä¿å­˜ä¹‹å‰çš„æ–‡æœ¬
                    chunks.append(Document(
                        page_content=current_chunk.strip(),
                        metadata=doc.metadata.copy()
                    ))
                    current_chunk = ""

                # ä»£ç å—å•ç‹¬ä½œä¸ºä¸€ä¸ªchunk
                chunks.append(Document(
                    page_content=part.strip(),
                    metadata=doc.metadata.copy()
                ))
            else:
                # æ™®é€šæ–‡æœ¬
                if len(current_chunk) + len(part) <= self.chunk_size:
                    current_chunk += part
                else:
                    if current_chunk:
                        chunks.append(Document(
                            page_content=current_chunk.strip(),
                            metadata=doc.metadata.copy()
                        ))
                    current_chunk = part

        # ä¿å­˜å‰©ä½™å†…å®¹
        if current_chunk.strip():
            chunks.append(Document(
                page_content=current_chunk.strip(),
                metadata=doc.metadata.copy()
            ))

        return chunks if chunks else [doc]

    def calculate_quality_score(self, doc: Document) -> float:
        """è®¡ç®—æ–‡æ¡£è´¨é‡è¯„åˆ†(0-1)

        è¯„åˆ†ç»´åº¦:
            - é•¿åº¦åˆç†æ€§(100-2000å­—ç¬¦): 0.25åˆ†
            - ç»“æ„å®Œæ•´æ€§(æœ‰æ ‡é¢˜): 0.25åˆ†
            - ä»£ç ç¤ºä¾‹(æŠ€æœ¯æ–‡æ¡£): 0.25åˆ†
            - æ¥æºå¯ä¿¡åº¦: 0.25åˆ†

        Args:
            doc: å¾…è¯„åˆ†çš„æ–‡æ¡£

        Returns:
            float: è´¨é‡è¯„åˆ†(0.0-1.0)

        Example:
            >>> doc = Document(
            ...     page_content="# Title\\n\\nContent with ```code```",
            ...     metadata={"source_type": "official_docs"}
            ... )
            >>> score = processor.calculate_quality_score(doc)
            >>> print(f"è´¨é‡è¯„åˆ†: {score}")
        """
        score = 0.0
        content = doc.page_content
        metadata = doc.metadata

        # 1. é•¿åº¦åˆç†æ€§(100-2000å­—ç¬¦)
        length = len(content)
        if 100 < length < 2000:
            score += 0.25
        elif 50 < length <= 100:
            score += 0.15  # ç¨çŸ­,å‡å°‘åˆ†æ•°
        elif 2000 <= length < 3000:
            score += 0.20  # ç¨é•¿,å‡å°‘åˆ†æ•°

        # 2. ç»“æ„å®Œæ•´æ€§(æœ‰ Markdown æ ‡é¢˜)
        if re.search(r'^#+\s', content, re.MULTILINE):
            score += 0.25

        # 3. ä»£ç ç¤ºä¾‹(æŠ€æœ¯æ–‡æ¡£å¿…å¤‡)
        if '```' in content:
            score += 0.25
        elif '`' in content:
            # æœ‰è¡Œå†…ä»£ç ,ç»™éƒ¨åˆ†åˆ†æ•°
            score += 0.10

        # 4. æ¥æºå¯ä¿¡åº¦
        source_type = metadata.get("source_type", "unknown")
        if source_type == "official_docs":
            score += 0.25
        elif source_type == "github_docs":
            score += 0.20
        elif source_type == "qa_dataset":
            score += 0.15
        else:
            score += 0.05  # å…¶ä»–æ¥æºç»™å°‘é‡åˆ†æ•°

        # ç¡®ä¿åˆ†æ•°åœ¨ 0-1 èŒƒå›´å†…
        return min(1.0, max(0.0, score))

    def process_document(
        self,
        doc: Document,
        clean: bool = True,
        split: bool = True,
        calculate_score: bool = True
    ) -> List[Document]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£(æ¸…æ´—ã€åˆ†å—ã€è¯„åˆ†)

        å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹:
            1. æ¸…æ´—æ–‡æ¡£å†…å®¹
            2. è¯­ä¹‰åˆ†å—(å¯é€‰)
            3. è®¡ç®—è´¨é‡è¯„åˆ†(å¯é€‰)

        Args:
            doc: åŸå§‹æ–‡æ¡£
            clean: æ˜¯å¦æ¸…æ´—æ–‡æ¡£
            split: æ˜¯å¦åˆ†å—
            calculate_score: æ˜¯å¦è®¡ç®—è´¨é‡è¯„åˆ†

        Returns:
            List[Document]: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨

        Example:
            >>> processor = DocumentProcessor()
            >>> raw_doc = Document(page_content="<p>Hello</p>")
            >>> processed_docs = processor.process_document(raw_doc)
            >>> print(f"å¤„ç†å: {len(processed_docs)} ä¸ªæ–‡æ¡£")
        """
        # 1. æ¸…æ´—
        if clean:
            doc = self.clean_document(doc)

        # 2. åˆ†å—
        if split:
            chunks = self.split_with_code_protection(doc)
        else:
            chunks = [doc]

        # 3. è®¡ç®—è´¨é‡è¯„åˆ†
        if calculate_score:
            for chunk in chunks:
                quality_score = self.calculate_quality_score(chunk)
                chunk.metadata["quality_score"] = quality_score

        return chunks

    def process_documents(
        self,
        docs: List[Document],
        clean: bool = True,
        split: bool = True,
        calculate_score: bool = True
    ) -> List[Document]:
        """æ‰¹é‡å¤„ç†æ–‡æ¡£

        Args:
            docs: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            clean: æ˜¯å¦æ¸…æ´—æ–‡æ¡£
            split: æ˜¯å¦åˆ†å—
            calculate_score: æ˜¯å¦è®¡ç®—è´¨é‡è¯„åˆ†

        Returns:
            List[Document]: å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨

        Example:
            >>> processor = DocumentProcessor()
            >>> raw_docs = [Document(page_content="Doc 1"), Document(page_content="Doc 2")]
            >>> processed_docs = processor.process_documents(raw_docs)
        """
        all_chunks = []

        for i, doc in enumerate(docs):
            try:
                chunks = self.process_document(
                    doc,
                    clean=clean,
                    split=split,
                    calculate_score=calculate_score
                )
                all_chunks.extend(chunks)

                if (i + 1) % 10 == 0:
                    print(f"   å¤„ç†è¿›åº¦: {i + 1}/{len(docs)}")

            except Exception as e:
                print(f"   âš ï¸  å¤„ç†æ–‡æ¡£ {i} å¤±è´¥: {e}")
                continue

        print(f"\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ: {len(docs)} ä¸ªæ–‡æ¡£ â†’ {len(all_chunks)} ä¸ªchunks")
        return all_chunks
