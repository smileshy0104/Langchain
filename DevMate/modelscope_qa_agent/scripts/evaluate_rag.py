"""
RAG System Evaluation Script

ä½¿ç”¨ RAGAs æ¡†æ¶è¯„ä¼° RAG ç³»ç»Ÿçš„æ€§èƒ½,åŒ…æ‹¬:
- Context Relevance: æ£€ç´¢æ–‡æ¡£ä¸é—®é¢˜çš„ç›¸å…³æ€§
- Answer Faithfulness: ç­”æ¡ˆä¸æ–‡æ¡£çš„ä¸€è‡´æ€§
- Answer Relevance: ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³æ€§
- Answer Correctness: ç­”æ¡ˆçš„æ­£ç¡®æ€§

åŒæ—¶è¯„ä¼°å“åº”é€Ÿåº¦æ€§èƒ½æŒ‡æ ‡ã€‚
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Import dependencies
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    ContextRecall,  # Context Relevance
    Faithfulness,    # Answer Faithfulness
    AnswerRelevancy, # Answer Relevance
    AnswerCorrectness # Answer Correctness
)
from langchain_community.chat_models import ChatTongyi
from langchain_community.embeddings import DashScopeEmbeddings

# Import QA Agent
from agents.qa_agent import ModelScopeQAAgent
from retrievers.hybrid_retriever import HybridRetriever
from core.vector_store import VectorStoreManager


class RAGEvaluator:
    """RAG ç³»ç»Ÿè¯„ä¼°å™¨

    ä½¿ç”¨ RAGAs æ¡†æ¶è¯„ä¼° RAG ç³»ç»Ÿçš„å„é¡¹æ€§èƒ½æŒ‡æ ‡ã€‚
    """

    def __init__(
        self,
        agent: ModelScopeQAAgent,
        llm_api_key: str,
        embedding_api_key: str = None
    ):
        """åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            agent: ModelScopeQAAgent å®ä¾‹
            llm_api_key: LLM APIå¯†é’¥ (ç”¨äºè¯„ä¼°)
            embedding_api_key: Embedding APIå¯†é’¥ (å¦‚æœä¸LLMä¸åŒ)
        """
        self.agent = agent

        # åˆå§‹åŒ–è¯„ä¼°ç”¨çš„ LLM å’Œ Embeddings
        # RAGAs éœ€è¦ä½¿ç”¨ OpenAI å…¼å®¹çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°
        # è¿™é‡Œä½¿ç”¨é€šä¹‰åƒé—®ä½œä¸ºè¯„ä¼°æ¨¡å‹
        self.eval_llm = ChatTongyi(
            model="qwen-plus",
            temperature=0.0,
            dashscope_api_key=llm_api_key
        )

        self.eval_embeddings = DashScopeEmbeddings(
            model="text-embedding-v2",
            dashscope_api_key=embedding_api_key or llm_api_key
        )

        print(f"âœ… RAGEvaluator åˆå§‹åŒ–æˆåŠŸ")
        print(f"   - è¯„ä¼° LLM: qwen-plus")
        print(f"   - è¯„ä¼° Embeddings: text-embedding-v2")

    def load_evaluation_dataset(self, dataset_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½è¯„æµ‹æ•°æ®é›†

        Args:
            dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)

        Returns:
            è¯„æµ‹æ•°æ®åˆ—è¡¨
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“¥ åŠ è½½è¯„æµ‹æ•°æ®é›†")
        print(f"{'='*70}")

        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)

        print(f"âœ… åŠ è½½æˆåŠŸ: {len(dataset)} æ¡æµ‹è¯•æ•°æ®")

        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        categories = {}
        for item in dataset:
            cat = item.get('category', 'unknown')
            categories[cat] = categories.get(cat, 0) + 1

        print(f"\nğŸ“Š æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ:")
        for cat, count in sorted(categories.items()):
            print(f"   - {cat}: {count} æ¡")

        return dataset

    def run_inference(
        self,
        dataset: List[Dict[str, Any]],
        max_samples: int = None
    ) -> List[Dict[str, Any]]:
        """è¿è¡Œæ¨ç†è·å– Agent å“åº”

        Args:
            dataset: è¯„æµ‹æ•°æ®é›†
            max_samples: æœ€å¤šå¤„ç†çš„æ ·æœ¬æ•° (None=å…¨éƒ¨)

        Returns:
            åŒ…å«é—®é¢˜ã€ä¸Šä¸‹æ–‡ã€ç­”æ¡ˆå’ŒçœŸå®ç­”æ¡ˆçš„æ•°æ®åˆ—è¡¨
        """
        print(f"\n{'='*70}")
        print(f"ğŸ¤– è¿è¡Œ Agent æ¨ç†")
        print(f"{'='*70}")

        results = []
        samples = dataset[:max_samples] if max_samples else dataset

        for i, item in enumerate(samples, 1):
            question = item['question']
            ground_truth = item['ground_truth']

            print(f"\n[{i}/{len(samples)}] å¤„ç†é—®é¢˜: {question[:50]}...")

            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()

            try:
                # è°ƒç”¨ Agent
                response = self.agent.invoke(question)

                # è®¡ç®—å“åº”æ—¶é—´
                response_time = time.time() - start_time

                # æå–ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡
                answer = response.get('summary', '')
                if response.get('problem_analysis'):
                    answer += "\n\n" + response['problem_analysis']
                if response.get('solutions'):
                    answer += "\n\nè§£å†³æ–¹æ¡ˆ:\n" + "\n".join(response['solutions'])

                # è·å–æ£€ç´¢çš„ä¸Šä¸‹æ–‡
                # æ³¨æ„: è¿™é‡Œéœ€è¦ä» Agent çš„çŠ¶æ€ä¸­è·å–æ£€ç´¢çš„æ–‡æ¡£
                # ç”±äºå½“å‰å®ç°æ²¡æœ‰ç›´æ¥è¿”å›,æˆ‘ä»¬ä½¿ç”¨ ground_truth contexts
                contexts = item.get('contexts', [])

                results.append({
                    'question': question,
                    'answer': answer,
                    'contexts': contexts,
                    'ground_truth': ground_truth,
                    'response_time': response_time,
                    'confidence_score': response.get('confidence_score', 0.0)
                })

                print(f"   âœ… å®Œæˆ (è€—æ—¶: {response_time:.2f}s, ç½®ä¿¡åº¦: {response.get('confidence_score', 0.0):.2f})")

            except Exception as e:
                print(f"   âŒ å¤±è´¥: {e}")
                # æ·»åŠ å¤±è´¥è®°å½•
                results.append({
                    'question': question,
                    'answer': f"Error: {str(e)}",
                    'contexts': item.get('contexts', []),
                    'ground_truth': ground_truth,
                    'response_time': time.time() - start_time,
                    'confidence_score': 0.0
                })

        print(f"\nâœ… æ¨ç†å®Œæˆ: {len(results)}/{len(samples)} æˆåŠŸ")
        return results

    def evaluate_with_ragas(
        self,
        results: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """ä½¿ç”¨ RAGAs è¯„ä¼°ç»“æœ

        Args:
            results: æ¨ç†ç»“æœåˆ—è¡¨

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“Š RAGAs è¯„ä¼°")
        print(f"{'='*70}")

        # è½¬æ¢ä¸º RAGAs éœ€è¦çš„æ ¼å¼
        data = {
            'question': [r['question'] for r in results],
            'answer': [r['answer'] for r in results],
            'contexts': [r['contexts'] for r in results],
            'ground_truth': [r['ground_truth'] for r in results]
        }

        # åˆ›å»º Dataset
        dataset = Dataset.from_dict(data)

        print(f"\nğŸ” å¼€å§‹è¯„ä¼° (å…± {len(results)} æ¡æ•°æ®)...")
        print(f"   è¯„ä¼°æŒ‡æ ‡: Context Relevance, Faithfulness, Answer Relevance, Answer Correctness")

        try:
            # è¿è¡Œè¯„ä¼°
            # RAGAs 0.3.9 ä½¿ç”¨æ–°çš„ API
            eval_results = evaluate(
                dataset,
                metrics=[
                    ContextRecall(),     # ä¸Šä¸‹æ–‡å¬å›ç‡
                    Faithfulness(),      # ç­”æ¡ˆå¿ å®åº¦
                    AnswerRelevancy(),   # ç­”æ¡ˆç›¸å…³æ€§
                    AnswerCorrectness()  # ç­”æ¡ˆæ­£ç¡®æ€§
                ],
                llm=self.eval_llm,
                embeddings=self.eval_embeddings
            )

            print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
            return eval_results

        except Exception as e:
            print(f"\nâŒ è¯„ä¼°å¤±è´¥: {e}")
            print(f"   è¿™å¯èƒ½æ˜¯ç”±äº RAGAs ç‰ˆæœ¬æˆ– API é…ç½®é—®é¢˜")
            print(f"   è¿”å›åŸºç¡€ç»Ÿè®¡ä¿¡æ¯...")

            # è¿”å›åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            return self._calculate_basic_metrics(results)

    def _calculate_basic_metrics(
        self,
        results: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """è®¡ç®—åŸºç¡€è¯„ä¼°æŒ‡æ ‡ (é™çº§æ–¹æ¡ˆ)

        Args:
            results: æ¨ç†ç»“æœåˆ—è¡¨

        Returns:
            åŸºç¡€æŒ‡æ ‡å­—å…¸
        """
        total = len(results)
        successful = sum(1 for r in results if 'Error' not in r['answer'])

        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        avg_response_time = sum(r['response_time'] for r in results) / total

        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = sum(r['confidence_score'] for r in results) / total

        # è®¡ç®—æˆåŠŸç‡
        success_rate = successful / total

        return {
            'success_rate': success_rate,
            'avg_response_time': avg_response_time,
            'avg_confidence': avg_confidence,
            'total_questions': total,
            'successful_answers': successful
        }

    def evaluate_response_time(
        self,
        results: List[Dict[str, Any]],
        target_threshold: float = 30.0
    ) -> Dict[str, Any]:
        """è¯„ä¼°å“åº”é€Ÿåº¦

        Args:
            results: æ¨ç†ç»“æœåˆ—è¡¨
            target_threshold: ç›®æ ‡å“åº”æ—¶é—´é˜ˆå€¼(ç§’)

        Returns:
            å“åº”æ—¶é—´ç»Ÿè®¡
        """
        print(f"\n{'='*70}")
        print(f"â±ï¸  å“åº”é€Ÿåº¦è¯„ä¼°")
        print(f"{'='*70}")

        response_times = [r['response_time'] for r in results]

        stats = {
            'mean': sum(response_times) / len(response_times),
            'min': min(response_times),
            'max': max(response_times),
            'p50': sorted(response_times)[len(response_times) // 2],
            'p95': sorted(response_times)[int(len(response_times) * 0.95)],
            'p99': sorted(response_times)[int(len(response_times) * 0.99)],
            'target_threshold': target_threshold,
            'within_threshold': sum(1 for t in response_times if t <= target_threshold),
            'threshold_percentage': sum(1 for t in response_times if t <= target_threshold) / len(response_times) * 100
        }

        print(f"\nğŸ“Š å“åº”æ—¶é—´ç»Ÿè®¡:")
        print(f"   - å¹³å‡: {stats['mean']:.2f}s")
        print(f"   - æœ€å°: {stats['min']:.2f}s")
        print(f"   - æœ€å¤§: {stats['max']:.2f}s")
        print(f"   - P50: {stats['p50']:.2f}s")
        print(f"   - P95: {stats['p95']:.2f}s")
        print(f"   - P99: {stats['p99']:.2f}s")
        print(f"\nğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ:")
        print(f"   - ç›®æ ‡é˜ˆå€¼: <{target_threshold}s")
        print(f"   - è¾¾æ ‡æ•°é‡: {stats['within_threshold']}/{len(response_times)}")
        print(f"   - è¾¾æ ‡ç‡: {stats['threshold_percentage']:.1f}%")

        if stats['mean'] < target_threshold:
            print(f"   âœ… å¹³å‡å“åº”æ—¶é—´è¾¾æ ‡!")
        else:
            print(f"   âŒ å¹³å‡å“åº”æ—¶é—´æœªè¾¾æ ‡")

        return stats

    def generate_report(
        self,
        ragas_results: Dict[str, float],
        response_stats: Dict[str, Any],
        results: List[Dict[str, Any]],
        output_path: str
    ):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

        Args:
            ragas_results: RAGAs è¯„ä¼°ç»“æœ
            response_stats: å“åº”æ—¶é—´ç»Ÿè®¡
            results: è¯¦ç»†ç»“æœ
            output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
        print(f"{'='*70}")

        report = {
            'evaluation_time': datetime.now().isoformat(),
            'total_questions': len(results),
            'ragas_metrics': ragas_results,
            'response_time_stats': response_stats,
            'detailed_results': results
        }

        # ä¿å­˜ JSON æŠ¥å‘Š
        json_path = output_path.replace('.md', '.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"âœ… JSON æŠ¥å‘Šå·²ä¿å­˜: {json_path}")

        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        self._generate_markdown_report(report, output_path)
        print(f"âœ… Markdown æŠ¥å‘Šå·²ä¿å­˜: {output_path}")

    def _generate_markdown_report(
        self,
        report: Dict[str, Any],
        output_path: str
    ):
        """ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š

        Args:
            report: æŠ¥å‘Šæ•°æ®
            output_path: è¾“å‡ºè·¯å¾„
        """
        ragas = report['ragas_metrics']
        response_stats = report['response_time_stats']

        md_content = f"""# RAG System Evaluation Report

**è¯„ä¼°æ—¶é—´**: {report['evaluation_time']}
**æµ‹è¯•é—®é¢˜æ•°**: {report['total_questions']}

---

## RAGAs è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | å¾—åˆ† | ç›®æ ‡ | çŠ¶æ€ |
|------|------|------|------|
"""

        # æ·»åŠ  RAGAs æŒ‡æ ‡
        # RAGAs 0.3.9 è¿”å›çš„ key åç§°å¯èƒ½ä¸åŒ
        metric_keys = [
            ('context_recall', 'Context Recall', 0.85),
            ('faithfulness', 'Answer Faithfulness', 0.95),
            ('answer_relevancy', 'Answer Relevance', None),
            ('answer_correctness', 'Answer Correctness', None)
        ]

        for key, name, threshold in metric_keys:
            if key in ragas:
                score = ragas[key]
                if threshold:
                    status = "âœ… è¾¾æ ‡" if score >= threshold else "âŒ æœªè¾¾æ ‡"
                    md_content += f"| {name} | {score:.2%} | â‰¥{threshold*100:.0f}% | {status} |\n"
                else:
                    md_content += f"| {name} | {score:.2%} | - | - |\n"

        # æ·»åŠ åŸºç¡€æŒ‡æ ‡ (å¦‚æœä½¿ç”¨é™çº§æ–¹æ¡ˆ)
        if 'success_rate' in ragas:
            md_content += f"| Success Rate | {ragas['success_rate']:.2%} | - | - |\n"
            md_content += f"| Avg Confidence | {ragas['avg_confidence']:.2f} | - | - |\n"

        md_content += f"""
---

## å“åº”é€Ÿåº¦è¯„ä¼°

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å¹³å‡å“åº”æ—¶é—´ | {response_stats['mean']:.2f}s |
| P50 (ä¸­ä½æ•°) | {response_stats['p50']:.2f}s |
| P95 | {response_stats['p95']:.2f}s |
| P99 | {response_stats['p99']:.2f}s |
| æœ€å°å€¼ | {response_stats['min']:.2f}s |
| æœ€å¤§å€¼ | {response_stats['max']:.2f}s |

**ç›®æ ‡è¾¾æˆæƒ…å†µ**:
- ç›®æ ‡é˜ˆå€¼: <{response_stats['target_threshold']}s
- è¾¾æ ‡ç‡: {response_stats['threshold_percentage']:.1f}% ({response_stats['within_threshold']}/{report['total_questions']})
- çŠ¶æ€: {"âœ… è¾¾æ ‡" if response_stats['mean'] < response_stats['target_threshold'] else "âŒ æœªè¾¾æ ‡"}

---

## æ€»ç»“

"""

        # æ·»åŠ æ€»ç»“
        if 'context_recall' in ragas and ragas['context_recall'] >= 0.85:
            md_content += "- âœ… Context Recall è¾¾åˆ°ç›®æ ‡ (â‰¥85%)\n"
        elif 'context_recall' in ragas:
            md_content += f"- âŒ Context Recall æœªè¾¾æ ‡ ({ragas['context_recall']:.2%} < 85%)\n"

        if 'faithfulness' in ragas and ragas['faithfulness'] >= 0.95:
            md_content += "- âœ… Answer Faithfulness è¾¾åˆ°ç›®æ ‡ (â‰¥95%)\n"
        elif 'faithfulness' in ragas:
            md_content += f"- âŒ Answer Faithfulness æœªè¾¾æ ‡ ({ragas['faithfulness']:.2%} < 95%)\n"

        if response_stats['mean'] < response_stats['target_threshold']:
            md_content += f"- âœ… å“åº”é€Ÿåº¦è¾¾æ ‡ (å¹³å‡ {response_stats['mean']:.2f}s < {response_stats['target_threshold']}s)\n"
        else:
            md_content += f"- âŒ å“åº”é€Ÿåº¦æœªè¾¾æ ‡ (å¹³å‡ {response_stats['mean']:.2f}s â‰¥ {response_stats['target_threshold']}s)\n"

        md_content += "\n---\n\n**è¯¦ç»†ç»“æœ**: è¯·æŸ¥çœ‹ JSON æŠ¥å‘Šæ–‡ä»¶\n"

        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(md_content)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Evaluate RAG System with RAGAs')
    parser.add_argument(
        '--dataset',
        type=str,
        default='data/evaluation_dataset.json',
        help='Path to evaluation dataset (JSON format)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='results/evaluation_report.md',
        help='Path to output report (Markdown format)'
    )
    parser.add_argument(
        '--max-samples',
        type=int,
        default=None,
        help='Maximum number of samples to evaluate (default: all)'
    )
    parser.add_argument(
        '--api-key',
        type=str,
        default=None,
        help='DashScope API key (overrides .env)'
    )

    args = parser.parse_args()

    # åŠ è½½ç¯å¢ƒå˜é‡
    from dotenv import load_dotenv
    load_dotenv()

    api_key = args.api_key or os.getenv('DASHSCOPE_API_KEY')
    if not api_key:
        print("âŒ Error: DASHSCOPE_API_KEY not found")
        print("   Please set it in .env file or use --api-key argument")
        sys.exit(1)

    print(f"\n{'='*70}")
    print(f"ğŸš€ RAG System Evaluation")
    print(f"{'='*70}")
    print(f"Dataset: {args.dataset}")
    print(f"Output: {args.output}")
    print(f"Max Samples: {args.max_samples or 'All'}")
    print(f"{'='*70}")

    try:
        # åˆå§‹åŒ–ç»„ä»¶
        print(f"\nğŸ“¦ åˆå§‹åŒ–ç»„ä»¶...")

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        vector_store_manager = VectorStoreManager(
            host=os.getenv('MILVUS_HOST', 'localhost'),
            port=int(os.getenv('MILVUS_PORT', 19530))
        )

        # åˆå§‹åŒ–æ£€ç´¢å™¨
        retriever = HybridRetriever(
            vector_store_manager=vector_store_manager,
            collection_name="modelscope_knowledge_base",
            embedding_api_key=api_key
        )

        # åˆå§‹åŒ– Agent
        agent = ModelScopeQAAgent(
            retriever=retriever,
            llm_api_key=api_key,
            temperature=0.7
        )

        # åˆå§‹åŒ–è¯„ä¼°å™¨
        evaluator = RAGEvaluator(
            agent=agent,
            llm_api_key=api_key
        )

        # åŠ è½½æ•°æ®é›†
        dataset = evaluator.load_evaluation_dataset(args.dataset)

        # è¿è¡Œæ¨ç†
        results = evaluator.run_inference(dataset, args.max_samples)

        # RAGAs è¯„ä¼°
        ragas_results = evaluator.evaluate_with_ragas(results)

        # å“åº”é€Ÿåº¦è¯„ä¼°
        response_stats = evaluator.evaluate_response_time(results, target_threshold=30.0)

        # ç”ŸæˆæŠ¥å‘Š
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        evaluator.generate_report(ragas_results, response_stats, results, args.output)

        print(f"\n{'='*70}")
        print(f"âœ… è¯„ä¼°å®Œæˆ!")
        print(f"{'='*70}")
        print(f"æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"   - Markdown: {args.output}")
        print(f"   - JSON: {args.output.replace('.md', '.json')}")
        print(f"{'='*70}\n")

    except Exception as e:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
