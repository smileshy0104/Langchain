#!/usr/bin/env python3
"""
å°†çˆ¬å–çš„æ•°æ®å¯¼å…¥å‘é‡æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python scripts/ingest_crawled_data.py --input data/processed/all_documents.jsonl
"""

import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from langchain_core.documents import Document
from core.vector_store import VectorStoreManager
from config.config_loader import load_config, get_config


def load_processed_documents(input_file: str) -> List[Document]:
    """
    åŠ è½½å¤„ç†åçš„æ–‡æ¡£

    Args:
        input_file: JSONLæ–‡ä»¶è·¯å¾„

    Returns:
        Documentåˆ—è¡¨
    """
    print(f"ğŸ“‚ åŠ è½½æ–‡æ¡£: {input_file}")

    documents = []
    input_path = Path(input_file)

    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return []

    with open(input_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f, 1):
            try:
                data = json.loads(line.strip())

                content = data.get('content', '')
                metadata = data.get('metadata', {})

                if content:
                    doc = Document(
                        page_content=content,
                        metadata=metadata
                    )
                    documents.append(doc)

                if i % 100 == 0:
                    print(f"   å·²åŠ è½½ {i} ä¸ªæ–‡æ¡£...")

            except Exception as e:
                print(f"âš ï¸  ç¬¬ {i} è¡Œè§£æå¤±è´¥: {e}")
                continue

    print(f"âœ… å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def ingest_documents(documents: List[Document], batch_size: int = 50):
    """
    å¯¼å…¥æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“

    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        batch_size: æ‰¹å¤„ç†å¤§å°
    """
    # åŠ è½½é…ç½®
    config = load_config()

    print("\n" + "=" * 70)
    print("å¼€å§‹å¯¼å…¥æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
    print(f"é›†åˆåç§°: {get_config('milvus.collection_name')}")
    print(f"æ–‡æ¡£æ•°é‡: {len(documents)}")
    print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print("=" * 70)

    try:
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        print("\nğŸ“Š åˆå§‹åŒ–å‘é‡å­˜å‚¨...")
        vector_store_manager = VectorStoreManager()
        vector_store = vector_store_manager.get_vector_store()

        # æ‰¹é‡å¯¼å…¥
        total_batches = (len(documents) + batch_size - 1) // batch_size
        print(f"\nğŸ“¥ å¼€å§‹æ‰¹é‡å¯¼å…¥ (å…± {total_batches} æ‰¹)...")

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_num = i // batch_size + 1

            print(f"\næ‰¹æ¬¡ {batch_num}/{total_batches}: å¯¼å…¥ {len(batch)} ä¸ªæ–‡æ¡£...")

            try:
                # ä½¿ç”¨add_documentsæ–¹æ³•
                vector_store.add_documents(batch)
                print(f"âœ… æ‰¹æ¬¡ {batch_num} å¯¼å…¥æˆåŠŸ")

            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_num} å¯¼å…¥å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue

        # è·å–ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 70)
        print("ğŸ“Š å¯¼å…¥ç»Ÿè®¡")
        print("=" * 70)

        # å°è¯•è·å–é›†åˆç»Ÿè®¡
        try:
            from pymilvus import connections, Collection
            connections.connect(
                alias="default",
                uri=get_config('milvus.uri')
            )
            collection = Collection(get_config('milvus.collection_name'))
            collection.load()
            stats = collection.num_entities
            print(f"âœ… å‘é‡åº“ä¸­å½“å‰æ–‡æ¡£æ€»æ•°: {stats}")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯: {e}")

        print("\nâœ… å¯¼å…¥å®Œæˆ!")

    except Exception as e:
        print(f"\nâŒ å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


def main():
    parser = argparse.ArgumentParser(description="å°†çˆ¬å–çš„æ•°æ®å¯¼å…¥å‘é‡æ•°æ®åº“")

    parser.add_argument(
        "--input",
        default="data/processed/all_documents.jsonl",
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„(JSONLæ ¼å¼)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=50,
        help="æ‰¹å¤„ç†å¤§å°"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ä»…åŠ è½½æ–‡æ¡£ä½†ä¸å¯¼å…¥(ç”¨äºæµ‹è¯•)"
    )

    args = parser.parse_args()

    print("=" * 70)
    print("çˆ¬å–æ•°æ®å¯¼å…¥å‘é‡æ•°æ®åº“")
    print("=" * 70)

    # åŠ è½½æ–‡æ¡£
    documents = load_processed_documents(args.input)

    if not documents:
        print("âŒ æ²¡æœ‰æ–‡æ¡£å¯ä»¥å¯¼å…¥")
        return

    # æ˜¾ç¤ºæ–‡æ¡£ç»Ÿè®¡
    print("\nğŸ“Š æ–‡æ¡£ç»Ÿè®¡:")
    source_types = {}
    for doc in documents:
        source_type = doc.metadata.get('source_type', 'unknown')
        source_types[source_type] = source_types.get(source_type, 0) + 1

    for source_type, count in sorted(source_types.items()):
        print(f"   {source_type}: {count} ä¸ªæ–‡æ¡£")

    if args.dry_run:
        print("\nâš ï¸  Dry-runæ¨¡å¼ï¼Œä¸æ‰§è¡Œå®é™…å¯¼å…¥")
        return

    # ç¡®è®¤
    try:
        response = input("\næ˜¯å¦ç»§ç»­å¯¼å…¥? (y/n): ")
        if response.lower() != 'y':
            print("âŒ ç”¨æˆ·å–æ¶ˆå¯¼å…¥")
            return
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·å–æ¶ˆå¯¼å…¥")
        return

    # å¯¼å…¥æ–‡æ¡£
    ingest_documents(documents, args.batch_size)


if __name__ == "__main__":
    main()
